@article{Johnson2015a,
annote = {Problem formulation:
- Given a synthesized robot controller A, and probabilistic models of environment, sensors, and actuators in LTL, find the probability of the controller satisfying a task specification (also in LTL)
- Also suggest revisions to the user so that the probability of satisfying the specifications increases.

LTL used to specify
- Environment probabilities 
- Sensor Uncertainty
- Actuation Uncertainty

Using the LTL, synthesize a controller as an automaton.

Then use a model checker (such as PRISM) to provide a probability of satisfying task specifications that might also have been specified with LTL.

Feedback is of one of four types:
- addition of a safety specification
- restriction of initial conditions
- refining of low-level components
- removal of liveness (deadlock) specifications.

Method of going from LTL -{\textgreater} Controller is specified in a previous set of papers.

This is a form of static checking for faults.},
author = {Johnson, Benjamin and Kress-Gazit, Hadas},
doi = {10.1177/0278364914562980},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
month = {may},
number = {6},
pages = {816--832},
title = {{Analyzing and revising synthesized controllers for robots with sensing and actuation errors}},
url = {http://journals.sagepub.com/doi/10.1177/0278364914562980},
volume = {34},
year = {2015}
}
@inproceedings{ZsoltKira2007,
annote = {Motivation {\&} Overview:
- Faults need to be specified a-priori in most other methods; not for this one.
- Robots have hierarchical "perceptual processing schemas" available to them as they solve a task, and we can leverage this information
- "A particular task coupled with the perceptual processing performed for that task produces task-specific cross-sensory and sensorimotor correlations that can be modeled. Such mappings will change whenever the capabilities of the robot [or the task] changes."
- The idea is to learn mappings of inputs and outputs of schemas for a particular task. Then, when an anomaly in I/O schemas is detected at runtime, the fault can be localized based on the sets of mappings that have changed.
- By simply using correlations between I/O in perceptual schemas, the models can be trained in an unsupervised manner.
- Using correlations, allows for the use of non-discrete, gradual adaptation of models.

Method:
- The hierarchical structure of the perceptual schema is available as a DAG.
- All perceptual schemas in the DAG become nodes in a fully connected graph, with links representing functions relating outputs of the 2 schemas at the vertices of the edge. The fully-connected graph is then a 'correlation graph'. (An alternative initialization of the correlation graph can actually use the DAG structure)
- Models correlating any two outputs can be of an arbitrary form. The only major requirement is that they learn in an unsupervised manner from nominal execution data.
- Then remove inconsistent edges from the graph based on a 'metric of consistency'
- Anomalies detected in a (subset of) models can then be used to localize a fault

Implementation in the paper:
- the links in the graph are "Self-Organizing Maps" - ANNs that are typically used for clustering
- The loss function (quantization error) of the self-organizing maps is used as a consistency metric

Verified and tested on a Pioneer robot},
author = {{Zsolt Kira}},
booktitle = {2007 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2007.4398978},
isbn = {978-1-4244-0911-2},
month = {oct},
pages = {1520--1526},
publisher = {IEEE},
title = {{Modeling cross-sensory and sensorimotor correlations to detect and localize faults in mobile robots}},
url = {http://ieeexplore.ieee.org/document/4398978/ https://www.cc.gatech.edu/{~}zk15/papers/KiraIROS07.pdf},
year = {2007}
}
@article{Smyth1994,
annote = {Motivations:
- "Fault detection and isolation is often a complicated and lengthy process due to the fact that it can be difficult to establish the root cause of a problem in the communications chain"
- Tackles monitoring dynamic systems with the purpose of detecting anomalous states that have not been modeled a priori
- Fault knowledge may not be exhaustive for infrequent faults

Assumptions:
- Data is amenable to an HMM model
- Multiple simultaneous faults are not possible
- Probability distributions in the HMM are stationary
- "In fault monitoring applications there is no sequence of data available describing transitions over time from state to state and, thus, the HMM estimation algorithms are not directly applicable. Instead, data are available from each state individually ... in such situations, prior knowledge must be used to determine the transition probabilities." This is done using MTBF for example; more discussion in a previous paper.
- The exhaustive-knowledge-of-fault-states assumption is relaxed halfway through the paper.

Methods:

First, introducing a discriminative approach to HMM observation probabilities:
- Hidden states correspond to distinct fault states
- Use a "discriminative" formulation of observation probabilities so that we learn/provide the inverse observation probability instead of the traditional forward one.
- Tried to estimate the forward observation probability with mixture models and the backwards observation probability with neural networks. Found the former to be extremely noisy and hard to estimate while the latter was much simpler and provided better recreation(?)

Then, adding in abilities for unknown states:
- Discriminative classifiers are hard to adapt to new classes and generative models are better at online adaptation. But the previous result prefers discriminative approach.
- Solution to the quandry is to use generative and discriminative approaches in parallel.
- Specifically, the discriminative model's prediction is conditioned on whether the data is coming from a known class. This latter conditioning is the output of the generative model.
- In order to more easily train the generative model in high dimensional data, the entire dimensionality of the data need not be used (or independence assumptions can be made!)},
author = {Smyth, P.},
doi = {10.1109/49.339929},
issn = {07338716},
journal = {IEEE Journal on Selected Areas in Communications},
number = {9},
pages = {1600--1612},
title = {{Markov monitoring with unknown states}},
url = {http://ieeexplore.ieee.org/document/339929/},
volume = {12},
year = {1994}
}
@article{Ricks2014,
annote = {- Automatically generate a Bayes Net to help with fault diagnosis and isolation given a system model diagram
- Compile Bayes Nets to "Arithmetic Circuits" for quick inference over the nodes given the evidence

- "One dimension is speed of fault progression, where we distinguish between faults that progress very quickly (abrupt faults) versus faults that progress very slowly (incipient faults). Another dimension is fault dynamics, where one typically distinguishes between persistent and intermittent faults. A third dimension is fault cardinality, where it is fruitful to distinguish between continuous (or parametric) faults and discrete faults. As an example, ‘stuck high' is a discrete fault, while ‘stuck at X', where X is a parameter that can vary over a real-valued interval, is considered a continuous fault. A fourth dimension is fault dependency, namely whether faults are independent or dependent; common cause faults and cascading faults are clear examples of dependent faults. A fifth dimension is fault scope; models and algorithms may make the single fault assumption or not."

- This paper - abrupt {\&} incipient, continuous {\&} discrete, independent {\&} non-cascading, single {\&} multiple.
- The paper defines 2 types of hidden nodes and 7 types of evidence nodes to aid in fault isolation based on the evidence.
- Each of the 7 types of evidence nodes is essentially a different type of "thresholding" mechanism on data streams in the system.},
author = {Ricks, Brian and Mengshoel, Ole J.},
doi = {10.1016/j.ijar.2014.02.005},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
month = {jul},
number = {5},
pages = {1207--1234},
title = {{Diagnosis for uncertain, dynamic and hybrid domains using Bayesian networks and arithmetic circuits}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0888613X14000346},
volume = {55},
year = {2014}
}
@techreport{rubinstein2008efficient,
annote = {Included as a baseline in the Park et al. anomaly detection code. Not sure why yet...},
author = {Rubinstein, Ron and Zibulevsky, Michael and Elad, Michael},
institution = {Computer Science Department, Technion},
title = {{Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit}},
url = {http://www.cs.technion.ac.il/{~}ronrubin/Publications/KSVD-OMP-v2.pdf},
year = {2008}
}
@article{Shah2008,
annote = {- In robotics, failures are driven by more than simply component failures. Therefore, the concept of failure should be replaced by the concept of interventions
- In hostile environments, there are basically 2 objectives - maximize human productivity, minimize human risk/exposure
- In some scenarios, one objective might be more important than the other. In general, need to adequately trade-off between both.
- An intervention is defined as a robotic agent receiving unplanned assistance from a human agent

There are 2 metrics that we operate over: Mean Time Between Interventions (MTBI) and Mean Time Completing Interventions (MTCI) / Mean Time to Intervene (MTTI)

MTBI:
- Cumulative time the system is not engaged in unplanned interventions
- Function of a) the environment, b) the autonomy of the agent defined as its ability to accommodate variations in its environment in pursuit of its goals, and c) the inherent component and system reliabilities.
- Analogous to Mean Time Between Failures

MTCI:
- Cumulative time spent by humans engaging in unplanned interventions
- Function of a) the nature of the failure, b) the design parameters describing the cognitive capabilities of agents - amount of information available to agents about the failure, the amount and type of information transfered between agents - c) the physical distance between the agents, d) the lag in communication, e) the available resources and tools
- Analogous to Mean Time To Repair

Simulation experiments show that:
- Increasing MTCI decreases the amount of time for other work, and the plateau of that time as a function of MTBI happens later ={\textgreater} low MTCI leads to more time to do other work.
- As MTBI increases, the probability of intervention decreases.
- Increasing MTCI by an order of amgnitude shifts the MTBI-Probability curve up ={\textgreater} the probability of intervention for a given MTBI increases
- As the frequency of interventions decreases (higher MTBI), the sensitivty of human performance to higher/lower MTCI decreases.
- Sensitivity of time for other work to MTBI increases as MTCI increases ={\textgreater} increasing duration of interventions creates greater sensitivity of time available for more work to the frequency of the interventions.
- Probability of intervention is 3 orders of magnitude more sensitive to MTBI than MTCI

Conclusions:
- Designer may be able to impact system performance by improving MTBI in the face of uncertain MTCI
- Good news because MTBI is largely a function of factors the designer can control
- Also, accurately assessing MTCI need not be necessary.},
author = {Shah, Julie A. and Saleh, Joseph H. and Hoffman, Jeffrey A.},
doi = {10.1016/j.ress.2007.06.007},
issn = {09518320},
journal = {Reliability Engineering {\&} System Safety},
month = {aug},
number = {8},
pages = {1280--1286},
title = {{Analytical basis for evaluating the effect of unplanned interventions on the effectiveness of a human–robot system}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0951832007001627},
volume = {93},
year = {2008}
}
@incollection{Kirchner2014,
annote = {Proposal for a system architecture, later implemented in the Dominik Kirchner thesis, for diagnosis and recovery in a multi-agent system.

- Just like monitoring plugins, nodes can have specific recovery plugins
- Diagnosis is done using a DBN
- Selecting a recovery is done based on recoveries specified using a MAS programming language (ALICE).},
author = {Kirchner, Dominik and Niemczyk, Stefan and Geihs, Kurt},
booktitle = {RoboCup 2013: Robot World Cup XVII},
doi = {10.1007/978-3-662-44468-9_27},
edition = {Lecture No},
isbn = {978-3-662-44468-9},
pages = {304--315},
publisher = {Springer, Berlin, Heidelberg},
title = {{RoSHA: A Multi-robot Self-healing Architecture}},
url = {http://link.springer.com/10.1007/978-3-662-44468-9{\_}27},
year = {2014}
}
@inproceedings{nushi2018towards,
annote = {Another presentation of the previous Nushi et al. work. The focus for this one is on the "views" of the system that are available to evaluate the performance - Content View (when does the system fail), and Component View (how does the system fail).

The paper provides methodologies for presenting the data on fixes learned by the previous Nushi paper to system designers in order to best aid in system improvement.},
author = {Nushi, Besmira and Kamar, Ece and Horvitz, Eric},
booktitle = {Sixth AAAI Conference on Human Computation and Crowdsourcing},
title = {{Towards accountable AI: Hybrid human-machine analyses for characterizing system failure}},
url = {https://www.aaai.org/ocs/index.php/HCOMP/HCOMP18/paper/view/17930/16917},
year = {2018}
}
@inproceedings{she2016incremental,
annote = {Deals with the problem of grounding language to robot action.

Insight: the goal state of the world is the driving force of the robot's actions. Thus modeling verb semantics through their effects on the state of the world can provide a link between high level language and low level primitive actions.

Insight: verbs are represented through a hypothesis space of fluents describing the world. Demonstrations prune these fluents.

Contributions:
- Can capture a "space" (instead of state) when an action (verb) is applicable
- In new situations, can identify the best hypotheses that apply and plan accordingly
- Has the potential for life-long learning

Insight from previous work: there are two types of verbs - manner verbs and result verbs. This work focuses on result verbs and uses hypothesis spaces to represent their result states.

Figure 1 includes a system diagram of the system that I want to build. However, I am more interested in adding both remote and local to that system diagram, as well as arbitrating between them.

Hypothesis space: specific-to-general hierarchy of fluents. Use heuristics to improve upon the set of fluents that are applicable to a given hypothesis. Then use more heuristic requirements, such as consistency and merging, to infer what can be done in different hypothesis spaces.

Use the dataset from Misra et. al '15. Use metrics of Instruction Edit Distance and State Jaccard Index. As baselines, used their own hypothesis selector with those from Misra, Memory, Most General (heuristic), and Most Frequent (heuristic).

Data shows the hypothesis space based method of action selection is superior to others.},
author = {She, Lanbo and Chai, Joyce},
booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
pages = {108--117},
title = {{Incremental acquisition of verb hypothesis space towards physical world interaction}},
url = {http://www.aclweb.org/anthology/P16-1011},
volume = {1},
year = {2016}
}
@inproceedings{choi2017optimal,
annote = {Introduces the concept of "Sentential Decision Diagrams" (SDD) as an efficient manner of calculating expectations on SDP required for the Chen et al 2015 (SDP) paper. Also posits that SDDs can be used directly for the feature (test) selection problem instead of being used only as an intermediary to calculate the SDP objective.},
author = {Choi, YooJung and Darwiche, Adnan and den Broeck, Guy},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI)},
title = {{Optimal feature selection for decision robustness in Bayesian networks}},
url = {https://par.nsf.gov/servlets/purl/10053970},
year = {2017}
}
@inproceedings{SzeZhengYong2017,
annote = {Formulates the active learning problem from Javdani et al. as an active diagnosis problem. Then provides theoretical guarantees to group-based active diagnosis. In this paper, they show that the proble of active diagnosis with persistent faults (and not intermittent ones) is akin to a problem of group-based active diagnosis.

Formulation:
- Estimate a discrete state of the system and not necessarily the exact fault
- The system consists of states (X) and sensor modes (Q); the hypothesis (in Golovin et al jargon) is a pair of (X,Q), and the system state, X, is the diagnosis group.
- "Diagnosis" in this case refers to the state of the system, regardless of the state of the fault. This is a state estimation in the face of faulty sensors problem.
- Also, they present an algorithm for estimating the submodularity parameter, which has no effect on the actual diagnosis algorithm.

Part of the actions available to the policy is opening/closing contactors and reading the sensor values},
author = {{Sze Zheng Yong} and {Lingyun Gao} and Ozay, Necmiye},
booktitle = {2017 American Control Conference (ACC)},
doi = {10.23919/ACC.2017.7963340},
isbn = {978-1-5090-5992-8},
month = {may},
pages = {2574--2581},
publisher = {IEEE},
title = {{Weak adaptive submodularity and group-based active diagnosis with applications to state estimation with persistent sensor faults}},
url = {http://ieeexplore.ieee.org/document/7963340/},
year = {2017}
}
@article{johnson2013bayesian,
annote = {Extends the HDP-HMM that was used in segmentation to use HDP-HSMM. By using HSMM and directly modeling the state duration, they are able to work around the problem of too many segments that HDP-HMM faced.

In order to train the HDP-HSMM, they had to create a new Gibbs sampling procedure. Luckily the code is available on Github (linked).},
author = {Johnson, Matthew J and Willsky, Alan S},
journal = {Journal of Machine Learning Research},
number = {Feb},
pages = {673--701},
title = {{Bayesian nonparametric hidden semi-Markov models}},
url = {https://github.com/mattjj/pyhsmm http://www.jmlr.org/papers/volume14/johnson13a/johnson13a.pdf},
volume = {14},
year = {2013}
}
@inproceedings{Morais2015,
annote = {Proof of concept of using an external robot to help in the diagnosis of the fault in another (in order to select the appropriate recovery strategy; though this is not done in the paper). The paper focuses on the development of a coordination strategy for improving diagnosis; not on the recovery plans that are then deployed.},
author = {Morais, Marcio G. and Meneguzzi, Felipe R. and Bordini, Rafael H. and Amory, Alexandre M.},
booktitle = {2015 International Conference on Advanced Robotics (ICAR)},
doi = {10.1109/ICAR.2015.7251486},
isbn = {978-1-4673-7509-2},
month = {jul},
pages = {395--400},
publisher = {IEEE},
title = {{Distributed fault diagnosis for multiple mobile robots using an agent programming language}},
url = {http://ieeexplore.ieee.org/document/7251486/},
year = {2015}
}
@inproceedings{Yong2018,
annote = {Adds on the problem of modeling stochastic/non-persistent fault to their previous active bayesian diagnosis problem.

In the case of stochastic faults, repeated actions may not lead to the same result/test output. Hence, this is not a simple extension of the persistent noise case, because repeated actions incur cost. This introduces an exploration/exploitation tradeoff.

Formalization:
- Introduce another variable in the mix - W to model the stochastic noise state. W is a time-indexed vector
- The full hypothesis set is X x Q x W{\^{}}T
- The problem is that of isolating the discrete state X

Not really sure how the algorithm operates or how it is different from the Golovin et al prior works.},
author = {Yong, Sze Zhang and Ozay, Necmiye},
booktitle = {2018 Annual American Control Conference (ACC)},
doi = {10.23919/ACC.2018.8431639},
isbn = {978-1-5386-5428-6},
month = {jun},
pages = {313--320},
publisher = {IEEE},
title = {{Discrete State Estimation with Persistent Sensor Faults and Non-Persistent Noise via Noisy Bayesian Active Diagnosis}},
url = {https://ieeexplore.ieee.org/document/8431639/},
year = {2018}
}
@article{Pattipati1990,
annote = {Formulates a sequential diagnostic problem where the problem of deciding which test to conduct in order to diagnose faults is formulated as an MDP. The cost of each step is formulated as some combination of a Huffman-code of the test results + some entropy measure. (Need more details).

The goal of this work seems to be to show that solving the MDP using AND-OR search and the cost metric defined with entropy is possible in finite time.},
author = {Pattipati, Krishna R. and Alexandridis, M.G.},
doi = {10.1109/21.105086},
issn = {00189472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
number = {4},
pages = {872--887},
title = {{Application of heuristic search and information theory to sequential fault diagnosis}},
url = {http://ieeexplore.ieee.org/document/105086/},
volume = {20},
year = {1990}
}
@inproceedings{Laursen2015,
annote = {Create reversible programs to automatically attempt error recoveries for certain classes of errors. Coupled with the Robotica journal paper.},
author = {Laursen, Johan Sund and Schultz, Ulrik Pagh and Ellekilde, Lars-Peter},
booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2015.7353609},
isbn = {978-1-4799-9994-1},
month = {sep},
pages = {1785--1792},
publisher = {IEEE},
title = {{Automatic error recovery in robot assembly operations using reverse execution}},
url = {http://ieeexplore.ieee.org/document/7353609/},
year = {2015}
}
@article{Crestani2015,
annote = {(According to the survey - proposes to develop dedicated monitors for each software component in mobile robots)

Many challenges to creating fault tolerant robots:
- No accepted standard of concepts, design methods, and terms
- Complex, dynamic, unstructured environments present a wide variety of faults
- Need to identify *relevant* faults that are most crucial for robot behavior in the given task and context
- Severity of failure must be linked to performance criteria
- Faults must generally be handled in real-time
- Lack of a global fault-tolerant approach

Current fault tolerance approaches might not work because:
- Difficult to ensure that all proposed fault tolerance approaches can run in real time
- Data based detection might not be suitable, especially when we have the robot model (aside: this is absurd)
- Traditional fault tolerance methods generally don't consider high level knowledge and/or context for faults

Three basic mechanisms proposed to perform FDD:
- Timing checks - supervise a robot's liveness using timers and watchdogs
- Reasonableness checks - verify the correctness of system variables based on algorithm constraints and other specs.
- Monitoring for diagnosis - model-based approaches to observed behaviour and parameter estimate predictions; differences between measured and estimated outputs are used to generate residuals that can then be used to detect and diagnose faults.
The work quotes a multitude of methods that apply different models to different robots to detect different faults.

Recovery can happen through:
- Execution control - often reactive means of dealing with failures during the monitoring of plan execution
- Replanning/Plan-repair - high-level and time-consuming fault handling.

Authors' solution

1) Conduct an FMEA/FMECA to determine failures of subsystems and their failure modes according to the "function" affected:
- There are 2 classes of failure modes - Complete (funcionality completely lost) and Partial
- There are 2 Types of failure modes - Permanent and Transient
- Use an Ishikawa diagram to identify the potential domains of failure: Sensor, Actuator, Energy (energy devices and management), Design (design or control alg. implementation), Environment (perception error), Control (software management, e.g. scheduling, problem)
- Four levels of failure severity based on available recovery strategies: Weak (adaptation of sub-objective params needed), Medium (needs different sub-objectives, but not necessarily different level of autonomy), Serious (needs a change in level of autonomy), Fatal (mission needs to be aborted)
- Create the table and make sure to include plans for detection and recovery in the table. Also include "Fault identifiers"

2) Based on the FMECA table, implement the different monitors and detectors for the identified failures

3) Diagnosis of faults should be based on residuals and incidence matrix
- A fault is defined by a vector of residuals - difference between measured values and model-based expected values
- Create a table correlating residuals to known identifiers of faults; called an incidence matrix. Each column in the matrix is a "fault signature"
- Signatures can be strongly isolating or weakly isolating. Using a set of rules, one should be able to isolate the cause of the failure.
- Based on identified causes, a "module state" database can keep information about the modules in action to help make recovery decisions.

3) Recovery solution generally depends on the severity of the identified fault. At a high level, the options available are:
- Reconfiguration - severity: weak.
- Adaptation - severity: medium.
- Autonomy Adjustment - severity: serious and communication available
- Safety Wait - severity: serious and communication unavailable
- Definitive Stop - severity: fatal

Architecture to implement this solution called COTAMA (Contextual Task Management). Some notes:
- Contains an FMECA database
- Contains detectors which feed to diagnosis
- Recovery happens though Global supervisor, Local supervisor, and Adaptation supervisor based on events from modules and/or events from a Recovery Supervisor.
- Recovery supervisor is responsible for triggering recovery based on state information in the FMECA database
- Global supervisor is a mission supervisor implemented as a Petri-Net

Contributions:
- Use FMECA to forecast faults. Also use the analysis on the robot
- Detection is informed by the forecasting
- Diagnosis through traversing fault signature and residuals. Allows for multiple fault diagnosis.
- Recovery strategies are implemented online; including asking humans for help

Drawbacks:

- FMECA is long and tedious
- Detection could take a long time
- Replanning is not an implemented recovery strategy yet.},
author = {Crestani, D. and Godary-Dejean, K. and Lapierre, L.},
doi = {10.1016/j.robot.2014.12.015},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
month = {jun},
pages = {140--155},
title = {{Enhancing fault tolerance of autonomous mobile robots}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889014003157},
volume = {68},
year = {2015}
}
@article{Johnson2014,
annote = {The goal is to make robots effective teammates in interdependent tasks. Prior works have proposed a bunch of terms with low operational value, eg. shared cooperative activity, lists of characteristics, etc. This work will fix that through the use of coactive design. The attributes to optimize are observability, predictability, and directability.

Coactive Design - Approach to designing HRI that takes interdependence between robot and human as the central organizing principle. Assumption - robots and humans are engaged in a "joint activity", and coactive design helps identify interdependence relationships to effectively accomplish the joint activity.

Interdependence:
- Set of complementary relationships that two or more parties rely on to manage dependencies, hard or soft, in a joint activity.
- Capacity is the total set of inherent attributes an agent needs to perform an activity individually
- Dependence exists when an entity lacks the capacity to perform an activity on its own

Attributes of interdependence
- Relationships between agents MUST be complementary
- Hard interdependence stems from a lack of capacity
- Soft interdependence is optional and opportunistic than strictly required
- Interdependence provides a common context and grounding for ALL agents

Determining Interdependence:
- Participants in a joint activity have additional requirements beyond task work. In coactive design, these requirements concern Observability, Predictability, and Directability (OPD)
- OPD should shape the design of the UI of the human operator, and the autonomy of the robot
- Observability - making pertinent aspects of one's status observable to others.
- Predictability - agent's actions should be predictable so that others can rely on predictions to make decisions
- Directability - agent's ability to direct the behaviour of others and to be directed.
- Beyond perception and cognition, many other "processes" can benefit from support.

The method:
- Identification process: input - task analysis, composition of team, situation alternatives; output - description of interdependencies and OPD requirements for all parties
- Selection and implementation: for each interdependence relationship (above), implement it to adhere to identified OPD levels
- Evaluation: Reiterate previous if new OPD and interdependencies. Else traditional task analyses.

Tools:
- Interdependence Analysis Table - Identify required capacities and asses capacity-to-perform and capacity-to-support
- Based on IA Table, can figure out interdependence relationships, OPD requirements (Fig 8, 9).},
author = {Johnson, Matthew and Bradshaw, Jeffrey M. and Feltovich, Paul J. and Jonker, Catholijn M. and {Van Riemsdijk}, M. Birna and Sierhuis, Maarten},
doi = {10.5898/JHRI.3.1.Johnson},
issn = {2163-0364},
journal = {Journal of Human-Robot Interaction},
month = {mar},
number = {1},
pages = {43},
title = {{Coactive Design: Designing Support for Interdependence in Joint Activity}},
url = {http://dl.acm.org/citation.cfm?id=3109825},
volume = {3},
year = {2014}
}
@article{Wilde2019,
annote = {Task constraints and their weighting can sometimes be hard to specify. We can use comparison-based learning where the weighting of the costs is learned iteratively as users indicate a preference of one task plan over another. Task plans in this case are robot paths through a map.

In this work, the plan of the robot (I think) determines the equivalence class while the weights of the users themselves determine the specific objects in the equaivalence class that we don't care about

Compare against Sadigh, Dragan, et al. preference learning work to show that posterior certinainty of weights is improved by this method.

Introduce a sampling based algorithm to solving the Equivalence Class Decision problem. Should evaluate as a possible alternative.},
author = {Wilde, Nils and Kulic, Dana and Smith, Stephen L.},
doi = {10.1109/LRA.2019.2897342},
issn = {2377-3766},
journal = {IEEE Robotics and Automation Letters},
month = {apr},
number = {2},
pages = {1691--1698},
title = {{Bayesian Active Learning for Collaborative Task Specification Using Equivalence Regions}},
url = {https://ieeexplore.ieee.org/document/8633961/},
volume = {4},
year = {2019}
}
@inproceedings{Wu2018,
annote = {Learn to recover from errors by replaying failed skill. If the failed skill has a dependency, then replay the dependency(-ies) until there is a skill that has no dependency.

The dependencies are annotated manually.

(The linked ResearchGate paper is most likely a pre-print of the RO-MAN paper)},
author = {Wu, Hongmin and Luo, Shuangqi and Lin, Hongbin and Duan, Shuangda and Guan, Yisheng and Rojas, Juan},
booktitle = {2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},
doi = {10.1109/ROMAN.2018.8525771},
isbn = {978-1-5386-7980-7},
month = {aug},
pages = {166--173},
publisher = {IEEE},
title = {{Recovering from External Disturbances in Online Manipulation through State-Dependent Revertive Recovery Policies}},
url = {https://ieeexplore.ieee.org/document/8525771/ https://www.researchgate.net/profile/Juan{\_}Rojas2/publication/318849263{\_}Anytime{\_}Anywhere{\_}Anomaly{\_}Recovery{\_}through{\_}an{\_}Online{\_}Robot{\_}Introspection{\_}Framework/links/59a51e2045851570311afe17/Anytime-Anywhere-Anomaly-},
year = {2018}
}
@article{Pettersson2007,
annote = {Implementation of hierarchical neural networks for fault detection and fault isolation.

- Use RBF networks and the activation of behaviours as input (alongwith feature extraction) to both detect faults and isolate them
- Problem of detecting a closed door (and if the door is closed or not)
- Model-free implementation of both detection and diagnosis
- Characterize the system on reaction speed, robustness, and isolation performance.
- Able to transfer models from simulation to the real robot.
- Only have three classes for isolation.

This is essentially a problem of inferring a failed belief detector based on time series characteristics across multiple series in a window.},
author = {Pettersson, Ola and Karlsson, L. and Saffiotti, A.},
doi = {10.1109/TSMCB.2007.895359},
issn = {1083-4419},
journal = {IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)},
month = {aug},
number = {4},
pages = {890--901},
title = {{Model-Free Execution Monitoring in Behavior-Based Robotics}},
url = {http://ieeexplore.ieee.org/document/4267877/},
volume = {37},
year = {2007}
}
@article{Prorok2017,
annote = {Assume:
- Tasks are defined according to skill sets
- Robots belong to sets of species
- Species have traits that can fulfill parts of the task specification
- Robots know: a) how tasks are connected (ordering of tasks), b) their task allocation, c) know the target trait distribution

Goal:
- Reallocate the robots of different species so that the desired trait distribution for the task is reached.
- Metric to minimize: the fraction of misallocated traits.

Details are in scanned highlights

Results:
- The more restrictive the communication topology, the harder it is to correctly allocate
- The more diverse the community, the harder it is to optimize. It's preferable to have species with redundant traits.},
author = {Prorok, Amanda and Hsieh, M. Ani and Kumar, Vijay},
doi = {10.1109/TRO.2016.2631593},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
month = {apr},
number = {2},
pages = {346--358},
title = {{The Impact of Diversity on Optimal Control Policies for Heterogeneous Robot Swarms}},
url = {http://ieeexplore.ieee.org/document/7819471/ https://drive.google.com/open?id=1p2vDCl0oWhdKLyGIJlKX9yJxQ1Fo3TqI},
volume = {33},
year = {2017}
}
@article{Zhou2015,
annote = {Particle filter based tracking of state / belief that can then be used for fault diagnosis when the situation calls for it.

Differentiates between "static" diagnosis vs. "dynamic" diagnosis. Static diagnosis checks the consistency between system constraints and a set of observations. Dynamic diagnosis extends the check to consistency checks in the state evolution over time.

Most common methods of dynamic diagnoses involve belief state estimation and tracking. Common problems that such methods need to overcome:
- Number of possible belief trajectories becomes too large
- Generally need some sort of preference criterion.

Contribution:
- Track belief states through the use of particle filters.
- Particles are of constant (unit) weight and weights are not updated, only the distribution of particles
- Use an uncertainty labeled planning graph to ground the particle filter estimator
- Particles are uniquely tagged to make the trajectory reconstruction easier during diagnosis.},
author = {Zhou, Gan and Feng, Wenquan and Zhao, Qi and Zhao, Hongbo},
doi = {10.3390/s151128031},
issn = {1424-8220},
journal = {Sensors},
month = {nov},
number = {11},
pages = {28031--28051},
title = {{State Tracking and Fault Diagnosis for Dynamic Systems Using Labeled Uncertainty Graph}},
url = {http://www.mdpi.com/1424-8220/15/11/28031},
volume = {15},
year = {2015}
}
@article{Nielsen2013,
abstract = {When using Bayesian networks for modelling the behavior of man-made machinery, it usually happens that a large part of the model is deterministic. For such Bayesian networks deterministic part of the model can be represented as a Boolean function, and a central part of belief updating reduces to the task of calculating the number of satisfying configurations in a Boolean function. In this paper we explore how advances in the calculation of Boolean functions can be adopted for belief updating, in particular within the context of troubleshooting. We present experimental results indicating a substantial speed-up compared to traditional junction tree propagation.},
annote = {Using Bayes Nets and boolean logic to represent the relationships between system components that can fail as a result of other components and other components that are "root causes". Uses Reduced Order Binary Decision Diagrams (ROBDDs) to update beliefs over the Bayes Net in linear time so that the system model can be used to "troubleshoot" the root cause given the evidence.},
archivePrefix = {arXiv},
arxivId = {1301.3880},
author = {Nielsen, Thomas D. and Wuillemin, Pierre-Henri and Jensen, Finn Verner and Kj{\ae}rulff, Uffe},
eprint = {1301.3880},
month = {jan},
title = {{Using ROBDDs for Inference in Bayesian Networks with Troubleshooting as an Example}},
url = {http://arxiv.org/abs/1301.3880},
year = {2013}
}
@article{Miller2017,
abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
annote = {Notes in the associated Google Doc},
archivePrefix = {arXiv},
arxivId = {1706.07269},
author = {Miller, Tim},
eprint = {1706.07269},
month = {jun},
title = {{Explanation in Artificial Intelligence: Insights from the Social Sciences}},
url = {http://arxiv.org/abs/1706.07269 https://docs.google.com/document/d/1SLT1B5lScWPkp6Kt4nTruPykkTEbDyJ7tJmJ6gNjp98/edit},
year = {2017}
}
@inproceedings{Bhattacharjya2007,
abstract = {Although a number of related algorithms have been developed to evaluate influence diagrams, exploiting the conditional independence in the diagram, the exact solution has remained intractable for many important problems. In this paper we introduce decision circuits as a means to exploit the local structure usually found in decision problems and to improve the performance of influence diagram analysis. This work builds on the probabilistic inference algorithms using arithmetic circuits to represent Bayesian belief networks [Darwiche, 2003]. Once compiled, these arithmetic circuits efficiently evaluate probabilistic queries on the belief network, and methods have been developed to exploit both the global and local structure of the network. We show that decision circuits can be constructed in a similar fashion and promise similar benefits.},
annote = {Introduction to Decision Circuits that are supposedly a very efficient manner of reasoning about an asymmetric decision problem MDP.},
archivePrefix = {arXiv},
arxivId = {1206.5257},
author = {Bhattacharjya, Debarun and Shachter, Ross D.},
booktitle = {UAI},
eprint = {1206.5257},
month = {jun},
title = {{Evaluating influence diagrams with decision circuits}},
url = {http://arxiv.org/abs/1206.5257},
year = {2007}
}
@article{Knepper2015,
annote = {TODO: Scan notes from the paper

- Use a CRF to ground action specifiers to utterances when asking for help
- The factors used in the CRF are able to better take into account the person's ability to understand the request(s) for help
- The above leads to better performance in a real robot task, as well as better subjective metrics of participant perceptions of the robot and its utterances.},
author = {Knepper, Ross A. and Tellex, Stefanie and Li, Adrian and Roy, Nicholas and Rus, Daniela},
doi = {10.1007/s10514-015-9460-1},
issn = {0929-5593},
journal = {Autonomous Robots},
month = {oct},
number = {3},
pages = {347--362},
title = {{Recovering from failure by asking for help}},
url = {http://link.springer.com/10.1007/s10514-015-9460-1},
volume = {39},
year = {2015}
}
@article{Khalastchi2015,
annote = {Fault detection based on Mahalanobis based distance metrics and clustering. Shows that such detection outperforms current state of the art anomaly detection methods. Not a diagnosis method.},
author = {Khalastchi, Eliahu and Kalech, Meir and Kaminka, Gal A. and Lin, Raz},
doi = {10.1007/s10115-014-0754-y},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
month = {jun},
number = {3},
pages = {657--688},
title = {{Online data-driven anomaly detection in autonomous robots}},
url = {http://link.springer.com/10.1007/s10115-014-0754-y},
volume = {43},
year = {2015}
}
@article{Bellala2012,
annote = {There are M objects and N subsets of these objects called queries. The goal is to find the unknown object in the least amount of queries. This paper looks at numerous variations of the problem and formulates algorithms for solving them.

Assume: unique identifiability for every object. This implies there exists a set of tests that distinguishes any 2 objects. (such an assumption might not be true unless humans exist)

For all algorithms, if cost of each query is unequal, then ensure that the picking happens by a cost that is normalized by the cost of each query = delta(q) / C(q)

Generalized Binary Search:
- the simplest manifestation of the problem
- minimize a metric called "reduction factor" $\backslash$in [0.5, 1] which is an approximation of the number of queries needed to identify an object.
- minimizing reduction factor ={\textgreater} choosing balanced splits.

Group Identification:
- objects belong to disjoint groups and that overlapping groups reduces to disjoint groups (not true according to Javdani)
- groups != meta-objects because test results can differ for objects within groups
- include a group reduction factor to ensure that leaves of decision tree only have one group
- algorithm called Group Identification Splitting Alg (GISA)
- objective function of GISA is not adaptive submodular; so use an approximated weighted entropy term in an algorithm called modified GISA
- modified GISA has tighter perf. bounds than the algorithm of Golovin et al. (don't know why)

Group Queries:
- queries are grouped into n disjoint categories. these categories need not be disjoint, though the paper analyzes them as such
- by presenting the user with a query group, we allow more flexibility to the user, but the performance may be suboptimal.
- we require a priori probabilities of users selecting any given test within a group
- algorithm utilizes the expectation of users selecting a query within a group to determine the query set to present
- called Group Queries Splitting Algorithm (GQSA)

Group Identification under Group Queries:
- combine GISA and GQSA into GIGQSA
- mentioned in the paper for completeness only; no tests were performed or results shown

Persistent Noise Identification:
- persistent noise is a stringent noise model where repeated queries yield the same result
- assume minimum hamming distance between objects, and a maximum number of errors/noise in queries
- requires knowledge of the probability of queries being in error
- given the above, augment the query to object association matrix with all possible noisy permutations of the object belonging to the same group.
- the augmentation does require recalculating the prior probabilities of the different objects
- the exhaustive enumeration can supposedly be avoided, but the weighted graph formulation of Golovin et al. seems cleaner.},
author = {Bellala, Gowtham and Bhavnani, Suresh K. and Scott, Clayton},
doi = {10.1109/TIT.2011.2169296},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
month = {jan},
number = {1},
pages = {459--478},
title = {{Group-Based Active Query Selection for Rapid Diagnosis in Time-Critical Situations}},
url = {http://ieeexplore.ieee.org/document/6121981/ https://www.researchgate.net/profile/Suresh{\_}Bhavnani/publication/220685228{\_}Group-Based{\_}Active{\_}Query{\_}Selection{\_}for{\_}Rapid{\_}Diagnosis{\_}in{\_}Time-Critical{\_}Situations/links/00463515627565a277000000.pdf https://drive.goo},
volume = {58},
year = {2012}
}
@inproceedings{Rojas2017,
annote = {(possibly a citation in the Wu et al. 2017 paper)

- The introduction has a good overview of the different means of approaching anomaly detection in contact manipulation tasks.
- Assume that action sequences can be encoded semantically as sub-skills and these subskills can then be strung together as words to define a task/sub task.
- Anomaly detection on strings of words can be useful for high-level planners.
- Moving up into a semantic hierarchy for the skill classification also results in dimensionality reduction that is useful from a computational perspective.

Evaluation:
- 38 nominal executions, 38 abnormal ones
- 4 skills in each trial

This forms the basis for later work on recoveries, I believe. Also, this builds off a "Snap assemblies" paper by the same authors that is the progenitor for the Wu et al. work too.},
author = {Rojas, Juan and Luo, Shuangqi and Zhu, Dingqiao and Du, Yunlong and Lin, Hongbin and Huang, Zhengjie and Kuang, Wenwei and Harada, Kensuke},
booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2017.8206438},
isbn = {978-1-5386-2682-5},
month = {sep},
pages = {5429--5436},
publisher = {IEEE},
title = {{Online robot introspection via wrench-based action grammars}},
url = {http://ieeexplore.ieee.org/document/8206438/},
year = {2017}
}
@article{Noshirvani2018,
annote = {Use extensions to UKF based parameter estimation from previous work to detect sensor and actuator faults. Then cleverly generate residuals so that it is easy to isolate the fault based on if-else conditions on the exact residuals that are active at the given moment.},
author = {Noshirvani, Gholamreza and Askari, Javad and Fekih, Afef},
doi = {10.1002/etep.2625},
issn = {20507038},
journal = {International Transactions on Electrical Energy Systems},
month = {nov},
number = {11},
pages = {e2625},
title = {{A robust fault detection and isolation filter for the pitch system of a variable speed wind turbine}},
url = {http://doi.wiley.com/10.1002/etep.2625},
volume = {28},
year = {2018}
}
@inproceedings{Chen2016,
annote = {From Dave: "Solving multiple sub-MDPs could be similar to this..."

POMDP situations - the robot needs to gather information about unknown quantities from noisy observations while achieving the task objective at the same time.

POMDP-lite - a formulation of a POMDP as a set of MDPs each of which is indexed by hidden paramaters (that must be estimated through observation). The POMDP state/obs uncertainty is captured by the transition dynamics of the MDP. By maintaining a belief distribution over the hidden parameters and following model-based RL techniques for the set of MDPs, a near-optimal solution can be found. TLDR: the hidden parameter MUST be static, otherwise you're stuck with general POMDPs.},
author = {Chen, Min and Frazzoli, Emilio and Hsu, David and Lee, Wee Sun},
booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487754},
isbn = {978-1-4673-8026-3},
month = {may},
pages = {5427--5433},
publisher = {IEEE},
title = {{POMDP-lite for robust robot planning under uncertainty}},
url = {http://ieeexplore.ieee.org/document/7487754/},
year = {2016}
}
@article{Zhang2018b,
annote = {Millisecond level coupled fault isolation or high-speed trains. Requires MIMO specification of the dynamical system, and some other controls magic that I don't really care about atm.

References the Zhang and Pattipati work.},
author = {Zhang, Kunpeng and Jiang, Bin and Tao, Gang and Chen, Fuyang},
doi = {10.1109/TCST.2017.2735360},
issn = {1063-6536},
journal = {IEEE Transactions on Control Systems Technology},
month = {sep},
number = {5},
pages = {1552--1566},
title = {{MIMO Evolution Model-Based Coupled Fault Estimation and Adaptive Control With High-Speed Train Applications}},
url = {https://ieeexplore.ieee.org/document/8032486/},
volume = {26},
year = {2018}
}
@inproceedings{Pattipati1994,
annote = {TEAMS is a software developed by Pattipati et al. and in use at NASA (according to a 2012 report, linked).

"... the system is modeled in the failure space. Thus, the model does not describe how the system normally functions, but how the effect of the various failure sources propagate to the monitoring points. The simplicity in this modeling approach allows: (a) modeling of systems at various stages in its design manufacturing and maintenance cycles, (b) modeling and analysis of large systems, and (c) modeling to be technology (i.e., electrical, mechanical, etc.) independent...

... A hierarchical dependency (similar to digraph, or information flow) graph captures the first-order causeeffect relationships between modules and test points. Modules may have (sub)modules (i.e., an embedded dependency subgraph) or be a submodule of a larger system. Thus, a system can have multiple levels of hierarchy. The modules are the failure sources, or the causes. A test node denotes a monitoring point, where the effects are observable. A link between two nodes A and B denotes that A affects B, or B depends on A. Higher-order dependencies can be inferred from these first-order dependencies. The redundancy for fault-tolerance, which may hide the failure of a component, is modeled as an AND node. An AND node with M-out-of-N redundancy indicates that at least M of its inputs must fail for the output of the AND node to fail. Switches model the various modes of system operation. In addition, they can be used to functionally isolate modules or break feedback loops in test mode to improve the testability of a system.},
author = {Pattipati, K.R. and Raghavan, V. and Shakeri, M. and Deb, S. and Shrestha, R.},
booktitle = {Proceedings of 1994 American Control Conference - ACC '94},
doi = {10.1109/ACC.1994.752424},
isbn = {0-7803-1783-1},
pages = {1989--1995},
publisher = {IEEE},
title = {{TEAMS: Testability Engineering and Maintenance System}},
url = {http://ieeexplore.ieee.org/document/752424/ https://www.teamqsi.com/tag/industrial/ https://www.teamqsi.com/solutions/overview/},
volume = {2},
year = {1994}
}
@inproceedings{Fox2008,
address = {New York, New York, USA},
annote = {The HMM version (precursor) of the HDP-HSMM work by Johnson et al. I think this is also the backing algorithm for the Butterfield segmentation work.},
author = {Fox, Emily B. and Sudderth, Erik B. and Jordan, Michael I. and Willsky, Alan S.},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390196},
isbn = {9781605582054},
pages = {312--319},
publisher = {ACM Press},
title = {{An HDP-HMM for systems with state persistence}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390196},
year = {2008}
}
@inproceedings{Haste2018,
annote = {(File 2 in the zip)

Presents a system to do Fault Tree Analysis and Response in a closed loop. Method of monitoring the timing and effects of both those in an integrated manner.},
author = {Haste, Deepak and Ghoshal, Sudipto and Johnson, Stephen B. and Moore, Craig},
booktitle = {AAAI Fall Symposium Series},
title = {{Model-based Design Assessment and Fault Management Visualization Technologies – A Feasibility Study}},
url = {http://makro.ink/sip/sip18/proceedings.zip https://www.teamqsi.com/},
year = {2018}
}
@inproceedings{fox2009sharing,
annote = {Underlying algorithm for Niekum et al.'s segmentation work.},
author = {Fox, Emily and Jordan, Michael I and Sudderth, Erik B and Willsky, Alan S},
booktitle = {Advances in Neural Information Processing Systems},
pages = {549--557},
title = {{Sharing features among dynamical systems with beta processes}},
year = {2009}
}
@article{Zhi-JieZhou2013,
annote = {Create an expert system called "Belief Rule Base" that can predict hidden behaviour of a system from observed data based on expert rules. This is appropriate for what the paper calls "hybrid data", where qualitative knowledge of the system and quantitative data from executions are both available.

Rules can be:
- specified by experts
- extracted from prior knowledge-bases
- extracted from historical data
- randomly initialized

This method works by assigning weights to the rules (based on observed data) to infer the hidden data. Since it works on rules, it can be non-Markovian(!). Also, by working with rules, a modicum of interpretability to the method is possible.

Experts can initialize weights to how important rules are, but if they are wrong, then there is a way to relearn the true weights based on data (most of the paper's contents).},
author = {{Zhi-Jie Zhou} and {Chang-Hua Hu} and {Bang-Cheng Zhang} and {Dong-Ling Xu} and {Yu-Wang Chen}},
doi = {10.1109/TSMCB.2012.2208266},
issn = {2168-2267},
journal = {IEEE Transactions on Cybernetics},
month = {apr},
number = {2},
pages = {402--411},
title = {{Hidden Behavior Prediction of Complex Systems Based on Hybrid Information}},
url = {http://ieeexplore.ieee.org/document/6268355/},
volume = {43},
year = {2013}
}
@article{Dong2007,
annote = {Working on the problem of using condition-based maintainance (CBM) using HSMM. Explains that CBM requires the need for diagnosis and prognosis. And that prognosis requires reasoning about time, as well as, perhaps, a generative model to do so.

This paper uses HSMMs for sensor fusion, fault diagnosis, and maintenance prognosis. In order to train the HSMM, they also develop "new" forward-backward algorithms.

Approach:
- Each component goes through N different but sequential health states. These are the N states for the HSMM.
- Train an HSMM for each distinct fault type
- Observations are often skewed based on the sensor input. Perform an LDA to better fuse the observation data

They can achieve 95{\%} accuracy on fault isolation and prognosis of system health.},
author = {Dong, Ming and He, David},
doi = {10.1016/j.ejor.2006.01.041},
issn = {03772217},
journal = {European Journal of Operational Research},
month = {may},
number = {3},
pages = {858--878},
title = {{Hidden semi-Markov model-based methodology for multi-sensor equipment health diagnosis and prognosis}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221706001421},
volume = {178},
year = {2007}
}
@article{Koenig2017,
annote = {(Notes in notebook)

Use influence diagrams as a means to learning task models. (details are forgotten already, but the scanned notes might help).},
author = {Koenig, Nathan and Matari{\'{c}}, Maja J.},
doi = {10.1007/s10514-016-9601-1},
issn = {0929-5593},
journal = {Autonomous Robots},
month = {jun},
number = {5},
pages = {1173--1188},
title = {{Robot life-long task learning from human demonstrations: a Bayesian approach}},
url = {http://link.springer.com/10.1007/s10514-016-9601-1},
volume = {41},
year = {2017}
}
@article{Kodali2013,
annote = {Uses the same coordinate descent method (I think) described for Factorial HMMs. Here, the problem is that of diagnosing coupled faults - defined as faults that are dependent on other faults in some structural fashion (check out the paper's first few diagrams for a taxonomy).

The model is a coupled HMM where in addition to factorizing the outputs to different sequences of hidden states, the hidden sequences are also coupled. In order to deal with that approach, a mixed-memory method, mentioned by Saul {\&} Jordan, '99 is used for training and inference.

In addition to simulations on generated toy data, there is also a simulation on dependent faults arising in hybrid cars and an autonomous mobile robot.},
author = {Kodali, Anuradha and Pattipati, Krishna R. and Singh, Satnam},
doi = {10.1109/TSMCA.2012.2210405},
issn = {2168-2216},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
month = {may},
number = {3},
pages = {522--534},
title = {{Coupled Factorial Hidden Markov Models (CFHMM) for Diagnosing Multiple and Coupled Faults}},
url = {http://ieeexplore.ieee.org/document/6425502/},
volume = {43},
year = {2013}
}
@incollection{Bihlmaier2016,
annote = {A fault detection and easy diagnostics monitor in ROS Indigo. Can be referenced as another source of (network) detection information.},
author = {Bihlmaier, Andreas and Hadlich, Matthias and W{\"{o}}rn, Heinz},
booktitle = {Robot Operating System (ROS), Studies in Computational Intelligence},
doi = {10.1007/978-3-319-26054-9_25},
edition = {vol 625},
editor = {Koubaa, Anis},
pages = {651--670},
title = {{Advanced ROS Network Introspection (ARNI)}},
url = {http://link.springer.com/10.1007/978-3-319-26054-9{\_}25},
year = {2016}
}
@article{Abreu2011,
annote = {Fault Spectrum Analysis (the baseline)
- Matrix with columns denoting the components that are present in a program
- Rows are each a test case
- A 0 ={\textgreater} test case did not test component. 1={\textgreater} test case did test component
- An error vector is a vector the size of the number of test cases, with ones if the test failed, and 0 otherwise
- Diagnosis computed by trying to get a similarity score for each column with the observed vector of errors. (A prior work shows some Molecular Bio similarity score, Ochiai score, works really well).

Proposed method:
- Generate candidate multiple fault locations
- Rank the candidates in a probabilistic manner

Journal extension of the SIGSOFT paper from 2008.

Old:
Don't really know the details, but this is referenced in one of the other FDD methods (Kalech, Autonomous Robots, 2018). Basically create a diagnosis method that is better for diagnosing (mulitple) faults than all other methods.},
author = {Abreu, Rui and Zoeteweij, Peter and van Gemund, Arjan J.C.},
doi = {10.1016/j.jss.2010.11.915},
issn = {01641212},
journal = {Journal of Systems and Software},
month = {apr},
number = {4},
pages = {573--586},
title = {{Simultaneous debugging of software faults}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0164121210003183},
volume = {84},
year = {2011}
}
@article{Kawabata2003,
annote = {Feels like a precursor to most robot fault diagnosis. Attaches sensors to all components, then uses a signature matrix like object and hand-specified rules of parsing the signature matrix in order to figure out the fault that is active. Accordingly, also presents methods of continuing in the event of the fault.},
author = {Kawabata, Kuniaki and Okina, Shinnosuke and Fujii, Teruo and Asama, Hajime},
doi = {10.1163/156855303770558697},
issn = {0169-1864},
journal = {Advanced Robotics},
month = {jan},
number = {9},
pages = {925--950},
title = {{A system for self-diagnosis of an autonomous mobile robot using an internal state sensory system: fault detection and coping with the internal condition}},
url = {https://www.tandfonline.com/doi/full/10.1163/156855303770558697},
volume = {17},
year = {2003}
}
@inproceedings{Khan2018,
annote = {(Paper 3 in the linked zip file)

Uses Situation Calculus and a lot of system modeling to get at system diagnoses},
author = {Khan, Shakil M. and Soutchanski, Mikhail},
booktitle = {AAAI Fall Symposium Series2},
title = {{Diagnosis as Computing Causal Chains from Event Traces}},
url = {http://makro.ink/sip/sip18/proceedings.zip},
year = {2018}
}
@inproceedings{Lu2017,
annote = {Concrete instantiation of someone else using the FDD methodology of COTAMA architecture},
author = {Lu, Xiaojun and Faragasso, Angela and Ji, Yonghoon and Kono, Hitoshi and Yamashita, Atsushi and Asama, Hajime},
booktitle = {2017 IEEE International Conference on Real-time Computing and Robotics (RCAR)},
doi = {10.1109/RCAR.2017.8311860},
isbn = {978-1-5386-2035-9},
month = {jul},
pages = {200--205},
publisher = {IEEE},
title = {{Combining multi-localization methods for fault diagnosis in autonomous mobile robot systems}},
url = {http://ieeexplore.ieee.org/document/8311860/},
year = {2017}
}
@article{Mishra2017,
annote = {Also be Pattipati et al. - a survey of the methods necessary to provide decision support and situation awareness to remote teleoperators.

Most useful is a chart of levels of practivity:
- What do we know - data
- What happened - event
- Why did it happen - diagnosis
- What could happen - prediction
- What to do - prescription
- What does it mean - semantic embedding},
author = {Mishra, Manisha and Sidoti, David and Avvari, Gopi Vinod and Mannaru, Pujitha and Ayala, Diego Fernando Martinez and Pattipati, Krishna R. and Kleinman, David L.},
doi = {10.1109/ACCESS.2017.2707091},
issn = {2169-3536},
journal = {IEEE Access},
pages = {12475--12495},
title = {{A Context-Driven Framework for Proactive Decision Support With Applications}},
url = {http://ieeexplore.ieee.org/document/7932848/},
volume = {5},
year = {2017}
}
@article{Lee2010,
annote = {Use a multivariate process monitoring metric called the Holtelling multivariate control chart to figure out when the data is not likely to be produced from the observation probabilities that have currently been trained. Then, if the metric from that tool is above a threshold, add a new state to the HMM and retrain.},
author = {Lee, Seungchul and Li, Lin and Ni, Jun},
doi = {10.1115/1.4001247},
issn = {10871357},
journal = {Journal of Manufacturing Science and Engineering},
number = {2},
pages = {021010},
title = {{Online Degradation Assessment and Adaptive Fault Detection Using Modified Hidden Markov Model}},
url = {http://manufacturingscience.asmedigitalcollection.asme.org/article.aspx?articleid=1469989},
volume = {132},
year = {2010}
}
@article{dhillon1997safety,
annote = {Possible methods of Reliability Analysis that can be applied to robots.},
author = {Dhillon, Balbir S and Fashandi, A R M},
journal = {Robotica},
number = {6},
pages = {701--708},
publisher = {Cambridge University Press},
title = {{Safety and reliability assessment techniques in robotics}},
url = {https://www.cambridge.org/core/services/aop-cambridge-core/content/view/307CD2157CAA56B2C94640D660BBE185/S0263574797000829a.pdf/safety-and-reliability-assessment-techniques-in-robotics.pdf},
volume = {15},
year = {1997}
}
@article{Zhang2016a,
annote = {Perform condition-based monitoring by forecasting by learning the process model as an HMM. Once the process model is learned, then we can use the uncertainty in the learned parameters to guess the potential accuracy of a prognosis.

Unfortunately, the current methods of estimating HMM params using EM (Baum-Welch) do not give accurate uncertainty estimates in the parameters. So this paper introduces a new way of Gibbs sampling that should provide better uncertainty estimates in the parameters.

The motivation for why the process is modeled as an HMM is brilliant, and perhaps even better in Zhang's thesis},
author = {Zhang, Deyi and Bailey, Andrew D. and Djurdjanovic, Dragan},
doi = {10.1109/TR.2016.2570561},
issn = {0018-9529},
journal = {IEEE Transactions on Reliability},
month = {sep},
number = {3},
pages = {1471--1482},
title = {{Bayesian Identification of Hidden Markov Models and Their Use for Condition-Based Monitoring}},
url = {http://ieeexplore.ieee.org/document/7502127/},
volume = {65},
year = {2016}
}
@article{Tian2018a,
annote = {More intuition on the growing algorithm that is mentioned in another work by the same authors:
- For some faults, there are inconsequential tests that do not help isolate it given outputs from other tests, we want to prune those
- Tests can also be automatically prioritized based on knowledge of their conditional probabilities of outputs.
- Even in the binary output case of tests (which is true for this work), the growing algorithm outperformed traditional rollout strategies in time and cost of the test sequence, given different signature matrices of varying sizes and densities.},
author = {Tian, Heng and Duan, Fuhai and Fan, Liang and Sang, Yong},
doi = {10.1016/j.ress.2018.06.002},
issn = {09518320},
journal = {Reliability Engineering {\&} System Safety},
month = {jun},
title = {{Novel solution for sequential fault diagnosis based on a growing algorithm}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S095183201830022X},
year = {2018}
}
@inproceedings{Nakamura2013,
annote = {Proposal for a system that recovers to different points in the robot task based on the type of the error encountered. Pointless; it has never been implemented on a real robot.},
author = {Nakamura, Akira and Nagata, Kazuyuki and Harada, Kensuke and Yamanobe, Natsuki and Tsuji, Tokuo and Foissotte, Torea and Kawai, Yoshihiro},
booktitle = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2013.6696860},
isbn = {978-1-4673-6358-7},
month = {nov},
pages = {3535--3542},
publisher = {IEEE},
title = {{Error recovery using task stratification and error classification for manipulation robots in various fields}},
url = {http://ieeexplore.ieee.org/document/6696860/},
year = {2013}
}
@article{Zenil2016,
abstract = {We investigate the properties of a Block Decomposition Method (BDM), which extends the power of a Coding Theorem Method (CTM) that approximates local estimations of algorithmic complexity based upon Solomonoff-Levin's theory of algorithmic probability providing a closer connection to algorithmic complexity than previous attempts based on statistical regularities e.g. as spotted by some popular lossless compression schemes. The strategy behind BDM is to find small computer programs that produce the components of a larger, decomposed object. The set of short computer programs can then be artfully arranged in sequence so as to produce the original object and to estimate an upper bound on the length of the shortest computer program that produces said original object. We show that the method provides efficient estimations of algorithmic complexity but that it performs like Shannon entropy when it loses accuracy. We estimate errors and study the behaviour of BDM for different boundary conditions, all of which are compared and assessed in detail. The measure may be adapted for use with more multi-dimensional objects than strings, objects such as arrays and tensors. To test the measure we demonstrate the power of CTM on low algorithmic-randomness objects that are assigned maximal entropy (e.g. {\$}\backslashpi{\$}) but whose numerical approximations are closer to the theoretical low algorithmic-randomness expectation. We also test the measure on larger objects including dual, isomorphic and cospectral graphs for which we know that algorithmic randomness is low. We also release implementations of the methods in most major programming languages---Wolfram Language (Mathematica), Matlab, R, Perl, Python, Pascal, C++, and Haskell---and a free online algorithmic complexity calculator.},
annote = {Introduces the Block Decomposition Method (BDM) of Zenil et al. Some of the later works are very hard to parse, so this is a good fallback reference of the method.},
archivePrefix = {arXiv},
arxivId = {1609.00110},
author = {Zenil, Hector and Hern{\'{a}}ndez-Orozco, Santiago and Kiani, Narsis A. and Soler-Toscano, Fernando and Rueda-Toicen, Antonio},
eprint = {1609.00110},
month = {sep},
title = {{A Decomposition Method for Global Evaluation of Shannon Entropy and Local Estimations of Algorithmic Complexity}},
url = {http://arxiv.org/abs/1609.00110},
year = {2016}
}
@inproceedings{gizzi2018generalized,
annote = {Anomalies vs. outliers - important irregularities are anomalies, unimportant ones are outliers/noise. Anomalies often require special treatment.

"This work is the first to use confidence values to separate anomalies from outliers."

Contextual anomalies - data instances that are anomalous in one context, but not others (from Chandola?). Instances can often be described using contextual {\&} behavioural attributes.

The pipeline is as follows:
- Start with normal "context" data, and predicted actions if we are in a normal context - train on non-anomalous data to get this
- If the predicted action does not match the observed action, but we're confident of the context, confident in our prediction, and confident in our observation, then we're in an anomalous situation and a new context should be created. Else, we should either ignore (outlier), or we should update our model of the context under consideration.
- The manner of getting consequences and probability values for the predictions, etc. is done using Dempster-Schafer. Not sure on the exact implementation

Experiments in this paper:
- anomaly detection in an aircraft maneuver
- anomaly detection in traffic speeds

Result: an algorithm that has very low anomaly detection false-positive rate and is invariant to:
- variations in the training data
- iterations of training on the data
- human participant noise
- time of occurrence of the anomaly

Main possible future work - use the calculated confidence values from DS Theory more intelligently.

Interesting idea - variation across the training data resulted in a smaller anomaly context; indicating perhaps a "true" causal context for the anomaly. Not justified in the paper.},
author = {Gizzi, Evana and {Le Vie}, Lisa and Scheutz, Matthias and Sarathy, Vasanth and Sinapov, Jivko},
booktitle = {MRC@ IJCAI},
pages = {1--7},
title = {{A Generalized Framework for Detecting Anomalies in Real-Time Using Contextual Information.}},
year = {2018}
}
@inproceedings{Bullard2018,
annote = {Focus:
- Combine information from multiple query types instead of focusing on a single query type
- Arbitrate automatically between query types to gather both informative features and representative instances
- Reason about when to make queries

Demo - request a demonstration of how a concept is embodied in the physical world
Label - select an unlabeled instance and request the correct label from user
Feature subset - request a subset of features useful for discriminating between task-relevant classes. Prior Bullard et al work has shown that "Human Feature Selection" (HFS) does as well, if not better, than automated feature selection for classification tasks. (Assuming that the features are semantically interpretable by a human)

(In this work, feature subset queries are only asked once as they are not likely to change. This might not be true for diagnosis)

Compares five different strategies for selecting queries:
- Random
- Label queries only (Active Learning)
- Demo queries only (LFD)
- Rule based query selection (often performed in Dialog Management)
- Decision Theoretic query selection - main contribution

Results show that despite some shortcomings, DT approach to query selection is able to adequately prioritize and choose between all three types of queries, as well as reason about when to ask queries.},
author = {Bullard, Kalesha and Thomaz, Andrea L. and Chernova, Sonia},
booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2018.8594279},
isbn = {978-1-5386-8094-0},
month = {oct},
pages = {6049--6056},
publisher = {IEEE},
title = {{Towards Intelligent Arbitration of Diverse Active Learning Queries}},
url = {https://ieeexplore.ieee.org/document/8594279/ https://static1.squarespace.com/static/5947387d440243a2a0af0e88/t/5b63ae9370a6ad76bddb9781/1533259414199/paper{\_}cam{\_}ready.pdf https://drive.google.com/file/d/1-Romn4EBBbeVGlMpex2PXnVxItn35qDB/view},
year = {2018}
}
@article{karimi2014comparing,
annote = {Uses ANNs to diagnose faults in the Tennessee Eastman process. There are 21 different faults.

Solution: use boosting. They also develop a different boosting strategy beyond AdaBoost to improve the performance of multiclass classification with multiple MLPs},
author = {Karimi, Pooria and Jazayeri-Rad, Hooshang},
journal = {Journal of Automation and Control},
number = {1},
pages = {21--32},
title = {{Comparing the fault diagnosis performances of single neural networks and two ensemble neural networks based on the boosting methods}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1005.5270{\&}rep=rep1{\&}type=pdf},
volume = {2},
year = {2014}
}
@inproceedings{beck2015skill,
annote = {TODO - A mechanism for diagnosing and then presenting recovery actions.},
author = {{Beck Anders Billes{\o}and Schwartz}, Anders Due and Fugl, Andreas Rune and Naumann, Martin and Kahl, Bj{\"{o}}rn},
booktitle = {FinE-R@ IROS},
pages = {5--10},
title = {{Skill-based Exception Handling and Error Recovery for Collaborative Industrial Robots.}},
url = {http://ceur-ws.org/Vol-1484/paper14.pdf},
year = {2015}
}
@inproceedings{Hornung2014,
annote = {- Model-free method because modeling has its drawbacks.
- Incremental learning because learning everything from the start is unreasonable.
- Introduces many methods of dimensionality reduction to show that their novelty detection on the KUKA robot is the best
- It is also very fast. This led Khalastchi and Kalech to their 2018 paper, which was a speed improvement over their previous work.

Requirements of novelty detection (from another work):
- robust trade-off - exclude novel samples and include known
- generalization - lower FPR and FNR
- adaptability - incorporate new information
- minimized complexity - for online evaluation
- independence - for varying dimensions and features
- parameter minimization - to make it easy on the user

The paper mentions a bunch of learning methods, including different SVM kernels, as well as dimensionality reduction methods, including PCA, before settling on their method, called HDRADS.},
author = {Hornung, Rachel and Urbanek, Holger and Klodmann, Julian and Osendorfer, Christian and van der Smagt, Patrick},
booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2014.6943078},
isbn = {978-1-4799-6934-0},
month = {sep},
pages = {3676--3683},
publisher = {IEEE},
title = {{Model-free robot anomaly detection}},
url = {http://ieeexplore.ieee.org/document/6943078/},
year = {2014}
}
@book{wald1973sequential,
annote = {A book on sequential hypothesis testing that (according to the cover) states that sequential decision makers can arrive at a solution much sooner and with fewer samples than a testing procedure with a predetermined number of samples.},
author = {Wald, Abraham},
publisher = {Courier Corporation},
title = {{Sequential analysis}},
year = {1973}
}
@article{Salmeron2018,
abstract = {Hybrid Bayesian networks have received an increasing attention during the last years. The difference with respect to standard Bayesian networks is that they can host discrete and continuous variables simultaneously, which extends the applicability of the Bayesian network framework in general. However, this extra feature also comes at a cost: inference in these types of models is computationally more challenging and the underlying models and updating procedures may not even support closed-form solutions. In this paper we provide an overview of the main trends and principled approaches for performing inference in hybrid Bayesian networks. The methods covered in the paper are organized and discussed according to their methodological basis. We consider how the methods have been extended and adapted to also include (hybrid) dynamic Bayesian networks, and we end with an overview of established software systems supporting inference in these types of models.},
annote = {Survey on methods for learning and inference on DBNs as well as pointers to software. Perhaps I should read it sometime?},
author = {Salmer{\'{o}}n, Antonio and Rum{\'{i}}, Rafael and Langseth, Helge and Nielsen, Thomas D. and Madsen, Anders L.},
doi = {10.1613/jair.1.11228},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
month = {aug},
pages = {799--828},
title = {{A Review of Inference Algorithms for Hybrid Bayesian Networks}},
url = {https://www.jair.org/index.php/jair/article/view/11228},
volume = {62},
year = {2018}
}
@inproceedings{Bullard2018a,
annote = {The Human Feature Selection approach is comparable to automatic feature selection approaches if the features are interpretable by the human.

Also indicates that we might be able to directly get feature subsets than query them incrementally.},
author = {Bullard, Kalesha and Chernova, Sonia and Thomaz, Andrea L.},
booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2018.8461012},
isbn = {978-1-5386-3081-5},
month = {may},
pages = {6923--6930},
publisher = {IEEE},
title = {{Human-Driven Feature Selection for a Robotic Agent Learning Classification Tasks from Demonstration}},
url = {https://ieeexplore.ieee.org/document/8461012/},
year = {2018}
}
@inproceedings{Gspandl2012,
annote = {Contribution: belief management using IndiGolog and diagnosis and repair in belief using history-based diagnosis.

IndiGolog is a logic-based agent programming and planning language that implements Situation Calculus. Situation Calculus is a second-order logical language with equality that allows reasoning about actions and their effects (eg. s' = do(s,a) and preconditions = Poss(a(x), s)).

History-based Diagnosis - "what happened?" instead of "what is wrong?".

To integrate this with IndiGolog, maintain a pool of beliefs. Operation is performed based on "best" belief that explains the history of events without too many required additions of exogenous observations (modeled as costs).},
author = {Gspandl, Stephan and Podesser, Siegfried and Reip, Michael and Steinbauer, Gerald and Wolfram, Mate},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6225078},
isbn = {978-1-4673-1405-3},
month = {may},
pages = {2992--2998},
publisher = {IEEE},
title = {{A dependable perception-decision-execution cycle for autonomous robots}},
url = {http://ieeexplore.ieee.org/document/6225078/},
year = {2012}
}
@book{jamshidi1993robotics,
annote = {Thorough usage of Fault-tree analysis for a rover on Mars. Also tries to calculate probabilities},
author = {Jamshidi, Mohammad and Eicker, Patrick J},
publisher = {Prentice-Hall},
title = {{Robotics and remote systems for hazardous environments}},
year = {1993}
}
@incollection{Gertler2014,
address = {London},
annote = {Appears to be a succinct summary of the model-based FDD methods described in the Isermann book},
author = {Gertler, Janos},
booktitle = {Encyclopedia of Systems and Control},
doi = {10.1007/978-1-4471-5102-9_223-1},
pages = {1--7},
publisher = {Springer London},
title = {{Fault Detection and Diagnosis}},
url = {http://link.springer.com/10.1007/978-1-4471-5102-9{\_}223-1},
year = {2014}
}
@article{Zhang2018a,
annote = {Tries to predict the failure times of aircraft power unit failures for business reasons. Models the probability of failure (time to failure) as a Weibull distribution-based renewal process. Involves the use of a "regeneration" parameter that is estimated from data and that is the main contribution of this paper.},
author = {Zhang, Yujie and Wang, Lulu and Wang, Shaonian and Wang, Peng and Liao, Haitao and Peng, Yu},
doi = {10.1016/j.microrel.2018.03.002},
issn = {00262714},
journal = {Microelectronics Reliability},
month = {may},
pages = {215--225},
title = {{Auxiliary power unit failure prediction using quantified generalized renewal process}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0026271418301070},
volume = {84},
year = {2018}
}
@article{Rong2018,
annote = {Create a filter on a Markov Jump System model to allow for uncertain transition matrices and possible delays in fault detection.},
author = {Rong, Lihong and Peng, Xiuyan and Zhang, Biao},
doi = {10.1007/s12555-017-0182-3},
issn = {1598-6446},
journal = {International Journal of Control, Automation and Systems},
month = {oct},
number = {5},
pages = {2021--2032},
title = {{A Reduced-order Fault Detection Filter Design for Polytopic Uncertain Continuous-time Markovian Jump Systems with Time-varying Delays}},
url = {http://link.springer.com/10.1007/s12555-017-0182-3},
volume = {16},
year = {2018}
}
@article{Chine2016,
annote = {Fault isolation for photovoltaic cells. Uses 2 methods - thresholding and comparing to signatures, and using an ANN based on the incoming signals

1. "a fault may be characterized by more than one fault signature"
2. "different faults can have the same combination of attributes},
author = {Chine, W. and Mellit, A. and Lughi, V. and Malek, A. and Sulligoi, G. and {Massi Pavan}, A.},
doi = {10.1016/j.renene.2016.01.036},
issn = {09601481},
journal = {Renewable Energy},
month = {may},
pages = {501--512},
title = {{A novel fault diagnosis technique for photovoltaic systems based on artificial neural networks}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0960148116300362},
volume = {90},
year = {2016}
}
@inproceedings{chen2015value,
annote = {Proposes the use of a new metric for active query selection - Same Decision Probability. Essentially, this is a metric that optimizes the expectation that the decision does not easily change in light of new information.

The paper introduces the metric and presents a way to calculate it in a computationally tractable manner.

This metric is useful in applications where the decision maker wishes to reduce their liability to unknown information.},
author = {Chen, Suming Jeremiah and Choi, Arthur and Darwiche, Adnan},
booktitle = {Twenty-Ninth AAAI Conference on Artificial Intelligence},
title = {{Value of information based on Decision Robustness}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/viewFile/9493/9786},
year = {2015}
}
@inproceedings{holladay2016active,
annote = {Learning user preferences (cost functions) by allowing users to pick an uncertain option. The uncertainty threshold for the uncertain option can be user dependent, but it's not something that we specifically want to learn.

Therefore, instead of trying to use queries to get at the cost/thresh pair, the work frames the problem as an ECD (Golovin et al) and specifies the same cost for a range of thresholds as belonging to the same equivalence class.

Also model user uncertainty as noisy using some psychological parameters for modeling that noise

Conducted a user study and found that CLAUS (the algorithm here) did indeed ask fewer queries than GBS. Also tested questionnaire prompts and found no significant difference.

Useful as an application of the active learning work. No algorithm provided on the polynomials though.},
author = {Holladay, Rachel and Javdani, Shervin and Dragan, Anca and Srinivasa, Siddhartha},
booktitle = {RSS Workshop on Model Learning for Human-Robot Communication},
title = {{Active comparison based learning incorporating user uncertainty and noise}},
url = {https://www.ri.cmu.edu/pub{\_}files/2016/6/claus.pdf},
year = {2016}
}
@phdthesis{zhang2017uncertainty,
annote = {Really good motivation for why condition-based monitoring is performed based on parameter estimation from HMMs.

Still unclear on the actual method of Gibbs sampling that is introduced for performing the parameter estimation.

Motivation:
- Normally, fault isolation is done with multiple HMMs, but it's hard to estimate the transition probabilities when the true set of hidden state sequences for each of the HMMs is unknown
- Using a single HMM and equating the hidden state to a fault state is more realistic/explainable and makes the model generative and suited for online adaptation, but it requires all faults to be known beforehand
- Adding the flexibility for an unknown state still has the problem that there is an assumption of a discrete/finite set of fault states.

"Modern engineering systems are highly complex that often consist of interacting dynamic systems. For such applications, the traditional approach for realizing HMM based diagnostic functionality becomes excessively cumbersome because of the need to train the condition monitoring processes to recognize a large number of faults or faults of various severities, some of which often cannot be anticipated in advance. Even for the cases one is able to anticipate in advance, many faults manifest themselves very differently under different control inputs and environmental conditions, which makes training of diagnostic units for all possible conditions and all possible faults infeasible. Finally, such systems consist of numerous subsystems, each of which could contain significant non-linearities, with multiple control and environmental inputs, as well as inputs from other subsystems. This situation permits anomalies in one system to cascade and incite anomalous behavior of other systems connected to it, which effectively masks the real source of the anomaly},
author = {Zhang, Deyi},
school = {The University of Texas at Austin},
title = {{Uncertainty quantification and its properties for hidden Markov models with application to condition based maintenance}},
url = {https://repositories.lib.utexas.edu/bitstream/handle/2152/63715/ZHANG-DISSERTATION-2017.pdf?sequence=1},
year = {2017}
}
@article{Martins2017,
annote = {Easier to digest version of the survey paper.

Interesting tidbit: unlike the results from the survey papers where FTA is the most popular method of Failure Analysis, in practice, anecdotal evidence shows that FMEA is more popular. This is because FMEA usually leads to more "actionable" recommendations.},
author = {Martins, Luiz Eduardo G. and Gorschek, Tony},
doi = {10.1109/MS.2017.94},
issn = {0740-7459},
journal = {IEEE Software},
number = {4},
pages = {49--57},
title = {{Requirements Engineering for Safety-Critical Systems: Overview and Challenges}},
url = {http://ieeexplore.ieee.org/document/7974683/},
volume = {34},
year = {2017}
}
@article{DAngelo2016,
annote = {Fault isolation often requires process models, which can be uncertain or impossible. Contribution:
- Use fuzzy bayesian MCMC for change-point detection in signals
- Based on the probability of detected change points, classify the fault

Classification seems to be some "immune"-inspired genetic algorithm},
author = {D'Angelo, Marcos F.S.V. and Palhares, Reinaldo M. and {Camargos Filho}, Murilo C.O. and Maia, Renato D. and Mendes, Jo{\~{a}}o B. and Ekel, Petr Ya.},
doi = {10.1016/j.asoc.2016.08.040},
issn = {15684946},
journal = {Applied Soft Computing},
month = {dec},
pages = {676--686},
title = {{A new fault classification approach applied to Tennessee Eastman benchmark process}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494616304343},
volume = {49},
year = {2016}
}
@inproceedings{Niekum2012,
annote = {// TODO},
author = {Niekum, Scott and Osentoski, Sarah and Konidaris, George and Barto, Andrew G.},
booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6386006},
isbn = {978-1-4673-1736-8},
keywords = {todo-add-notes},
mendeley-tags = {todo-add-notes},
month = {oct},
pages = {5239--5246},
publisher = {IEEE},
title = {{Learning and generalization of complex tasks from unstructured demonstrations}},
url = {http://ieeexplore.ieee.org/document/6386006/},
year = {2012}
}
@article{Pu2017,
abstract = {We consider the problem of diagnosis where a set of simple observations are used to infer a potentially complex hidden hypothesis. Finding the optimal subset of observations is intractable in general, thus we focus on the problem of active diagnosis, where the agent selects the next most-informative observation based on the results of previous observations. We show that under the assumption of uniform observation entropy, one can build an implication model which directly predicts the outcome of the potential next observation conditioned on the results of past observations, and selects the observation with the maximum entropy. This approach enjoys reduced computation complexity by bypassing the complicated hypothesis space, and can be trained on observation data alone, learning how to query without knowledge of the hidden hypothesis.},
annote = {Interesting side project that posits that in some domains, it might be better to not model a complex hypothesis space before attempting to garner observations. Rather, modeling associations between successive (or a sequence of) observations might be enough. They use a neural network to implement such a model and show that it gets decent performance in selecting minimal queries in order to achieve some performance.},
archivePrefix = {arXiv},
arxivId = {1704.06131},
author = {Pu, Yewen and Kaelbling, Leslie P and Solar-Lezama, Armando},
eprint = {1704.06131},
month = {apr},
title = {{Learning to Acquire Information}},
url = {http://arxiv.org/abs/1704.06131},
year = {2017}
}
@article{Kim2018,
abstract = {It is essential to provide reliable localization results to allow mobile robots to navigate autonomously. Even though many state-of-the-art localization schemes have so far shown satisfactory performance in various environments, localization has still been difficult under specific conditions, such as extreme environmental changes. Since many robots cannot diagnose for themselves whether the localization results are reliable, there can be serious autonomous navigation problems. To solve this problem, this study proposes a self-diagnosis scheme for the localization status. In this study, two indicators are empirically defined for the self-diagnosis of localization status. Each indicator shows significant changes when there are difficulties in light detection and ranging (LiDAR) sensor-based localization. In addition, the classification model of localization status is trained through machine learning using the two indicators. A robot can diagnose the localization status itself using the proposed classification model. To verify the usefulness of the proposed method, we carried out localization experiments in real environments. The proposed classification model successfully detected situations where the localization accuracy is significantly degraded due to extreme environmental changes.},
annote = {The robot realizing that it is mislocalized is a hard problem. In this work, the authors train an SVM to attempt to provide a classification of localization confidence.},
author = {Kim, Jiwoong and Park, Jooyoung and Chung, Woojin},
doi = {10.3390/s18093168},
issn = {1424-8220},
journal = {Sensors},
month = {sep},
number = {9},
pages = {3168},
title = {{Self-Diagnosis of Localization Status for Autonomous Mobile Robots}},
url = {http://www.mdpi.com/1424-8220/18/9/3168},
volume = {18},
year = {2018}
}
@article{Natarajan2013,
annote = {Generalization of the Coupled HSMM paper that allows for much more hierarchical models. Also includes details on how to train those models as well as perform inference with them.},
author = {Natarajan, Pradeep and Nevatia, Ramakant},
doi = {10.1016/j.cviu.2012.08.011},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
month = {oct},
number = {10},
pages = {1329--1344},
title = {{Hierarchical multi-channel hidden semi Markov graphical models for activity recognition}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S107731421200166X},
volume = {117},
year = {2013}
}
@article{Park2018,
annote = {Anomaly detection using an LSTM autoencoder in a robot-assisted feeding task.

LSTM-based VAE featuring:
- denoising criterion for training
- reconstruction posterior
- progress-based prior on the latent space
- anomaly threshold classifier based on the latent space representation

The results show that the method developed by Park et al outperforms previous HMM-GP, one-class SVMs, and other types of autoencoders.},
author = {Park, Daehyung and Hoshi, Yuuna and Kemp, Charles C.},
doi = {10.1109/LRA.2018.2801475},
issn = {2377-3766},
journal = {IEEE Robotics and Automation Letters},
month = {jul},
number = {3},
pages = {1544--1551},
title = {{A Multimodal Anomaly Detector for Robot-Assisted Feeding Using an LSTM-Based Variational Autoencoder}},
url = {http://ieeexplore.ieee.org/document/8279425/},
volume = {3},
year = {2018}
}
@article{Wienke2017,
abstract = {In January 2015 we distributed an online survey about failures in robotics and intelligent systems across robotics researchers. The aim of this survey was to find out which types of failures currently exist, what their origins are, and how systems are monitored and debugged - with a special focus on performance bugs. This report summarizes the findings of the survey.},
annote = {Interesting study exploring the different types of faults that are experienced in the field and the tools that professionals use to debug and fix the faults.},
archivePrefix = {arXiv},
arxivId = {1708.07379},
author = {Wienke, Johannes and Wrede, Sebastian},
eprint = {1708.07379},
month = {aug},
title = {{Results of the Survey: Failures in Robotics and Intelligent Systems}},
url = {http://arxiv.org/abs/1708.07379},
year = {2017}
}
@article{Woodman2012,
annote = {Traditional failure modes analyses don't apply to robots because they often fail to capture environmental factors that are potential issues and hazards. This paper adopts a variant of HAZOP, called SHARD, and extends it to scenarios specific to mobile autonomous robots to help create a hazards analysis.

Contributions:
- Method of writing the safety specifications
- How those specifications are verified
- How they are implemented in the final system

Safety policy is an interlock written in software. They prevent a robot from generating unsafe actions.

Similar to KB-system design, policies are written as conditionals and actions to take. Safety policies either allow a system to function, or impose restrictions based on the specifications. Whether to activate a safety policy or not can be a result of the initial hazards analysis

The Safety Policy execution process is a DAG. Sensor data is sent to the safety policy generator and the controller. The policy generator then either modifies the parameters of the controller or mediates the controller's connections with the actuator to ensure safety according to the output of SHARD.

In practice, to perform SHARD, it helps to break the task by performing an HTA. (need to read more on this).},
author = {Woodman, Roger and Winfield, Alan F.T. and Harper, Chris and Fraser, Mike},
doi = {10.1177/0278364912459665},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
month = {nov},
number = {13},
pages = {1603--1626},
title = {{Building safer robots: Safety driven control}},
url = {http://journals.sagepub.com/doi/10.1177/0278364912459665},
volume = {31},
year = {2012}
}
@article{Pearl2012,
abstract = {The do-calculus was developed in 1995 to facilitate the identification of causal effects in non-parametric models. The completeness proofs of [Huang and Valtorta, 2006] and [Shpitser and Pearl, 2006] and the graphical criteria of [Tian and Shpitser, 2010] have laid this identification problem to rest. Recent explorations unveil the usefulness of the do-calculus in three additional areas: mediation analysis [Pearl, 2012], transportability [Pearl and Bareinboim, 2011] and metasynthesis. Meta-synthesis (freshly coined) is the task of fusing empirical results from several diverse studies, conducted on heterogeneous populations and under different conditions, so as to synthesize an estimate of a causal relation in some target environment, potentially different from those under study. The talk surveys these results with emphasis on the challenges posed by meta-synthesis. For background material, see http://bayes.cs.ucla.edu/csl{\_}papers.html},
annote = {- Follow up talk on do-calculus. Also a very basic summary of the Causality textbook.
- Provides references, until 2012, of resources for determining and doing causal inference.},
archivePrefix = {arXiv},
arxivId = {1210.4852},
author = {Pearl, Judea},
eprint = {1210.4852},
month = {oct},
title = {{The Do-Calculus Revisited}},
url = {http://arxiv.org/abs/1210.4852},
year = {2012}
}
@book{hinchey2006agent,
annote = {(from the appendix of the ESHA paper)

Architecture model for cognitive processes within an autonomous agent

Contains:
- Perceptors - input data
- Effectors - actuators
- Agent Communications - dialogue and social behaviour; perception and execution of those behaviours
- Execution - deciding actions to take to achieve plan
- Modelling and State - storage of knowledge base and/or world model
- Agent Reasoning - source of logical reasoning and inference; decides what type of behaviour (reflexive, social, etc.) to execute
- Planning and Scheduling - generation and monitoring of action goals produced by reasoning
- Agenda - middleware between planner and the Execution.

Based on those modules, the architecture provides a classification of behaviour types according to the different cognitive processes that are engaged during those behaviours:
- Reflexive - fixed/stereotyped action pattern initiated directly by percepts
- Reactive - reasoned action initiated by events in the environment. Mode 1 - triggered by another agent. Mode 2 - triggered by a percept
- Deliberative - reasoned and planned action initiated by external events. Mode 1 - triggered by another agent. Mode 2 - triggered by a percept.
- Proactive - action initiated by the agent itself due to internal motivations
- Social - dialogue with other agents that may also trigger action. Mode 1 - triggered by another agent. Mode 2 - triggered by the agent itself.

Model need not correspond to the software processes. It helps specify the functional/cognitive process requirements instead.},
author = {Rouff, Christopher A and Hinchey, Michael and Rash, James and Truszkowski, Walter and Gordon-Spears, Diana},
publisher = {Springer Science {\&} Business Media},
title = {{Agent technology from a formal perspective}},
year = {2006}
}
@inproceedings{jaiem2016fault,
address = {Barcelona, Spain},
annote = {Next generation recovery strategies for the COTAMA architecture folks. Involves the reallocation of computing resources, I think. Does not perform the diagnosis problem.},
author = {Jaiem, Lotfi and Lapierre, Lionel and Godary-Dejean, Karen and Crestani, Didier},
booktitle = {SysTol: Control and Fault-Tolerant Systems},
title = {{Fault tolerant autonomous robots using mission performance guided resources allocation}},
url = {https://hal-lirmm.ccsd.cnrs.fr/lirmm-01591460/document},
year = {2016}
}
@inproceedings{Olteanu2017,
address = {New York, New York, USA},
annote = {Propensity Score Matching as a method of matching individuals between 2 groups in order to isolate causal factors. Read about what the method actually is.},
author = {Olteanu, Alexandra and Varol, Onur and Kiciman, Emre},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing - CSCW '17},
doi = {10.1145/2998181.2998353},
isbn = {9781450343350},
pages = {370--386},
publisher = {ACM Press},
title = {{Distilling the Outcomes of Personal Experiences}},
url = {http://dl.acm.org/citation.cfm?doid=2998181.2998353 https://www.microsoft.com/en-us/research/wp-content/uploads/2016/10/cscw17-distilling-the-outcomes-of-personal-experiences-a-propensity-scored-analysis.pdf},
year = {2017}
}
@inproceedings{bramley2017causal,
annote = {It is important to time interventions correctly in order to make them an effective tool for uncovering causal structure.

People judge causality based on:
- event ordering
- cause-effect delays are in line with expectations
- low variance across instances

People also judge causality based on interventions - idealized actions that set variables in a system.

Focus: Provide a link between timing and interventions, especially how they effect each other in causal learning.

Notes:
- People follow interventions along the principle of confirmatory testing (positive testing)
- Past work makes simplifying assumptions about cyclical causal structures. This work tries to model hypotheses on cyclical causal structure.

Results:
- Reliable causal connection allowed for greater accuracy than unreliable ones (both ANOVA, and Bayesian prediction)
- No difference in accuracy between cyclic and acyclic models (Bayesian prediction only). There is a difference with ANOVA
- Longer intervals between interventions was positively correlated with accuracy
- Variability between intervention gaps was inversely related to accuracy
- Participants frequently added connections but rarely removed them.
- Interventions on root variables helped the most with the learning.},
author = {Bramley, Neil R and Mayrhofer, Ralf and Gerstenberg, Tobias and Lagnado, David A},
booktitle = {Proceedings of the 39th Annual Meeting of the Cognitive Science Society. Austin, TX: Cognitive Science Society},
title = {{Causal learning from interventions and dynamics in continuous time}},
url = {http://web.mit.edu/tger/www/papers/Causal learning from interventions and dynamics in continuous time, Bramley et al., 2017.pdf},
year = {2017}
}
@article{Yu2010,
annote = {Notes present as highlights on local filesystem. It's a very thorough overview of HSMMs and a must-read in case we are considering semi-Markov models.},
author = {Yu, Shun-Zheng},
doi = {10.1016/j.artint.2009.11.011},
issn = {00043702},
journal = {Artificial Intelligence},
month = {feb},
number = {2},
pages = {215--243},
title = {{Hidden semi-Markov models}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370209001416},
volume = {174},
year = {2010}
}
@inproceedings{Jung2018,
address = {Warsaw, Poland},
annote = {Performs active diagnosis by exciting the monitored process and looking for errors.

Define the process as a function of the input, the (hidden) state, and the fault. Based on a sensitivity of the system state to the fault (here defined as a continuous additive fault to some internal states), the input can be augmented such that the residuals for isolation become more pronounced, and the input can also be augmented such that the effects of the fault are mitigated. The manner of figuring out how to augment the input is through a relaxed convex optimization formulation of the problem.},
author = {Jung, Daniel Eriksson and Ahmed, Qadeer},
booktitle = {IFAC Safe Process},
title = {{Active Fault Management in Autonomous Systems Using Sensitivity Analysis}},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318324509},
year = {2018}
}
@inproceedings{liu2015complexity,
annote = {"The test sequencing problem involves a decision maker with one key high-stakes decision and a set of information gathering activities." This paper advocates the use of "decision circuits" over decision trees or influence diagrams in the pursuit of an exact solution to the test sequencing problem.

Aside: Influence diagram is a manner of representing a decision problem with decision nodes (rectangles), uncertainty/deterministic nodes (ovals) that are variables or functions, and value nodes (diamonds) that are objectives. Influence diagrams and decision trees (not ML!) are complementary views of the same problem. Decision trees enumerate the possibilities, ID's make the influences of variables clear.

Assume that there is a disorder, D, with Pr(D) that must be diagnosed using (expensive) tests T. Tests produce uncertain, but visible results, and it is never optimal to repeat tests. The Influence Diagram for the problem is in Fig. 1 and Table 1; it is quite informative. The goal is to maximize the value from treating the disorder while minimizing the costs of the tests.

Method:
- Formulate an MDP
- State at each step is the result of all the tests performed so far
- Actions are the tests that are performed
- The final state is D (I think?)

Solution to the MDP can be calculated using Decision Trees in a back-tracking like manner. Bhattacharya and Schacter introduced decision circuits that are supposedly better.

Best understanding of Decision Circuits:
- Similar to Arithmetic Circuits (whatever they are) for Bayes Nets
- All possible outputs are enumerated as leaves
- Then probabilities are backed up
- Such a method essentially encourages the reuse of computation in a back-tracking search formulation of decision trees.
- Apparently decision circuits also allow for sensitivity analysis; not sure what that means.},
author = {Liu, Wenhao and Shachter, Ross D},
booktitle = {UAI},
pages = {494--503},
title = {{Complexity of the Exact Solution to the Test Sequencing Problem.}},
url = {https://www.researchgate.net/profile/Ross{\_}Shachter/publication/283777837{\_}Complexity{\_}of{\_}the{\_}exact{\_}solution{\_}to{\_}the{\_}test{\_}sequencing{\_}problem/links/5756068908ae155a87b9ce9c.pdf http://www.lumina.com/technology/influence-diagrams/},
year = {2015}
}
@article{Taubig2012,
annote = {Created a standards compliant collision checker for autonomous vehicles.

FMEA -{\textgreater} Annotation of C functions -{\textgreater} Code Reviews -{\textgreater} formal specification of functions -{\textgreater} reviews of formal specifications -{\textgreater} formal proof (contribution) -{\textgreater} dynamic analysis and test.

Also used a subset of C, called MISRA C, for writing the code.

Appendix A, Table 2 contains the list of requirements of the standard that this work conformed to.},
author = {T{\"{a}}ubig, Holger and Frese, Udo and Hertzberg, Christoph and L{\"{u}}th, Christoph and Mohr, Stefan and Vorobev, Elena and Walter, Dennis},
doi = {10.1007/s10514-011-9271-y},
issn = {0929-5593},
journal = {Autonomous Robots},
month = {apr},
number = {3},
pages = {303--331},
title = {{Guaranteeing functional safety: design for provability and computer-aided verification}},
url = {http://link.springer.com/10.1007/s10514-011-9271-y},
volume = {32},
year = {2012}
}
@article{Li2006,
annote = {Creates a finite state transition diagram of reliability for a Markov Jump System by considering semi-markov properties (the duration of a state along with the actual transition). Leads to better fault detection, I think.},
author = {Li, Hongbin and Zhao, Qing},
doi = {10.1243/09596518JSCE225},
issn = {0959-6518},
journal = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
month = {aug},
number = {5},
pages = {329--338},
title = {{Reliability Evaluation of Fault Tolerant Control with a Semi-Markov Fault Detection and Isolation Model}},
url = {http://journals.sagepub.com/doi/10.1243/09596518JSCE225},
volume = {220},
year = {2006}
}
@inproceedings{davis2018causal,
annote = {TODO},
author = {Davis, Zachary J and Bramley, Neil R and Rehder, Bob},
booktitle = {Proceedings of the 40th Annual Conference of the Cognitive Science Society. Austin, TX: Cognitive Science Society},
title = {{Causal Structure Learning with Continuous Variables in Continuous Time}},
url = {https://mindmodeling.org/cogsci2018/papers/0073/0073.pdf},
year = {2018}
}
@inproceedings{Py2004,
annote = {Lots of reasons why executions of hierarchical autonomous systems such as robots are hard to verify:
- High level models are often incomplete and only completed by low level modules
- Components are often complex and hard to validate
- Functional modules are often unaware of each other - base can move while arm is not tucked
- It is often impossible to cover the breadth of test cases through simulation.

Execution Control - Filter (safety bag) that ensures that controls to the system never reach an inconsistent state.

The controller needs:
- observability - ability to monitor and catch all events that lead to or participate in a system inconsistency
- controllability - needs to be able to control the system
- synchronous - makes life easier (aside: meh)
- verifiability - provides a formalism and representation to allow the developer to check the behaviour
The controller interfaces between the decisional layer and the functional layer. Also needs to monitor communications between functions.

Proposal:
- Create a Request and Resource Checker. All requests go through R2C.
- R2C maintains a state of the system. If the state changes, then a checker determines if the request is satisfiable
- R2C then launches commands based on the results of the checks.

Method:
- Developed a constraint language to specify the state of a system and the constraints
- Maintenance of constraint rules in real time is done by way of an Ordered binary decision diagram (OBDD)-like data structure called Ordered Constraint Rules Diagram.},
author = {Py, F. and Ingrand, F.},
booktitle = {2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)},
doi = {10.1109/IROS.2004.1389549},
isbn = {0-7803-8463-6},
pages = {1136--1141},
publisher = {IEEE},
title = {{Dependable execution control for autonomous robots}},
url = {http://ieeexplore.ieee.org/document/1389549/},
volume = {2},
year = {2004}
}
@inproceedings{Natarajan2007,
annote = {Coupled semi-Markov models, which allow for multiple simultaneous state sequences in a HSMM. Referenced by Yu, 2010.

Methods for training and inference using coupled HSMMs. The method does really well in activity recognition tests; makes you wonder though why it didn't take off...},
author = {Natarajan, Pradeep and Nevatia, Ramakant},
booktitle = {2007 IEEE Workshop on Motion and Video Computing (WMVC'07)},
doi = {10.1109/WMVC.2007.12},
isbn = {0-7695-2793-0},
month = {feb},
pages = {10--10},
publisher = {IEEE},
title = {{Coupled Hidden Semi Markov Models for Activity Recognition}},
url = {http://ieeexplore.ieee.org/document/4118806/},
year = {2007}
}
@inproceedings{golovin2010near,
annote = {Formulate the test sequencing problem as a Bayesian Active Learning problem - this is the beginning. The arXiv version has the extended proofs.

- Optimal Decision Tree Problem is the name of the problem in the face of noiseless tests. It can be solved within reasonable time by Generalized Binary Search
- This paper deals with noisy observations; hence the problem extends not to identifying a single hypothesis, but to that of identifying an equivalance class of hypotheses - Equivalence Class Determination problem.
- In the noiseless case, the GBS works because it is monotone - the objective function increases with every test, and because it is adaptively submodular - adding a test later has a smaller benefit than using it now.
- The metrics of Value-of-Information, entropy, and GBS don't work with noisy observations because GBS assumes that greedily singling out hypotheses can separate equivalence classes (they may not) and entropy assumes that a sum of a single test is the same as applying those tests in arbitrary order.

Method:
- Represent the hypotheses as equivalence classes and add edges between hypotheses only if they belong to different equivalence classes
- Cut edges with each test if they are inconsistent with the test outcome
- In order to not have to keep track of all the equivalence classes, the method approximates the edge weights somehow; don't understand this. The resulting objective of approximate edge weights is akin to something called Renyi entropy.

Evaluation:
- The method is more accurate than other methods for recovering parameters of the lottery (I think)
- The method is also very capable of classifying humans according to the types of behavioural profiles they followed. Again, my understanding of this section is vague.},
author = {Golovin, Daniel and Krause, Andreas and Ray, Debajyoti},
booktitle = {Advances in Neural Information Processing Systems},
pages = {766--774},
title = {{Near-optimal bayesian active learning with noisy observations}},
url = {https://papers.nips.cc/paper/4073-near-optimal-bayesian-active-learning-with-noisy-observations.pdf https://arxiv.org/pdf/1010.3091.pdf https://drive.google.com/file/d/1{\_}K7UST4cU-Sy1dlc4KnLQZEgdnsZfrZr/view},
year = {2010}
}
@article{Bramley2017,
annote = {Summary of all the work presented through user studies in 2014, 2017 papers and others so far. Cite this when it comes out.

Most useful to simply look at intro and discussions to experiments. The description of likelihood function estimation for the Bayesian learners could be useful, but likely not.},
author = {Bramley, Neil R and Gerstenberg, Tobias and Mayrhofer, Ralf and Lagnado, David A},
title = {{Time in Causal Structure Learning}},
url = {http://discovery.ucl.ac.uk/10050383/1/bramley et al.pdf https://github.com/neilbramley/time{\_}cause},
year = {2017}
}
@inproceedings{Abdul2018,
address = {New York, New York, USA},
annote = {Large scale survey of various papers on XAI. Started with about 200 papers that for which topics were modeled. Then the papers citing those paper, or cited by those papers were added to a dataset. The resulting analysis then conducted keyword-based analysis of the abstracts and paper contents to present the current landscape of XAI research.


(In the following I focus on the desired characteristics of XAI and the role of causality in them)
Methods for XAI  have generally had mathematical rigor but often suffer from a lack of usability, practical interpretability, and efficacy on real users
Context-aware systems should be make intelligible by informing systems what they know, how they know it, and what they are doing with that information
From Psychology: there are different types of explanations - functional (teleological), mechanistic, causal.
HCI research in explanations have neglected to utilize prior advances in CS and Cognitive Science:
The use of "Why" questions for program debugging
The use of counterfactual reasoning for causal explanations
Focus of current XAI is on mathematics required to transform complex "black box" into a simpler one. Neglect whether those explanations are usable or practical in the real world
Causal framework of Pearl and Halpern with Causal Bayes Nets could provide a strong theoretical framework for XAI
Allow users to explore useful explanations with interactive methods},
author = {Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y. and Kankanhalli, Mohan},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI '18},
doi = {10.1145/3173574.3174156},
isbn = {9781450356206},
pages = {1--18},
publisher = {ACM Press},
title = {{Trends and Trajectories for Explainable, Accountable and Intelligible Systems}},
url = {http://dl.acm.org/citation.cfm?doid=3173574.3174156},
year = {2018}
}
@article{Vilela2017,
annote = {Systematic Literature Review of the different methods of Requirements Engineering and Safety Analysis. (Use this paper as a reference if you ever need to write a survey paper)

Highlights:
- Safety arguments (for the safety or not) of a system were/are mostly conducted using free text. Goal Structuring Notation (GSN) instead is a graphical argumentation technique.
- Similar to the findings of the Martins and Gorschek paper, there seems to a need for common models between the requirements, design, and safety teams
- SysML has been used for safety specification; especially in a SysML based design process
- Galvao Martins {\&} De Oliviera, 2014 show a process of deriving safety requirements from a Fault Tree Analysis
- Papers that propose new Metrics and Language often have the least rigor, because they (often) don't contain experiments to verify and test the metrics and design.
- Papers that propose new Approaches, Methods, Checklists, and Language often have the worst relevance, as they are frequently tested only in toy domains.
- FTA is very widely cited. FMEA, not as much. However, PHA, which is the next most often cited method, depends heavily upon the use of FMEA.
- According to Leveson, 1995, if quantitative methods are used, qualitative analysis must precede them because causal factors must be identified before numerical values can be assigned to them.
- Safety is often confused with reliability... Safety is not equivalent to reliability or reliability implies safety
- Lack of experience with safety requirements modeling hampers their usage in practice
- Lack of a clear method of deploying safety checklists during requirements development hampers the adoption of safety engineering in practice - what are the core set of activities to be performed by a requirements engineer during a safety analysis?
- Advocate extending safety concepts to UML/SysML-like diagrams to promote the exchange of information
- Consistency checking in large safety requirements specifications demands more approaches than simply checking for testability or ambiguity.

There are two methods of safety analysis:
- Inductive - start with the set of failures and analyzes potential consequences. It is forward analysis. Eg: FMEA
- Deductive - determine the set of paths that can lead to a particular hazard or accident. It is backward analysis. Eg: Fault tree analysis

Also possible to distinguish analysis as quantitative and qualitative:
- Quantitative - probabilities or pdfs are assigned to events in a chain and the overall likelihood of a loss is calculated
- Qualitative - not as rigorous},
author = {Vilela, J{\'{e}}ssyka and Castro, Jaelson and Martins, Luiz Eduardo G. and Gorschek, Tony},
doi = {10.1016/j.jss.2016.11.031},
issn = {01641212},
journal = {Journal of Systems and Software},
month = {mar},
pages = {68--92},
title = {{Integration between requirements engineering and safety analysis: A systematic literature review}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121216302333},
volume = {125},
year = {2017}
}
@inproceedings{Racca2018,
address = {New York, New York, USA},
annote = {Different types of queries to ask in order to learn prior probabilities and the transition functions for a task model.},
author = {Racca, Mattia and Kyrki, Ville},
booktitle = {Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction - HRI '18},
doi = {10.1145/3171221.3171241},
isbn = {9781450349536},
pages = {123--131},
publisher = {ACM Press},
title = {{Active Robot Learning for Temporal Task Models}},
url = {http://dl.acm.org/citation.cfm?doid=3171221.3171241},
year = {2018}
}
@inproceedings{Gao2018,
address = {Melbourne, Australia},
annote = {From abstract: addresses the problem of action-effect prediction. Actions are verb-noun pairs, and effects are states of the physical world as depicted by images.

Goals:
- Facilitate cause (action) prediction: given a state of the world, predict what actions might have caused it
- Facilitate effect prediction: given an action as a verb-noun pair, identify the image that predicts the outcome.

Learned action-verb -{\textgreater} image models by using annotated human data and Google Image search.

Results:
- By training a CNN on pairs of verbs and images for actions and the effects, they got the best classification results on correctly classifying noisy web images.
- Also use embeddings for the action-effect phrases that are output. Success with LSTM + some other non-interesting things.

Takeaway - allowing for action-effect prediction from both the text and images should allow for better task execution.},
author = {Gao, Qiaozi and Yang, Shaohua and Chai, Joyce Y. and Vanderwende, Lucy},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
pages = {1--12},
publisher = {Association for Computational Linguistics},
title = {{What Action Causes This? Towards Naive Physical Action-Effect Prediction}},
url = {http://aclweb.org/anthology/P18-1086},
year = {2018}
}
@inproceedings{Chen2018,
annote = {(Paper 4 in the ZIP file)

Deep Space mission planning and execution requires:

1. Advance prediction
2. Root Cause Analysis
3. Human-machine cooperation for knowledge extraction and decision making
4. "What-if" scenario analysis

Presents a System-Invariant Analysis Tech (SIAT) that models temporal and spatial dependencies between components during nominal operation. Then produces information about faults and diagnoses when an error occurs.

Modeling is performed by using an autoregressive model, Y = f(X). A strong dependency between a pair of components is called an "invariant". Invariants can then be represented in a graph, which can then be traversed to get at the root cause of the fault.

They don't actually present a solution to the problem},
author = {Chen, Haifeng and Horak, Matthew and Narayanappa, Sadananda and Woodward, Kevin},
booktitle = {AAAI Fall Symposium Series},
title = {{Integrating AI into Planning, Diagnostic, and Prescription Systems for Human Deep Space Exploration Missions}},
url = {http://makro.ink/sip/sip18/proceedings.zip},
year = {2018}
}
@article{Khalastchi2018,
annote = {Creates a sensor-based Fault Detection and Diagnosis method that extracts expected patterns from nominal sensor data and then produces readings of faults or not.

The method is better than naive correlation checks of sensor readings, and is compared against SVM, Linear Classifiers, and some of the authors' prior works.

Uses the method of Abreu et al (BARINEL/ZOLTAR) for the fault diagnosis process.},
author = {Khalastchi, Eliahu and Kalech, Meir},
doi = {10.1007/s10514-017-9688-z},
issn = {0929-5593},
journal = {Autonomous Robots},
month = {aug},
number = {6},
pages = {1231--1248},
title = {{A sensor-based approach for fault detection and diagnosis for robotic systems}},
url = {http://link.springer.com/10.1007/s10514-017-9688-z},
volume = {42},
year = {2018}
}
@inproceedings{Durand2010,
annote = {Ways of increasing dependability
- Fault forecasting - FMECA
- Fault avoidance - software development best practices including modularity, and where possible, formal verification and validation
- Fault tolerance - robustness; depends on detection, diagnosis, and recovery mechanisms from faults

Objective of paper:
- Four steps to increase dependability - fault identification, fault detection, fault diagnosis, and fault recovery.
- This paper focuses on the recovery mechanism - COTAMA
- One of the main recovery strategies is sharing autonomy

Method:
- Observation module to listen for and detect faults
- The system architecture is modular, and there is an adaptation module that can reconfigure the system as needed
- Global Supervisor is in charge of the mission and provides objectives to the Local Supervisor
- Local Supervisor is in charge of sending sub-objectives to the Scheduler. Also manages HRI, should there be a need for HRI
- Adapter Supervisor changes subobjectives given the autonomy level. It can either change the parameters of subobjectives, or change subobjectives to degraded versions of themselves (eg: perform path planning without collision avoidance)
- Contextual Supervisor uses fault diagnoses to send events to the other supervisors. Adapter is contacted if weak or medium severity faults; Local is contacted for HRI in case of hard; Global is contacted for termination in case of fatal.

Showcase the transitions between different levels of autonomy through Petri-Nets},
author = {Durand, B. and Godary-Dejean, K. and Lapierre, L. and Passama, R. and Crestani, D.},
booktitle = {2010 Conference on Control and Fault-Tolerant Systems (SysTol)},
doi = {10.1109/SYSTOL.2010.5676030},
isbn = {978-1-4244-8153-8},
month = {oct},
pages = {24--29},
publisher = {IEEE},
title = {{Fault tolerance enhancement using autonomy adaptation for autonomous mobile robots}},
url = {http://ieeexplore.ieee.org/document/5676030/ https://hal-lirmm.ccsd.cnrs.fr/file/index/docid/547856/filename/Article{\_}SYSTOL{\_}2010.pdf},
year = {2010}
}
@article{Jung2018a,
annote = {Given the goal of performing FDD with signature matrices, formulate an online optimization algorithm to select the best set of residuals from all the available residuals.

Formulate the problem in terms of "fault isolability" performance criteria that must be met. Unfortunately, such a criterion is often hard in an optimization problem. Jung et al reformulate the structure of the problem so that it can be optimized using an interior point MATLAB procedure.

This is essentially a process of feature selection.},
author = {Jung, Daniel Eriksson and Frisk, Erik},
doi = {10.1016/j.automatica.2018.08.006},
issn = {00051098},
journal = {Automatica},
month = {nov},
pages = {143--149},
title = {{Residual selection for fault detection and isolation using convex optimization}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0005109818303960},
volume = {97},
year = {2018}
}
@article{Kohlbrecher2015,
annote = {Team E in the Yanco paper

Multiple operators could request and share information
System is never fully autonomous; human members are supervisors that provide goals, teammates that solve perception, and operators that change execution params.
Perception, Motion Planning, and motion control were handled onboard. Task execution was off-board.
2 ROS systems with 2 ROS build systems. (Aside: sounds like a nightmare on the build)
Three computers made up the Operator Control System - One to monitor connections to the robot, one for the main motion planning and control, and one for perception.
Use CommsBridge to send data robustly to a remote interface. For commands, they use TCP. For high frequency robot state, convert floating point numbers to scaled integer representations, compressed with protobuf, and then sent the result in UDP packets.
Operator's SA is based on intermittent robot position updates
Images are foveated. ROI can be specified by the operator, and the region is sent to the robot; which then returns a higher resolution image within the ROI
Goals for interface:
Allow operator to select the data streams relevant to the task
Since the task is 3D, so need to design for interactions with 3D environment using a 2D interface
Used widgets that could be enabled or disabled based on the task at hand
Used 3D templates to facilitate perception and reduce the bandwidth load on the network
The interface also had a “ghost” robot that showed the robot's plan before executing it
3 different views:
Map view: projection of robot into the map. Also projects any data into the map. Regions of interest can be selected, and then data from that region is sent over
Main view: Visualization of 3D data and fine manipulation control. Contains a ghost robot that can be used for motion planning. Can split 3D view to see multiple perspectives at once.
Camera View: Single image or video feed. Can overlay 3D data in here.
Predefined grasp poses, that could be computed and selected ahead of time. Also allowed for joint level control
Some nodes stored all this robot state on the controller side of the ROS master, and forwarded to clients as needed.
Minimize mouse movement, and provided keyboard shortcuts
However, they still found that too much information clutter was a main point of failure at the DRC
Created a world model server that saved snapshots of the pointclouds and the corresponding tf frames. These snapshots could be queried at any given point in time.

They found that the bottleneck for task execution was on the operator side.},
author = {Kohlbrecher, Stefan and Romay, Alberto and Stumpf, Alexander and Gupta, Anant and von Stryk, Oskar and Bacim, Felipe and Bowman, Doug A. and Goins, Alex and Balasubramanian, Ravi and Conner, David C.},
doi = {10.1002/rob.21558},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {may},
number = {3},
pages = {352--377},
title = {{Human-robot Teaming for Rescue Missions: Team ViGIR's Approach to the 2013 DARPA Robotics Challenge Trials}},
url = {http://doi.wiley.com/10.1002/rob.21558 https://docs.google.com/document/d/1hV6ICOF5n-ba77ct9wo6fZ0GM0L-6EP4J78GzigU{\_}58/edit?usp=sharing},
volume = {32},
year = {2015}
}
@article{Mhenni2018,
annote = {Generation of both FMEA and FTA for a more holistic approach to Safety specification from SysML.

As opposed to prior work, such as that of David et al., the system here converts directly to FMEA/FTA from SysML, without the need for model checkers such as AltaRica. Accomplish this by adding a new diagram type to the SysML spec.

From a skim, the steps are:

1. Create a SysML specification
2. Generate an automated FMEA (and block diagrams?)
3. Update the automated FMEA based on expert input
4. Update the factors in the SysML diagram based on expert input
5. Use the updated block diagrams as graphs to traverse when generating an FTA},
author = {Mhenni, Faida and Nguyen, Nga and Choley, Jean-Yves},
doi = {10.1109/JSYST.2016.2547460},
issn = {1932-8184},
journal = {IEEE Systems Journal},
month = {mar},
number = {1},
pages = {161--172},
title = {{SafeSysE: A Safety Analysis Integration in Systems Engineering Approach}},
url = {http://ieeexplore.ieee.org/document/7458116/},
volume = {12},
year = {2018}
}
@phdthesis{chen2017near,
annote = {The PhD thesis of Yuxin Chen, the author of the Chen et al. Value of Information arXiv work for interactive troubleshooting. There are other applications of the value-of-information metric for active information acquisition that are also explored.},
author = {Chen, Yuxin},
school = {ETH Zurich},
title = {{Near-optimal Adaptive Information Acquisition: Theory and Applications}},
url = {https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/139/1/chen17-phdthesis.pdf},
year = {2017}
}
@article{Fabre2005,
annote = {Deals with the problem of distributed diagnosis - trading off speed for lower memory requirements. There are supervisors with knowledge of local system fault diagnoses; this must be shared with other supervisors part of the same network.

(Read only if required in the future).},
author = {Fabre, Eric and Benveniste, Albert and Haar, Stefan and Jard, Claude},
doi = {10.1007/s10626-005-5238-5},
issn = {0924-6703},
journal = {Discrete Event Dynamic Systems},
month = {mar},
number = {1},
pages = {33--84},
title = {{Distributed Monitoring of Concurrent and Asynchronous Systems}},
url = {http://link.springer.com/10.1007/s10626-005-5238-5},
volume = {15},
year = {2005}
}
@article{PEARL1995,
annote = {Presents a method (do-calculus) for conducting causal inference from observational data. This method relies heavily on causal diagrams - specific types of Bayes' Nets that make modeling assumptions about causality explicit.

Additional help in understanding the work: https://arxiv.org/pdf/1305.5506.pdf

Graphical modeling
- Graphical methods, especially DAGs, are a good way to represent conditional independences between variables. These reduce to recursive product decompositions of the vars: P(X|parents(X))
- d-separation is a very important concept for learning and making inferences in such graphs. (Cheatsheet: http://web.mit.edu/jmn/www/6.034/d-separation.pdf)
- Each child parent relationship represents a noisy autonomous physical mechanism between the corresponding quantities. p(X|parents(X)) = f(*parents(X), independent noise)
- Actually, imposing the functional form on the relationship between cause and effect creates conditional independence structures, which then decompose naturally to the DAG structure of Bayes Nets.
- The functional form is more powerful, because it allows for counterfactual investigations by setting the values of the function inputs. The process of setting the value of the intervention in all other PD function inputs is called a causal effect.
- The main operation of performing a causal intervention, assuming X1 is intervened, is: Pr(X1,X2,...)/Pr(X1|parents(X1)). Essentially, from the original joint distribution, remove the node and its parents, as they are no longer dependent on other parts of the graph.

Controlling for confounds
- Confounds are the set of unknown variables that might be responsible for the true causal mechanism between two variables.
- There are three types of nodes in a causal graph: the query variables (the variables we want to verify the causal relationship between), the unobservable variables (variables that our observational data does not measure), and the concomitants (these are observable variables beyond the query variables that help reduce confounding biases).
- Ignorability: condition that renders a set of concomitant variables sufficient for identifying a causal effect. In a graph, this condition is captured by the back-door criterion, which essentially states that the concomitant set of variables should not contain variables dependent on the cause, and should separate the effect from having a cause on the cause. Check Fig. 2
- Identifiability: a causal model between two variables is identifiable if it is uniquely determined given the observational data. If the observations contain concomitant variables satisfying the back-door criterion, then the causal effect is identifiable.
- If observable covariates satisfying the back-door criterion cannot be found, then use the rules of conditional independence (the front-door criterion) to find the causal effect between variables, if possible. (TODO: It is unclear to me why the conditional rules formulated in the paper work the way they do).

Do-Calculus
- There are three rules to the calculus: insertion/deletion of observations, exchange of actions and observations, {\&} insertion/deletion of actions.

Perhaps the following paper is more clear on the derivation of do-calculus?},
author = {Pearl, Judea},
doi = {10.1093/biomet/82.4.669},
issn = {0006-3444},
journal = {Biometrika},
number = {4},
pages = {669--688},
title = {{Causal diagrams for empirical research}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/82.4.669 http://bayes.cs.ucla.edu/R218-B.pdf},
volume = {82},
year = {1995}
}
@inproceedings{Durand2010a,
annote = {(COTAMA)

Robustness and fault tolerance are dependent on situation detection, diagnosis, and recovery.

Fault detection:
- timing checks, watchdogs
- reasonableness checks
- safety-bag checks (command verification)
- model-based monitoring and diagnosis
Model-based systems are preferred, but they might not be able to handle large state-spaces. To localize the failed component, can use a failure dependency graph

Fault recovery: usually stop and restart the robot, or its failing components

Proposed method for fault detection and diagnosis:

1. Identification - FMECA. Identify the failure modes of functions and their corresponding severity; implies a functional decomposition of the system.
1. Detection - Create an observer for different control modules.
1. Reaction - Based on FMECA criticality analysis, reaction to the fault can be appropriate

Identification of failure severity: Weak, Medium, Hard, Fatal (look at journal paper).

Causal inference of the fault is done with an Ishikawa cause-effect diagram. There are no details on the incidence matrix.},
author = {Durand, B and Godary-Dejean, K and Lapierre, L and Crestani, D},
booktitle = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2010.5649337},
isbn = {978-1-4244-6674-0},
month = {oct},
pages = {1018--1023},
publisher = {IEEE},
title = {{Global methodology in control architecture to improve mobile robot reliability}},
url = {http://ieeexplore.ieee.org/document/5649337/ https://hal-lirmm.ccsd.cnrs.fr/lirmm-00547867/document},
year = {2010}
}
@inproceedings{Sauppe2015,
address = {New York, New York, USA},
annote = {Long term study of Baxter deployed to multiple work environments. There are a lot of lessons to take away from this, the biggest ones for this project (causal-isolation) are:
1. Operators and maintenance staff often did not have enough context (even when collocated) to know what was wrong with the robot and how to fix it. So a diagnosis module is greatly needed. Aside output of the dianosis could either be speech or visual (on the face).
2. Operators viewed the robot as a teammate; maintenance viewed it as equipment.
3. Would be nice to answer "What's wrong?" or "How do I fix it?"},
author = {Saupp{\'{e}}, Allison and Mutlu, Bilge},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems - CHI '15},
doi = {10.1145/2702123.2702181},
isbn = {9781450331456},
pages = {3613--3622},
publisher = {ACM Press},
title = {{The Social Impact of a Robot Co-Worker in Industrial Settings}},
url = {http://dl.acm.org/citation.cfm?doid=2702123.2702181},
year = {2015}
}
@article{Wei2017,
annote = {Aside: k-out-of-n configuration of a system - for the overall system to be functional, atleast k out of a total n components must be working.

Goal: sequencing tests in a k-out-of-n system with precedence constraints and with imperfect tests

Contributions:
- model for the above problem domain
- use tabu search (do not revisit states) as an efficient method for finding good solutions
- use importance sampling during search as a method of evaluation.

Framework:
- N components must be sequentially tested
- Each component tested at most once
- Components are in 1 of 2 states
- System is functional if atleast k components return OK
- Tests of each component can be imperfect with false positives and false negatives
- Tests have costs
- There exists a partial ordered graph that dictates which tests can be conducted before others
- The goal is to create a sequence of tests that honours the partial ordering such that a threshold confidence level of true positives {\textgreater}= k is reached (or until all components are tested)
- Also minimize the total cost of the tests

Method:
- Formulate the problem as an optimization problem to be solved with MCTS
- In order to reduce the variance in the estimated parameters, use importance sampling

Results and the exact details of the algorithm can be examined if need be.},
author = {Wei, Wenchao and Li, Hongbo and Leus, Roel},
doi = {10.1016/j.dss.2017.09.009},
issn = {01679236},
journal = {Decision Support Systems},
month = {nov},
pages = {104--116},
title = {{Test sequencing for sequential system diagnosis with precedence constraints and imperfect tests}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S016792361730177X},
volume = {103},
year = {2017}
}
@article{David2010,
annote = {(Well-cited. Also good citations for UML -{\textgreater} Fault trees)

Propose a three-step deductive method for reliability analysis:
- From a system spec, deduce dysfunctional behaviour using an FMEA and identify the impacted requirements
- Construct a model integrating the functional and dysfunctional behaviours using a formal language
- Analyze and quantify the dysfunctional behaviour

Assumptions:
- Model expressing the architecture and functional behaviour of the system exists (can be started with PHA, if necessary)
- Developers have access to the behaviour of components usually used in design.

By encouraging the use of SysML, they want to "nudge" designers to use taxonomies of failures when conducting their FMEA.

Using various parts of the SysML specification, the system generates preliminary FMEAs. Then these can be iterated upon to create a final FMEA.},
author = {David, Pierre and Idasiak, Vincent and Kratz, Fr{\'{e}}d{\'{e}}ric},
doi = {10.1016/j.ress.2009.11.015},
issn = {09518320},
journal = {Reliability Engineering {\&} System Safety},
month = {apr},
number = {4},
pages = {431--450},
title = {{Reliability study of complex physical systems using SysML}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0951832009002671},
volume = {95},
year = {2010}
}
@inproceedings{Chai2018,
address = {California},
annote = {Overview of research conducted by Joyce Chai, which demonstrates the power of causal representations for language grounding. Gist of the paper follows.

There are two types of language grounding:

1. Semantic Grounding - Grounds an agent's percepts and actions to concepts in its mental model. Technically, this is true for humans too, but we have no way of affecting a human's grounding of terms directly.
- Grounding to perception - Involves grounding entities in a language instruction to the actor, the patient, and the instrument. Also, some of those things might be implicit and might have to be inferred from a multi-modal input.
- Grounding to action - An instruction provided through language also needs to be ground to a robot's basic action capabilities. Depending on the level of abstraction, such grounding could involve large-ish subtask plans.

2. Communicative Grounding is required for a channel of communication. Implicit and explicit confirmation is key to establishing common ground. One of the challenges to the prior of the common ground is the preseumed knowledge of each communicative agent. Argument of this paper: causal representation can provide a robust communicative grounding.

Causal Representations of Verbs:
- An action is a verb-noun pair.
- An effect can be represented in various ways, for example dimensions of state change, language and phrase descriptions, or even predicate calculus.
- A causal representation of verbs is a mapping from actions to effects.
- The representation of effects allows for different capabilities.

Some results (more details with other papers in this section):
- Grounding actions to physical dimensions (shape, color, texture, etc) of state change can allow for the prediction of affordances (actions) given a noun. Gao et al. 2016.
- With the above grounding, one can also predict the actions that might cause the dimensions of a state to change. (Although more work needs to be done here, and more data needs to be collected). Especially powerful is the ability to infer the missing steps in an instruction that were implicit in the instruction. Gao et al. 2018
- The mapping of action effects to state change semantics, when represented through logical constructs, can allow for incremental grounding of verbs to a hypothesis space of logical constructs. The logical constructs can then be used for planning. She {\&} Chai 2016
- Incremental acquisition of a mapping is much better than all-at-once acquisition of knowledge of a task, especially for the end task model. However, it does take longer. Therefore, the robot needs to be better about asking questions when collecting incremental knowledge. She {\&} Chai 2017. This is motivation for intelligent active learning.},
author = {Chai, Joyce Y. and Gao, Qiaozi and She, Lanbo and Yang, Shaohua and Saba-Sadiya, Sari and Xu, Guangyue},
booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2018/1},
isbn = {9780999241127},
month = {jul},
pages = {2--9},
publisher = {International Joint Conferences on Artificial Intelligence Organization},
title = {{Language to Action: Towards Interactive Task Learning with Physical Agents}},
url = {https://www.ijcai.org/proceedings/2018/1},
year = {2018}
}
@inproceedings{Tomatis2003,
annote = {Citation for the fact that most robot errors are software related.

4086 critical errors
3216 (78{\%}) errors are a result of PC program (software)
694 (16{\%}) errors are from localization (software)
176 errors were from hardware

Also can use this as a reference to the fact that bugs stabilize after deployment.},
author = {Tomatis, N. and Terrien, G. and Piguet, R. and Burnier, D. and Bouabdallah, S. and Arras, K.O. and Siegwart, R.},
booktitle = {IEEE International Conference on Robotics and Automation (Cat. No.03CH37422)},
doi = {10.1109/ROBOT.2003.1242256},
isbn = {0-7803-7736-2},
pages = {4246--4251},
publisher = {IEEE},
title = {{Designing a secure and robust mobile interacting robot for the long term}},
url = {http://ieeexplore.ieee.org/document/1242256/},
volume = {3},
year = {2003}
}
@article{Martins2016,
annote = {Based on Systematic Literature Review.

Distribution of approaches used for safety specification:
- FTA and FMEA are the most common means of specifying safety requirements.
- Methods such as Goal Structuring Notation (GSN) and Requirements state machine language (RSML) are gaining in popularity.
- SysML and UML are also used.
- Formal methods are not preferred by practitioners, and informal methods such as FTA, FMEA are still much preferred.

Conclusions:
- Traditional methods remain the most dominant, but new methods such as RSML and GSN are gaining in popularity
- The new methods need more validation.
- The new methods also need more training
- Until those happen, the traditional methods will dominate.},
author = {Martins, Luiz Eduardo G. and Gorschek, Tony},
doi = {10.1016/j.infsof.2016.04.002},
issn = {09505849},
journal = {Information and Software Technology},
month = {jul},
pages = {71--89},
title = {{Requirements engineering for safety-critical systems: A systematic literature review}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584916300568},
volume = {75},
year = {2016}
}
@article{Trave-Massuyes2014,
annote = {Although the paper is new, the work it cites is quite old - took 2 years to get the paper published.

Notes copied from the Research Document


Diagnosis is often comprised of detection, isolation (localization), and identification (type of fault and the magnitude of the fault's effects)
The paper focuses on model-based diagnosis
Data-driven detection and diagnosis is often complementary to a model-based approach
Challenges to data-based arise from scarce instrumentation, or from too much data
Fault Detection and Isolation (FDI) Introduction - Controls
Rely on behavioural models that establish constraints between a set of measured variables (inputs and outputs) and a set of unmeasured internal variables.
All the methods are equivalent
The fundamental problem is to generate residuals - scalars that tend to 0 if the observed trajectory of measured variables is consistent with the system model
Methods are based on:
Parameter estimation of the models - dependent on the identifiability (conditions in dynamical systems theory specifying if parameters can be identified)
State estimation of the unknown variables - often performed through filters and observers
Parity space diagnosis that eliminates unknown variables by the use of relations - called Analytical Redundancy Relations (ARR). These are relations between measured variables (and their derivatives) that are satisfied only if the system model is consistent.
An ARR is a vector of residuals for each (set of) measured variables (or their derivatives). ARRi=[ri(z1)  ri(z2)  ri(z3) ... ]
A fault signature is the set of residuals that correspond to a given fault. FSj=[r1(zj)  r2(zj)  r3(zj)  ...]T
A signature matrix is therefore a matrix that comprises of ARRs as rows and fault signatures as columns.
Diagnosis (DX) Theory Introduction - AI
Assume a system description, a set of components, and a set of observations. Also assume a predicate on components to specify abnormal behaviour
Components are said to be faulty if assuming them and faulty and others as normal leads to observations that are consistent with the system definition
As specified by Reiter, using the concept of hitting sets of conflict sets, the goal is to find the minimum set of components that are faulty. Reasons:
Parsimony and Occam's Razor
All supersets of minimal diagnoses are themselves, also diagnoses
Comparing FDI and DX
Differences
FDI does not make explicit use of components
DX explicitly represents nominal behaviour with the use of (the negation of) the operator AB
To make the representations equivalent, a System Representation Equivalence needs to be made
Assume that faults are only a result of component failures
Define the support of an ARR as the subset of components that are diagnosed by the ARR. This is possible because of the above assumption
Given the assumption and the definition
ARR violations with FDI methods are equivalent to the conflict sets used in DX
Fault detectability is specified by d-completeness of a set of ARRs - For all incorrect observations, there exists an ARR that will detect the inconsistency
Fault isolability is specified by the i-completeness of a set of ARRs - For all inconsistent components, there exists an ARR that is also violated that has the inconsistent component(s) in its support.
With the above properties
ARRs are redundant only if they have the same signature AND the same support.
Assuming that no prior knowledge rules out failures in a component (exonerates it), the diagnosis from FDI is the same as that from DX
FDI generally deals with single faults. DX considers multiple faults by default, but the number of diagnoses is exponential in the number of faults, so there must be a preference criteria on the generated diagnoses
ARRs are generally determined offline with an online consistency check. DX is almost always used online, and hence more flexible, but also more computationally intensive
Connecting FDI and DX
Causal-model based diagnosis
Create a graph structure, causal graph, by explicitly modeling dependencies between variables / components. Edges in the graph are called influences.
Use FDI for fault detection and DX approaches for isolation
Unlike standard DX methods, dependencies leveraged by the causal models are exhibited from the outset
Using this method:
Use FDI techniques to generate a residual - detection
Diagnostic reasoning is then a global procedure that is underpinned by the causal structure
A misbehaving variable is a result of violations of one any of the influences to the variable.
Extensions: include a time delay in the influence exerted by one component on another.
Hybrid Systems based diagnosis
Relevant to the diagnosis problem because switching dynamics is often representative of a fault event
The goal is to identify the discrete state transition dynamics based on the observed continuous measured variables.
Use the FDI methods at their core to generate hypotheses of possible fault/no-fault discrete states, and use DX methods to efficiently search through the hypothesis space of all discrete states.
Methods:
Multiple model estimation:
System is represented as a system of differential equations parameterized by Markovian parameters that follow an unknown transition matrix
Avoids exponential growth of the number of hypotheses by merging them
Particle filters:
Optimal Bayesian estimate of faults given events, and can model any type of probability distribution
Difficult to apply to diagnosis because probability of faults is inherently very low; need a very large number of particles
Other dedicated methods:
Other methods suffer from a problem of hypothesis loss, which is unacceptable for fault diagnosis
Propose instead search guided by fault rank heuristics, or making partial changes, or using a search through event space. The last is expanded upon.
Extending with Data-driven methods
Two possible methodologies
Assess state of the system and provide a diagnosis
Generate models, which are painful to generate for FDI and DX, automatically from the data. The paper chooses this
Models can be learned as chronicles - requires an abstraction of traces of a system to discrete events. Then a chronicle is a known type of event that occurs within certain timing constraints
The goal of learning a model is then to learn chronicles given data about the system
Extending FDI and DX
Plan diagnosis - diagnosis can help during the planning process and it isn't always leveraged then.
Active diagnosis - plan to take actions that assist in diagnosis in the case of failure. Further terminology: Pervasive diagnosis, Informative planning, safe diagnosability, active safe diagnosis},
author = {Trav{\'{e}}-Massuy{\`{e}}s, L.},
doi = {10.1016/j.engappai.2013.09.018},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
month = {jan},
pages = {1--16},
title = {{Bridging control and artificial intelligence theories for diagnosis: A survey}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0952197613001929 https://drive.google.com/file/d/1CXhk08Tjo4mNLO5ng8QyQTd{\_}-OA2rIWm/view?usp=sharing https://docs.google.com/document/d/1uxM1ZNC{\_}BqdM7AGrHiS2bYLFSOQoSvbVbNj3Hxudr2E/edit},
volume = {27},
year = {2014}
}
@article{Hu2016,
abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
annote = {Using logical rules as a regularizer for training a neural network on sparse data can often lead to much better performance than trying complex architectures, etc.

The goal is to create twin-networks - a teacher and student. The teacher creates a probability distribution of the student's projection of the data based on "soft-logic" rules. The teacher's distribution is then used to regularize the student's function.

The loss function for determining the closeness of the teacher and student's distributions is a form of KL-divergence.

The teacher and student are trained iteratively. This is almost a cooperative spin on GANs.

Note to self: Interesting concept and I can explore it in the future should I need to implement something akin to this system.},
archivePrefix = {arXiv},
arxivId = {1603.06318},
author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
eprint = {1603.06318},
month = {mar},
title = {{Harnessing Deep Neural Networks with Logic Rules}},
url = {http://arxiv.org/abs/1603.06318},
year = {2016}
}
@inproceedings{Atkeson2015,
annote = {Safety methods and operator interface allowed operators to recover from all difficult situations. Only team in the DRC that attempted all tasks and did not fall, or require a reset.

Strategy: “Slow and Steady” - Go slow to reduce the risk of falling, and for fall prediction/safety code to kick in if things went wrong. Assumption: others would go too fast, leading to operator and robot errors

Used an algorithm (details in the paper) to predict when a safety critical anomaly (especially one resulting in a fall) might be happening. When detected, all other controllers are paused and manual recovery is requested.

Interface for manual control:
Operators could specify tasks and task components. Or specify movements or velocities in joint coordinates.
Direct teleop and simple grasping operations were used to perform surprise tasks
Stored behaviour replay was used
Targets could be specified in displayed images and on point clouds - LIDAR was fused with stereo, and operator “scribbles” were used for object segmentation

Lessons:
Simply stopping what the robot was doing often gave operators enough time to successfully intervene.
Would be good to be able to classify robot internal errors vs. external errors because the responses to each are different.
Sensing is cheap. Redundant sensor systems make calibration and the detection of errors much easier.
Related: superhuman sensing should be strived for
We need to leverage contact with the world, not shun all contact.
Operators want to control the robot at many levels
Operator error under time pressure was the cause of most DRC failures
Implement an “undo” for when errors happen
Interfaces need to be idiot proof, require no typing, have no check boxes, and minimize the options the operator has to choose between (paper says to minimize the information displayed too, but I disagree with that)
Behaviours are often fragile and often overfit to the test bed they were developed within
Beeps once over a threshold, and increasing beeps the more dangerous or uncertain a robot is, anecdotally improved operator attentional load.},
author = {Atkeson, Christopher G. and Babu, B. P. W. and Banerjee, N. and Berenson, D. and Bove, C. P. and Cui, X. and DeDonato, M. and Du, R. and Feng, S. and Franklin, P. and Gennert, M. and Graff, J. P. and He, P. and Jaeger, A. and Kim, J. and Knoedler, K. and Li, L. and Liu, C. and Long, X. and Padir, T. and Polido, F. and Tighe, G. G. and Xinjilefu, X.},
booktitle = {2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)},
doi = {10.1109/HUMANOIDS.2015.7363436},
isbn = {978-1-4799-6885-5},
month = {nov},
pages = {623--630},
publisher = {IEEE},
title = {{No falls, no resets: Reliable humanoid behavior in the DARPA robotics challenge}},
url = {http://ieeexplore.ieee.org/document/7363436/ https://www.cs.cmu.edu/{~}cga/drc/ https://docs.google.com/document/d/1hV6ICOF5n-ba77ct9wo6fZ0GM0L-6EP4J78GzigU{\_}58/edit?usp=sharing},
year = {2015}
}
@article{meganck2006learning,
annote = {Structure learning of causal models. Given different types of graphs, use a partial graphical model specification to decide the experiments to run in order to arrive at the underlying causal model.

Part of this method involves substituting latent variables with bidirectional edges, but I don't quite understand it.},
author = {Meganck, Stijn and Maes, Sam and Leray, Philippe and Manderick, Bernard},
title = {{Learning semi-markovian causal models using experiments}},
year = {2006}
}
@inproceedings{Short2019,
annote = {TODO: Get notes. At a high level, this is a novel way to create disturbances to robot task in order to learn more robust task models.},
author = {Short, Elaine Schaertl and Allevato, Adam and Thomaz, Andrea L.},
booktitle = {2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
doi = {10.1109/HRI.2019.8673019},
isbn = {978-1-5386-8555-6},
month = {mar},
pages = {468--477},
publisher = {IEEE},
title = {{SAIL: Simulation-Informed Active In-the-Wild Learning}},
url = {https://ieeexplore.ieee.org/document/8673019/},
year = {2019}
}
@article{Li2017,
annote = {Theoretical paper on creating a fault estimator in the face of unknown transition rates between modes in a Markov Jump System. The only assumption (I think) is that the system outputs are Lipschitz continuous. The solution involves including an intermediate observer (I think)},
author = {Li, Li-Wei and Yang, Guang-Hong},
doi = {10.1080/00207721.2016.1216199},
issn = {0020-7721},
journal = {International Journal of Systems Science},
month = {mar},
number = {4},
pages = {805--817},
title = {{Fault estimation for a class of nonlinear Markov jump systems with general uncertain transition rates}},
url = {https://www.tandfonline.com/doi/full/10.1080/00207721.2016.1216199},
volume = {48},
year = {2017}
}
@article{Guiochet2017,
annote = {Lots of references to other works that we should atleast check.

- Introduces the concept of "advanced robots" that are robots distinct from industrial robots, with greater degrees of interaction and decisional autonomy
- Advanced robots have multiple new failure modes and potential hazards that industrial robots do not have
- When autonomy increases, so does the software complexity and the likelihood it contains faults
- From a short survey, most robotic faults are software driven.
- A safety culture and widely accepted methods for certification of robots are lacking
- Software in advanced robots is mainly component based
- Testing is preferred, but it is difficult to test beyond scenario-based testing. And scenario-based testing provides limited coverage; more suitable for debugging than validation
- It is difficult to define suitable test oracles for autonomous systems
- Static verification of a system is possible; more realistic to perform dynamic verification however.
- Static verification often suffers from problems of a) error-prone modeling, b) model representativity, and c) combinatory explosion.
- Fault forecasting is often done thorugh FMEA (bottom-up) and FTA (top-down)
- Several challenges of existing fault forecasting methods for robots:
a) causality analysis is limited due to complexity and non-determinism of the decisional layer
b) probabilities of unwanted events are hard to estimate
c) hazardous situations can occur due to sequences of the decisions instead of a logical combination of events
d) uncertainty in perception, heuristics, etc. can lead to faults and this can be hard to analyze (current methods propagate faults)
- Fault checking is often done through means of monitors - timing monitors, execution monitors, belief monitors, and catch-all safety monitors},
author = {Guiochet, J{\'{e}}r{\'{e}}mie and Machin, Mathilde and Waeselynck, H{\'{e}}l{\`{e}}ne},
doi = {10.1016/j.robot.2017.04.004},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
month = {aug},
pages = {43--52},
title = {{Safety-critical advanced robots: A survey}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0921889016300768},
volume = {94},
year = {2017}
}
@article{Ait-Kadi2018,
annote = {Assuming knowledge of test probabilities and knowledge of tests to different components separated as groups, can we figure out a way of scheduling an optimal test sequence to isolate an error in an embedded system? Answer: yes we can.

The method uses some set-cover theories and reasoning about the costs of different schedules. The paper is mostly theoretical.},
author = {A{\"{i}}t-Kadi, Daoud and Simeu-Abazi, Zineb and Arous, Ahmed},
doi = {10.1007/s10845-015-1088-7},
issn = {0956-5515},
journal = {Journal of Intelligent Manufacturing},
month = {mar},
number = {3},
pages = {641--649},
title = {{Fault isolation by test scheduling for embedded systems using a probabilistic approach}},
url = {http://link.springer.com/10.1007/s10845-015-1088-7},
volume = {29},
year = {2018}
}
@inproceedings{she2017interactive,
annote = {Robots need to continuously learn the meanings of new verbs and what those verbs mean in the action representations of human partners. Pitfalls of prev work: a) assume perfect representation of the environment and no sensor noise b) multiple demonstrations with no social interaction. This work introduces interactive learning form of the previous work that can also deal with noisy perception.

Notes:
- Compared to procedure-based representations, state-based representations allow for planning at execution time (closed loop control)
- Leverage the concept of label queries, goal queries, etc. within an MDP to ask the right questions at the right time to improve hypothesis space induction.
- For evaluation, they used a simulated human learner.

Results:
- It is best to account for perceptual uncertainty
- Interactive learning is best.},
author = {She, Lanbo and Chai, Joyce},
booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
pages = {1634--1644},
title = {{Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication}},
url = {http://www.cse.msu.edu/{~}jchai/Papers/ACL2017.pdf},
volume = {1},
year = {2017}
}
@article{He2013,
annote = {Isolate 8 different types of faults in a manufacturing process with 2 different decision trees. DT1 to detect if there is a fault and DT2 to isolate the fault and indicate the parameters that are responsible (explainability).

Assuming you know the data generating distribution, then the decision tree can be trained on the simulated data from the data generating distribution.},
author = {He, Shu-Guang and He, Zhen and Wang, Gang A.},
doi = {10.1007/s10845-011-0533-5},
issn = {0956-5515},
journal = {Journal of Intelligent Manufacturing},
month = {feb},
number = {1},
pages = {25--34},
title = {{Online monitoring and fault identification of mean shifts in bivariate processes using decision tree learning techniques}},
url = {http://link.springer.com/10.1007/s10845-011-0533-5},
volume = {24},
year = {2013}
}
@book{pearl2009causality,
annote = {Pearl's Causality textbook for references to do-calculus should I need it.},
author = {Pearl, Judea},
publisher = {Cambridge university press},
title = {{Causality}},
year = {2009}
}
@article{Podgurski1990,
annote = {TARANTULA debugging tool that is often used as a baseline in the software debugging papers.},
author = {Podgurski, A. and Clarke, L.A.},
doi = {10.1109/32.58784},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
number = {9},
pages = {965--979},
title = {{A formal model of program dependences and its implications for software testing, debugging, and maintenance}},
url = {http://ieeexplore.ieee.org/document/58784/},
volume = {16},
year = {1990}
}
@article{Bellala2013,
annote = {(Second link is to a ZIP file of the supp. material)

Contributions:
- Use single fault diagnoses based on ranked lists and the AUC of the fault classification for all types of fault classification. Do not use belief progation (Zheng et al. 2005), information gain, or explicit multiple fault diagnosis
- Belief propagation used for multiple faults is not scalable, doesn't converge, and can oscillate
- Information gain does not generalize to multiple faults when assuming only a single fault. Also requires an accurate noise model

Insights:
- Domain expert is better suited to selecting top-t diagnoses given more knowledge of the situation
- By presenting rankings and cutting off via threshold, we can still use a single fault diagnosis outputs in a multiple fault scenario.
- AUC vs. Info-gain hypothesis - info-gain exploits noise model early on while AUC explores before exploiting

Method:
- Need a bipartite graph model relating tests to faults. Also need the CPT for the fault model.
- Both AUC and Entropy only need the posterior probabilities of faults
- In the single fault scenario, the posterior probabilities of faults are easy to calculate with Bayes rule; no need for belief propagation
- When estimating the AUC, use upper rectangles among the possible choices. Apparently explained in the supplementary material
- For the noisy case, AUC doesn't need knowledge of the noise parameters, but it's a bit unclear how to calculate the AUC metrics. I think it uses Hamming distances.},
author = {Bellala, G. and Stanley, J. and Bhavnani, S. K. and Scott, C.},
doi = {10.1109/TPAMI.2013.30},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {sep},
number = {9},
pages = {2078--2090},
title = {{A Rank-Based Approach to Active Diagnosis}},
url = {http://ieeexplore.ieee.org/document/6420840/ https://ieeexplore.ieee.org/ielx5/34/6560338/6420840/ttp2013990012.zip?tp={\&}arnumber=6420840 https://drive.google.com/file/d/1hvfrgs1aLenXmj6QWIprlz4cgTq2-7eC/view},
volume = {35},
year = {2013}
}
@inproceedings{Scheutz2007a,
annote = {Paper is about the fault detection and recovery of DIARC components. Basically DIARC consists of host and registry servers that periodically send heartbeats. In the event of a component failure, some predefined Prolog constraints on the functions, locations, etc. of the system allows DIARC to either bring nodes up, substitute one node for another, or signal an error. The constraints and logic for the program can also be used inside DIARC components for introspection of available robot capabilities.

In the one result that this paper has, the authors compared fault scenarios to non-fault ones and found that there was no noticeable difference in the execution time.},
author = {Scheutz, Matthias and Kramer, James},
booktitle = {Proceedings 2007 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2007.364045},
isbn = {1-4244-0602-1},
issn = {1050-4729},
month = {apr},
pages = {3699--3704},
publisher = {IEEE},
title = {{Reflection and Reasoning Mechanisms for Failure Detection and Recovery in a Distributed Robotic Architecture for Complex Robots}},
url = {http://ieeexplore.ieee.org/document/4209663/},
year = {2007}
}
@article{Johansson2016,
abstract = {Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, "Would this patient have lower blood sugar had she received a different medication?". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.},
annote = {Approach the idea of counterfactual inference as one of domain transfer. Essentially, the goal is to add to the loss function during training such that:

1. The normal loss of the data to observed outcomes is minimized (the normal method)
2. The loss of the function on data points that are "near" each other but have different classes is also minimized
3. A notion of "discrepancy" in the class of functions is minimized - to the best of my understanding, this is the maximum distance between two functions from the same hypothesis class evaluated on the factual and counterfactual distributions

The paper provides theoretical bounds on the performance of the counterfactual inference.

The result of adding the loss function is a manner of feature selection where imbalanced features are selected based on a tradeoff between predictive power and its imbalance in the dataset.},
archivePrefix = {arXiv},
arxivId = {1605.03661},
author = {Johansson, Fredrik D. and Shalit, Uri and Sontag, David},
eprint = {1605.03661},
month = {may},
title = {{Learning Representations for Counterfactual Inference}},
url = {http://arxiv.org/abs/1605.03661},
year = {2016}
}
@article{Settles2012,
author = {Settles, Burr},
doi = {10.2200/S00429ED1V01Y201207AIM018},
file = {:home/banerjs/Documents/Papers/Settles - 2012 - Active Learning.pdf:pdf},
issn = {1939-4608},
journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
month = {jun},
number = {1},
pages = {1--114},
title = {{Active Learning}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00429ED1V01Y201207AIM018},
volume = {6},
year = {2012}
}
@inproceedings{Butterfield2010,
annote = {// TODO},
author = {Butterfield, Jesse and Osentoski, Sarah and Jay, Graylin and Jenkins, Odest Chadwicke},
booktitle = {2010 10th IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/ICHR.2010.5686284},
isbn = {978-1-4244-8688-5},
keywords = {todo-add-notes},
mendeley-tags = {todo-add-notes},
month = {dec},
pages = {328--333},
publisher = {IEEE},
title = {{Learning from demonstration using a multi-valued function regressor for time-series data}},
url = {http://ieeexplore.ieee.org/document/5686284/},
year = {2010}
}
@article{Wu2018a,
abstract = {Robot manipulation is increasingly poised to interact with humans in co-shared workspaces. Despite increasingly robust manipulation and control algorithms, failure modes continue to exist whenever models do not capture the dynamics of the unstructured environment. To obtain longer-term horizons in robot automation, robots must develop introspection and recovery abilities. We contribute a set of recovery policies to deal with anomalies produced by external disturbances as well as anomaly classification through the use of non-parametric statistics with memoized variational inference with scalable adaptation. A recovery critic stands atop of a tightly-integrated, graph-based online motion-generation and introspection system that resolves a wide range of anomalous situations. Policies, skills, and introspection models are learned incrementally and contextually in a task. Two task-level recovery policies: re-enactment and adaptation resolve accidental and persistent anomalies respectively. The introspection system uses non-parametric priors along with Markov jump linear systems and memoized variational inference with scalable adaptation to learn a model from the data. Extensive real-robot experimentation with various strenuous anomalous conditions is induced and resolved at different phases of a task and in different combinations. The system executes around-the-clock introspection and recovery and even elicited self-recovery when misclassifications occurred.},
annote = {TODO: Write down or scan notes from this paper. This was used as the baseline in the failed user study.

At a high level, the paper:
1. Creates a manipulation state machine and detects faults using the log-likelihood of HMMs trained on nominal vs. faulty execution
2. Fault isolation happens by training HMMs for each class of fault and checking the log-likelihood of observed sequences to the HMM's prediction
3. Recovery occurs to human chosen points in the manipulation state machine. During recovery new states can be added to the state machine, and the process is recursive.

The dataset is available online, and the code uses bnpy as its underlying framework for modeling the HMM.},
archivePrefix = {arXiv},
arxivId = {1809.03979},
author = {Wu, Hongmin and Luo, Shuangqi and Chen, Longxin and Duan, Shuangda and Chumkamon, Sakmongkon and Liu, Dong and Guan, Yisheng and Rojas, Juan},
eprint = {1809.03979},
month = {sep},
title = {{Endowing Robots with Longer-term Autonomy by Recovering from External Disturbances in Manipulation through Grounded Anomaly Classification and Recovery Policies}},
url = {http://arxiv.org/abs/1809.03979 http://www.juanrojas.net/re{\_}enact{\_}adapt/},
year = {2018}
}
@inproceedings{she2014back,
annote = {From Abstract: Approach for robotic arm to learn new actions. Three-tier action knowledge representation. Supports connection between symbolic language and continuous sensorimotor robot representations, and supports application of planning to novel situations. Step-by-step instructions also led to better learning than one-shot instructions.

System design paper that describes the interaction between human and robot to allow the robot to learn a block stacking task.

Levels of actions:
- Pick, Move, Place -{\textgreater} Trajectories
- Action specs from humans -{\textgreater} Pick, Move, Place

High level actions are modeled as desired goal states.

Results:
- Collaborative instruction from a human takes more time, but leads to much better task execution performance.
- Assumption, this is true for the current setup and paradigms. I want to say that the findings generalize, but that would not be supported by the data.},
author = {She, Lanbo and Yang, Shaohua and Cheng, Yu and Jia, Yunyi and Chai, Joyce and Xi, Ning},
booktitle = {Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)},
pages = {89--97},
title = {{Back to the blocks world: Learning new actions through situated human-robot dialogue}},
url = {http://www.aclweb.org/anthology/W14-4313},
year = {2014}
}
@inproceedings{FigueroaRivera2019,
annote = {DO NOT USE

Proposed architecture for a diagnosis and recovery system that straight-up plagiarizes the architectures used by Crestani et al. and RoSHA (though to be fair, it looks like RoSHA is a proposal only).},
author = {{Figueroa Rivera}, Luis J. and {Ramos Ruiz}, Ariel Y. and Chandrasekaran, Balasubramaniyan},
booktitle = {2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)},
doi = {10.1109/CCWC.2019.8666570},
isbn = {978-1-7281-0554-3},
month = {jan},
pages = {0435--0439},
publisher = {IEEE},
title = {{A Robotic Control System for Fault Tolerance and Safety using Human Robot Interaction}},
url = {https://ieeexplore.ieee.org/document/8666570/},
year = {2019}
}
@inproceedings{Zaman2013,
annote = {1. Listens to observations as FOL clauses on an observations topic
2. Given a model of a system, fault is detected if current observations contradict the model.
3. A hitting set algorithm then tries to find the set of diagnoses that resolve the contradiction.

Check out the ROS implementation},
author = {Zaman, Safdar and Steinbauer, Gerald and Maurer, Johannes and Lepej, Peter and Uran, Suzana},
booktitle = {2013 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6630618},
isbn = {978-1-4673-5643-5},
month = {may},
pages = {482--489},
publisher = {IEEE},
title = {{An integrated model-based diagnosis and repair architecture for ROS-based robot systems}},
url = {http://ieeexplore.ieee.org/document/6630618/ http://wiki.ros.org/tug{\_}ist{\_}model{\_}based{\_}diagnosis},
year = {2013}
}
@article{Liu2016,
abstract = {Attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition. In this work, we propose an attention-based neural network model for joint intent detection and slot filling, both of which are critical steps for many speech understanding and dialog systems. Unlike in machine translation and speech recognition, alignment is explicit in slot filling. We explore different strategies in incorporating this alignment information to the encoder-decoder framework. Learning from the attention mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such attentions provide additional information to the intent classification and slot label prediction. Our independent task models achieve state-of-the-art intent detection error rate and slot filling F1 score on the benchmark ATIS task. Our joint training model further obtains 0.56{\%} absolute (23.8{\%} relative) error reduction on intent detection and 0.23{\%} absolute gain on slot filling over the independent task models.},
annote = {In the problem of "slot filling" where input tokens have to be assigned to "slots", attention can help greatly. Bidirectional RNNs also seem to help, but perhaps not as much as attention in helping align the inputs.},
archivePrefix = {arXiv},
arxivId = {1609.01454},
author = {Liu, Bing and Lane, Ian},
eprint = {1609.01454},
month = {sep},
title = {{Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling}},
url = {http://arxiv.org/abs/1609.01454},
year = {2016}
}
@inproceedings{tenenbaum2001structure,
annote = {Human causal learning needs to be explained as more than just parameter learning in a fixed graph structure; instead, it needs to account for changing the graph structure itself.

Experiments with three different datasets shows that learning the graph structure is much more in line with the human data than simply trying to learn the parameter estimates.},
author = {Tenenbaum, Joshua B and Griffiths, Thomas L},
booktitle = {Advances in neural information processing systems},
pages = {59--65},
title = {{Structure learning in human causal induction}},
url = {http://papers.nips.cc/paper/1845-structure-learning-in-human-causal-induction.pdf},
year = {2001}
}
@article{Guan2015,
annote = {Diagnosis in a swarm of modular robots. Diagnosis in this scenario implies localizing the faulty robot(s) in the swarm. Use Dijkstra and some other heursitics to propagate the location of a detected fault.},
author = {Guan, Enguang and Fei, Jian and Pan, Gen and Fu, Zhuang and Yan, Weixin and Zhao, Yanzheng},
doi = {10.5772/60092},
issn = {1729-8814},
journal = {International Journal of Advanced Robotic Systems},
month = {apr},
number = {4},
pages = {31},
title = {{Fault Self-Diagnosis for Modular Robotic Systems Using M-Lattice Modules}},
url = {http://journals.sagepub.com/doi/10.5772/60092},
volume = {12},
year = {2015}
}
@article{Pomeranz2018,
annote = {Deals with the problem of selecting fault detectors from among many detectors, given knowledge of the faults they diagnose. Similar to a residual selection problem but using set-cover-like formulations.},
author = {Pomeranz, Irith},
doi = {10.1109/TVLSI.2018.2849972},
issn = {1063-8210},
journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
month = {oct},
number = {10},
pages = {2160--2164},
title = {{Selecting Functional Test Sequences for Defect Diagnosis}},
url = {https://ieeexplore.ieee.org/document/8418476/},
volume = {26},
year = {2018}
}
@inproceedings{Fong2006,
annote = {- Try to develop a HRI/OS that facilitates communication and teaming between humans and robots
- The proposed system is more of a mission scheduler; it is not actually code that runs on the robot
- The rest of the paper is basically a validation of the relationships between team reliability metrics as proposed in the 2008 journal paper. (refer to that)},
author = {Fong, Terrence and Scholtz, Jean and Shah, Julie A. and Fluckiger, Lorenzo and Kunz, Clayton and Lees, David and Schreiner, John and Siegel, Michael and Hiatt, Laura M. and Nourbakhsh, Illah and Simmons, Reid and Antonishek, Brian and Bugajska, Magda and Ambrose, Robert and Burridge, Robert and Schultz, Alan and Trafton, J. Gregory},
booktitle = {2006 IEEE International Conference on Systems, Man and Cybernetics},
doi = {10.1109/ICSMC.2006.384609},
isbn = {1-4244-0099-6},
month = {oct},
pages = {3198--3203},
publisher = {IEEE},
title = {{A Preliminary Study of Peer-to-Peer Human-Robot Interaction}},
url = {http://ieeexplore.ieee.org/document/4274373/},
year = {2006}
}
@article{Lin1999,
annote = {Most human interactions with robots consists of maintenance or troubleshooting. When sufficient troubleshooting knowledge isn't available, then there can be frustration as humans are forced to wait for others who do have the knowledge. Solution is to train everyone in basic troubleshooting.},
author = {Lin, Ching-torng and {J. Wang}, Mao-jiun},
doi = {10.1016/S0169-8141(97)00103-0},
issn = {01698141},
journal = {International Journal of Industrial Ergonomics},
month = {jan},
number = {1-2},
pages = {83--94},
title = {{Human–robot interaction in an aircraft wing drilling system}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0169814197001030},
volume = {23},
year = {1999}
}
@article{Chen2018a,
annote = {Anomaly detection in time series data, such as ECG signals.
- Assume that there is a process and an observation with measurement and output noise that are independent of each other
- Use a Kalman filter to filter the data
- Train an "Echo State Network" (a simpler network than regular RNNs?) on the normal data in a given dataset
- The echo state network should output the probability of the process. If the data is normal, then the mutual information beween the process and the original data should be the same as the information between the process and the estimate of the process (or the data and estimate of the data?)
- We can use the KL-divergence based on the above idea to determine if an anomaly exists at a timestep.},
author = {Chen, Qing and Zhang, Anguo and Huang, Tingwen and He, Qianping and Song, Yongduan},
doi = {10.1007/s00521-018-3747-z},
issn = {0941-0643},
journal = {Neural Computing and Applications},
month = {oct},
title = {{Imbalanced dataset-based echo state networks for anomaly detection}},
url = {http://link.springer.com/10.1007/s00521-018-3747-z},
year = {2018}
}
@article{Szafir2017,
annote = {Goal:
- Inform the design of UI interfaces in support of mobile sensing and data collection

From Coactive Design:
- Levels-of-autonomy approach to design is fundamentally flawed
- Improving autonomy often comes at the price of increased system opacity
- Autonomy is instead a product of self-sufficiency and self-directedness

Weaknesses of coactive design:
- Interdependence Analysis Table, while useful, is not data driven and often based on the designer's best judgement
- Coactive design has only been useful in tasks for real-time action coordination. H-R teams often need support for planning phases of a task too
- Interdependence relationships identified in the design process often lack a representation and means of communication to the end user.

Tasks used in initial data collection:
- Explore the environment to get a detailed description of the remote location
- Inspecting a number of boxes and recording the bar codes of undamaged boxes
- Monitoring the confederate while recording the mistakes made by the confederate.
- Space robots need to explore
- Space robots need to help with logistics management

Tasks used in the robot interface evaluation:
- Three Planned Tasks (Main Priority)
+ Sound survey - measure sound levels at four locations and decide whether repairs are needed. Forced dependency on the robot as participants wait on the robot to take measurements.
+ Pipe Inspection - take a picture of pipes "before" a certain date. Forced dependency on the human as the decision to take a picture is made by the human.
+ Inventory Logistics - find a missing box. Interdependent
- Two Unplanned Tasks (Main Priority)
+ Air-Quality Measurement - Spontaneously take an air quality measurement anywhere in the room
+ Robot-Realignment - Robot needed help relocalizing in the environment. Unlike previous task, did not need the robot to be redirected.
- Secondary task to better measure free time. Participants had to select the graph with a higher average temperature.

NOTE: Really well designed user study and a really well-written paper. Highly recommend using as a reference},
author = {Szafir, Daniel and Mutlu, Bilge and Fong, Terrence},
doi = {10.1177/0278364916688256},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
month = {jun},
number = {5-7},
pages = {514--542},
title = {{Designing planning and control interfaces to support user collaboration with flying robots}},
url = {http://journals.sagepub.com/doi/10.1177/0278364916688256},
volume = {36},
year = {2017}
}
@inproceedings{jacobs2014iso,
annote = {Summary of the requirements and scope of ISO13482. It applies to general purpose mobile manipulators performing household/care tasks.

ABB attachment details the method of how risk should be identified according to ISO 13849; which is what the ISO 13482 standard is based on.

ANSI attachment includes a summary of the ANSI equivalent of the ISO standards

(The ISO standard itself is behind a paywall)},
author = {Jacobs, Theo and Virk, Gurvinder Singh},
booktitle = {ISR/Robotik 2014; 41st International Symposium on Robotics; Proceedings of},
organization = {VDE},
pages = {1--6},
title = {{ISO 13482-The new safety standard for personal care robots}},
url = {https://library.e.abb.com/public/f282e8fb773fa733c1257996004307a6/EN{\_}ISO{\_}13849-1{\_}2TLC172003B02002.pdf https://ieeexplore.ieee.org/abstract/document/6840202/ https://www.rockwellautomation.com/resources/downloads/rockwellautomation/pdf/events/raotm/session},
year = {2014}
}
@article{Rosen2017,
abstract = {Efficient motion intent communication is necessary for safe and collaborative work environments with collocated humans and robots. Humans efficiently communicate their motion intent to other humans through gestures, gaze, and social cues. However, robots often have difficulty efficiently communicating their motion intent to humans via these methods. Many existing methods for robot motion intent communication rely on 2D displays, which require the human to continually pause their work and check a visualization. We propose a mixed reality head-mounted display visualization of the proposed robot motion over the wearer's real-world view of the robot and its environment. To evaluate the effectiveness of this system against a 2D display visualization and against no visualization, we asked 32 participants to labeled different robot arm motions as either colliding or non-colliding with blocks on a table. We found a 16{\%} increase in accuracy with a 62{\%} decrease in the time it took to complete the task compared to the next best system. This demonstrates that a mixed-reality HMD allows a human to more quickly and accurately tell where the robot is going to move than the compared baselines.},
annote = {Testing Mixed/Augmented Reality vs. Desktop Monitor vs. No interface for E-Stopping a planned robot motion path.

Results show that:
- Accuracy and sensitivity of stopping robot: None {\textless} Computer {\textless} MR
- Time to complete task: Computer {\textless} MR
- TLX: None {\textgreater} Computer {\textless} MR
- Usability: None {\textgreater} Computer {\textless} MR
- Similar story to usability in other qualitative judgements
- Almost everyone preferred the MR. Those that didn't, preferred the Computer.},
archivePrefix = {arXiv},
arxivId = {1708.03655},
author = {Rosen, Eric and Whitney, David and Phillips, Elizabeth and Chien, Gary and Tompkin, James and Konidaris, George and Tellex, Stefanie},
eprint = {1708.03655},
month = {aug},
title = {{Communicating Robot Arm Motion Intent Through Mixed Reality Head-mounted Displays}},
url = {http://arxiv.org/abs/1708.03655},
year = {2017}
}
@article{Schoelkopf2012,
abstract = {We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.},
annote = {Referenced as a meta-study that posits that semi-supervised learning is more likely to succeed in the anticausal direction.},
archivePrefix = {arXiv},
arxivId = {1206.6471},
author = {Schoelkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
eprint = {1206.6471},
month = {jun},
title = {{On Causal and Anticausal Learning}},
url = {http://arxiv.org/abs/1206.6471},
year = {2012}
}
@book{Dori2016,
address = {New York, NY},
annote = {Book introduces the concepts and differences between the Object Process Model (OPM) specification of a system, and the SysML specification.

Dori is the developer of OPM.},
author = {Dori, Dov},
doi = {10.1007/978-1-4939-3295-5},
isbn = {978-1-4939-3294-8},
publisher = {Springer New York},
title = {{Model-Based Systems Engineering with OPM and SysML}},
url = {http://link.springer.com/10.1007/978-1-4939-3295-5},
year = {2016}
}
@article{Zhang2013,
annote = {A combination of the work by Kodali et al. for diagnosing faults with time delays as well as faults that might be coupled. The method generally assumes a time window-ed approach to the problem. The exact details of what's happening here will need more time.

The paper introduces an MCMC sampling mechanism that allows inference using the fault model.

In terms of results: they run simulated experiments with different sizes of problems and show runtime and correctness results on those simulated toy domains. With the experiments they examine the effect of different parameter settings for the problem such as the number of states, the number of tests, etc.},
author = {Zhang, Shigang and Pattipati, Krishna R. and Hu, Zheng and Wen, Xisen and Sankavaram, Chaitanya},
doi = {10.1109/TSMC.2013.2244209},
issn = {2168-2216},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
month = {nov},
number = {6},
pages = {1424--1439},
title = {{Dynamic Coupled Fault Diagnosis With Propagation and Observation Delays}},
url = {http://ieeexplore.ieee.org/document/6471841/},
volume = {43},
year = {2013}
}
@incollection{Abdollahi2016,
annote = {- Consider the system as a set of possible fault causes that have noisy influences on tests, which in turn have noisy influences on observations.
- This work assumes that observations == tests
- Even with the noisy influences of causes on tests, the problem of diagnosing the set of causes given test outputs can be an NP-hard problem
- The paper poses a relaxed Lagrangian-based optimization technique for solving the problem
- The paper differentiates between static and dynamic diagnosis. In dynamic diagnosis, at every iteration, the causes (which influence tests) have a set probability of appearing and of disappearing. This additional probability complicates matters
- The paper reformulates the complicated problem as separable smaller problems that can be computed. (look at prior Pattipati work)
- Dynamic diagnosis naturally leads to active probing and active diagnosis. This is also called the test sequencing problem (lots of references here)},
author = {Abdollahi, Ali and Pattipati, Krishna R. and Kodali, Anuradha and Singh, Satnam and Zhang, Shigang and Luh, Peter B.},
doi = {10.1007/978-3-319-30599-8_5},
pages = {109--139},
title = {{Probabilistic Graphical Models for Fault Diagnosis in Complex Systems}},
url = {http://link.springer.com/10.1007/978-3-319-30599-8{\_}5},
year = {2016}
}
@incollection{Poola2017,
annote = {Survey of faults and methods of overcoming them in distributed computation workflows (such as for comp. bio.)

The more interesting graph here mentions the different distributions that are appropriate to modeling the probability of faults (and their characteristics); Sec. 15.6. The Weibull distribution seems the most appropriate.},
author = {Poola, Deepak and Salehi, Mohsen Amini and Ramamohanarao, Kotagiri and Buyya, Rajkumar},
booktitle = {Software Architecture for Big Data and the Cloud},
doi = {10.1016/B978-0-12-805467-3.00015-6},
pages = {285--320},
publisher = {Elsevier},
title = {{A Taxonomy and Survey of Fault-Tolerant Workflow Management Systems in Cloud and Distributed Computing Environments}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128054673000156},
year = {2017}
}
@article{Gossler2015,
annote = {Summary:
- Given a system model comprised of components, how should blame be ascribed to a component for a detected failure.
- Assuming a trace of events from each component and a specification of valid traces for each component, then we can use counterfactuals to ascribe blame for a fault to a particular component.
- This is essentially a search over the space of all possible traces if the logged faults are replaced with correct traces.

Check the scan for highlights

Some notable highlights:
- Hume definition of causality - if the first object had not been, the second had never existed
- A set of components are a necessary cause for a failure if the faults in those compoenents are replaced with correct behviour, the system level fault is removed. This is a safety property.
- A set of components are a sufficient cause for a failure if the removal of failures in components not part of the set still results in the system level fault. This is a liveness (deadlock-free) property.
- You have to propagate (based on the model) the effects of removing the faults in traces; otherwise, you can get erroneous causal blames
- In another work, a diagnosis for incorrect behaviour is the minimal set of components whose failure explains the observation
- Blaming is undecidable in general.},
author = {G{\"{o}}ssler, Gregor and {Le M{\'{e}}tayer}, Daniel},
doi = {10.1016/j.scico.2015.06.010},
issn = {01676423},
journal = {Science of Computer Programming},
month = {dec},
pages = {223--235},
title = {{A general framework for blaming in component-based systems}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0167642315001331 https://drive.google.com/open?id=1-ZKwG68m80y-pmPTHCneguefcfXRzKV6},
volume = {113},
year = {2015}
}
@article{mooij2016distinguishing,
annote = {Paper on Additive Noise Models, conditions when learning causal models is possible, and information-theoretic conditions using which, causal structures can be learned.},
author = {Mooij, Joris M and Peters, Jonas and Janzing, Dominik and Zscheischler, Jakob and Sch{\"{o}}lkopf, Bernhard},
journal = {The Journal of Machine Learning Research},
number = {1},
pages = {1103--1204},
publisher = {JMLR. org},
title = {{Distinguishing cause from effect using observational data: methods and benchmarks}},
url = {http://www.jmlr.org/papers/volume17/14-518/14-518.pdf},
volume = {17},
year = {2016}
}
@article{Wu2016a,
annote = {Design of a filter to generate residuals for a robot manipulator modeled as a Markov Jump System. Did not really understand, tbh, but it seems like there is also a "performance" metric that is optimized for the fault detection so that the problem can be converted into a convex optimization problem.},
author = {Wu, Ligang and Luo, Wensheng and Zeng, Yi and Li, Fanbiao and Zheng, Zhong},
doi = {10.1109/TIE.2016.2541087},
issn = {0278-0046},
journal = {IEEE Transactions on Industrial Electronics},
month = {jul},
number = {7},
pages = {4387--4399},
title = {{Fault Detection for Underactuated Manipulators Modeled by Markovian Jump Systems}},
url = {http://ieeexplore.ieee.org/document/7432014/},
volume = {63},
year = {2016}
}
@inproceedings{yang2016hierarchical,
annote = {Use bidirectional GRUs, hierarchies, and attention to outperform all other previous models in a sentiment analysis task. Creates word embeddings that are used to embed sentences, which are then used to embed the entire paragraph. Can perhaps be used to deal with very long sequences.},
author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {1480--1489},
title = {{Hierarchical attention networks for document classification}},
url = {http://www.aclweb.org/anthology/N16-1174},
year = {2016}
}
@inproceedings{kuntz2011probabilistic,
annote = {Generating fault trees from counterfactuals:
- drawback of current FTA is that it relies on engineers to come up with potential failures
- build upon prior work of generating counterfactuals from a search in probabilistic model specification search space (no idea what this is).},
author = {Kuntz, Matthias and Leitner-Fischer, Florian and Leue, Stefan},
booktitle = {International Conference on Computer Safety, Reliability, and Security},
organization = {Springer},
pages = {71--84},
title = {{From probabilistic counterexamples via causality to fault trees}},
url = {https://kops.uni-konstanz.de/bitstream/handle/123456789/21286/Kuntz{\_}212868.pdf?sequence=1 https://link.springer.com/10.1007{\%}2F978-3-642-24270-0{\_}6},
year = {2011}
}
@article{Qiu2016,
annote = {There's a variant of GAs called Differential Evolution Algorithms, and they seem to be popular in China for solving the test sequencing problem as formulated by Pattipati et al.

This paper includes a velocity term in the sampling procedure for the algorithm when creating an optimal test sequence, and numerical results show that it works. Details are unclear.},
author = {Qiu, Xiao-Hong and Hu, Yu-Ting and Li, Bo},
doi = {10.1007/s11633-016-1008-0},
issn = {1476-8186},
journal = {International Journal of Automation and Computing},
month = {sep},
title = {{Sequential fault diagnosis using an inertial velocity differential evolution algorithm}},
url = {http://link.springer.com/10.1007/s11633-016-1008-0},
year = {2016}
}
@inproceedings{ghahramani1996factorial,
annote = {Factorial HMMs might be a good abstraction for our problem domain. This is commonly used in the Pattipati et al. works.

Essentially, there are multiple state sequences that combine to create an observation sequence.},
author = {Ghahramani, Zoubin and Jordan, Michael I},
booktitle = {Advances in Neural Information Processing Systems},
pages = {472--478},
title = {{Factorial hidden Markov models}},
url = {http://papers.nips.cc/paper/1144-factorial-hidden-markov-models.pdf https://github.com/regevs/factorial{\_}hmm},
year = {1996}
}
@inproceedings{beck2011spacio,
annote = {Skill segmentation and clustering for robot skills. Used in the Skill-based exception handling workshop paper that's published in 2015},
author = {{Beck Anders Billes{\o}and Risager}, Claus and Andersen, Nils A and Ravn, Ole},
booktitle = {14th International Conference on Information Fusion},
organization = {IEEE},
pages = {1--8},
title = {{Spacio-temporal situation assessment for mobile robots}},
url = {https://ieeexplore.ieee.org/abstract/document/5977650 https://core.ac.uk/download/pdf/13754135.pdf},
year = {2011}
}
@incollection{Alemzadeh2015,
annote = {STAMP - Systems-Theoretic Accident Model and Processes - models systems as hierarchical control structures where components at each level impose safety constraints on the activity of lower levels.

The authors analyzed an existing database of accidents with robot surgeons and classified different accidents and hazards.

Then, they used Systems-Theoretic Process Analysis (STPA) to model the hazards and possible causal factors in the database of errors.

Then, to stress test the robot system, they developed a test bench to inject errors into the robot to test the safety systems.

According to the survey paper of Guiochet et al., this is one of the few papers in robotics to systematically model the safety of a robot through a systematic analysis (in this case, STPA of the surgical robot software RAVEN II)},
author = {Alemzadeh, Homa and Chen, Daniel and Lewis, Andrew and Kalbarczyk, Zbigniew and Raman, Jaishankar and Leveson, Nancy and Iyer, Ravishankar},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-319-24255-2_16},
pages = {213--227},
title = {{Systems-Theoretic Safety Assessment of Robotic Telesurgical Systems}},
url = {http://link.springer.com/10.1007/978-3-319-24255-2{\_}16 https://arxiv.org/pdf/1504.07135.pdf},
year = {2015}
}
@inproceedings{Zheng2005,
abstract = {We consider the problem of diagnosing faults in a system represented by a Bayesian network, where diagnosis corresponds to recovering the most likely state of unobserved nodes given the outcomes of tests (observed nodes). Finding an optimal subset of tests in this setting is intractable in general. We show that it is difficult even to compute the next most-informative test using greedy test selection, as it involves several entropy terms whose exact computation is intractable. We propose an approximate approach that utilizes the loopy belief propagation infrastructure to simultaneously compute approximations of marginal and conditional entropies on multiple subsets of nodes. We apply our method to fault diagnosis in computer networks, and show the algorithm to be very effective on realistic Internet-like topologies. We also provide theoretical justification for the greedy test selection approach, along with some performance guarantees.},
address = {Edinburgh},
annote = {Diagnosis is formulated as an inference problem on a Bayes Net with the goal of assigning most likely states to hidden nodes given outcomes of test nodes. This is MAP inference.

Active diagnosis - tests are selected sequentially in order to minimize the cost of testing. This work uses entropy as part of the cost function.

The goal of this work: minimize entropy (uncertainty) in hidden nodes given a sequence of test outcomes, while minimizing the cost of testing.

Contribution: use approximate belief propagation in a Bayes Net represented as a factor graph as a means of inferring hidden node states for large Bayes Nets.

Aside: the process of selecting a test to reduce entropy while also minimizing the cost of the test is akin to value-of-information analysis, which was studied by ref. [9] (Heckerman, Horvitz, Middleton, '93) for troubleshooting a long time ago.},
archivePrefix = {arXiv},
arxivId = {1207.1418},
author = {Zheng, Alice X. and Rish, Irina and Beygelzimer, Alina},
booktitle = {Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence},
eprint = {1207.1418},
month = {jul},
pages = {675--682},
publisher = {AUAI Press},
title = {{Efficient Test Selection in Active Diagnosis via Entropy Approximation}},
url = {http://arxiv.org/abs/1207.1418 https://dl.acm.org/citation.cfm?id=3020418},
year = {2005}
}
@article{Tian2018,
annote = {- Deals with the problem of when the diagnostic output from tests is multivalued and the different values help isolate faults. Converting to a bunch of binary tests is often not the best strategy
- Also selects tests based on an iterative "growing algorithm" than a "rollout" strategy. Not clear what the distinction between the two is; the former seems to involve the concept of necessary vs. unnecessary tests though.
- The result is that the iterated growing algorithm outperforms the rollout strategy.},
author = {Tian, Heng and Duan, Fuhai and Fan, Liang and Sang, Yong},
doi = {10.1177/1748006X18770356},
issn = {1748-006X},
journal = {Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability},
month = {may},
pages = {1748006X1877035},
title = {{Fault diagnostic strategy of multivalued attribute system basedon growing algorithm}},
url = {http://journals.sagepub.com/doi/10.1177/1748006X18770356},
year = {2018}
}
@article{Battaglia2018,
abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
annote = {From Weiyu - why graph NNs are the way of the future},
archivePrefix = {arXiv},
arxivId = {1806.01261},
author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
eprint = {1806.01261},
month = {jun},
title = {{Relational inductive biases, deep learning, and graph networks}},
url = {http://arxiv.org/abs/1806.01261},
year = {2018}
}
@article{Shakeri2000,
annote = {Test Sequencing Problem solution is a decision tree that specifies the test to perform next based on the outcome of previous tests.

Goal of this work - construct a sequence for multiple fault diagnosis given single fault diagnosis sequence as a building block.

Recitification - replacement of a potentially faulty component without prior diagnosis. Can potentially lead of high false alarm rates.

Due to fault masking, applying single fault isolation multiple times (instead of combined multi-fault) also leads to high false alarm.

Tests are thought to be 100{\%} reliable (Check out 7, 16 for unreliable tests. Also check out Wald [17] because it supposedly shows the benefit of sequential tests over block tests)

Can check the details later, but essentially the method computes hitting sets of candidates, "resolves errors" in the hitting sets, and then tests to see if the system is OK.},
author = {Shakeri, M. and Raghavan, V. and Pattipati, Krishna R. and Patterson-Hine, A.},
doi = {10.1109/3468.823474},
issn = {10834427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
number = {1},
pages = {1--14},
title = {{Sequential testing algorithms for multiple fault diagnosis}},
url = {http://ieeexplore.ieee.org/document/823474/},
volume = {30},
year = {2000}
}
@inproceedings{bramley2014order,
annote = {Hume - causal connection when:
- temporal precedence
- contiguity
- constant conjunction

Aim: show that temporal ordering of events is more indicative of causation than the timing between events

User study to model participants' beliefs on causal structure given temporal data. The model is based on a Bayesian update with two different models:
- conservative (strong) - causes can only precede
- weak - causes can precede or occur at the same time.

Results show participants update beliefs more in line with the non-simultaneous (conservative) model.

However, participants are more conservative than the baseline model predicts (possibly due to anchoring?). Therefore, we should add a conservatism parameter to the belief update. Another possible explanation - belief updates are non-Bayesian.},
author = {Bramley, Neil and Gerstenberg, Tobias and Lagnado, David},
booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
number = {36},
title = {{The order of things: Inferring causal structure from temporal patterns}},
url = {http://web.mit.edu/tger/www/papers/The order of things (Bramley, Gerstenberg, Lagnado, 2014).pdf},
volume = {36},
year = {2014}
}
@article{Wang2015,
annote = {System introduction for monitoring a remote factory robot by multiple users without using a lot of network bandwidth.},
author = {Wang, Lihui},
doi = {10.1007/s00170-013-4864-6},
issn = {0268-3768},
journal = {The International Journal of Advanced Manufacturing Technology},
month = {dec},
number = {9-12},
pages = {1433--1445},
title = {{Collaborative robot monitoring and control for enhanced sustainability}},
url = {http://link.springer.com/10.1007/s00170-013-4864-6},
volume = {81},
year = {2015}
}
@article{Kodali2013a,
annote = {Deals with the case when fault signatures overlap and a fault with a minimal signature might subsume the combined signatures of two other faults. Such a diagnosis problem can be solved in the "dynamic" domain where the residuals are available at different times. This paper deals with the above issue, and when the delay times between the presence of the fault and a test output are known.

The paper introduces a Viterbi + Relaxed Lagrangian optimization algorithm to estimate the states of multiple factorial HMMs in order to solve this problem. Details a bit unclear.},
author = {Kodali, Anuradha and Singh, Satnam and Pattipati, Krishna R.},
doi = {10.1109/TSMCA.2012.2208101},
issn = {2168-2216},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
month = {may},
number = {3},
pages = {547--562},
title = {{Dynamic Set-Covering for Real-Time Multiple Fault Diagnosis With Delayed Test Outcomes}},
url = {http://ieeexplore.ieee.org/document/6301764/},
volume = {43},
year = {2013}
}
@book{pearl2018book,
annote = {The book that started it all. It seems to be the go-to reference for the ladder of causation.},
author = {Pearl, Judea and Mackenzie, Dana},
publisher = {Hachette UK},
title = {{The Book of Why: The New Science of Cause and Effect}},
year = {2018}
}
@article{saul1999mixed,
annote = {Extension of Factorial Markov Models to higher order Markov situations (depending on more than just the last state) to make them more expressive and also make parameter learning possible in complex situations.

See the Zhong and Ghosh paper for a succinct summary of the contributions in this paper.},
author = {Saul, Lawrence K and Jordan, Michael I},
journal = {Machine learning},
number = {1},
pages = {75--87},
publisher = {Springer},
title = {{Mixed memory markov models: Decomposing complex stochastic processes as mixtures of simpler ones}},
url = {https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/Speak12To13Semester1/mixmem{\_}ml99.pdf},
volume = {37},
year = {1999}
}
@article{Baah2010,
annote = {Converts programs to probabilistic graphical models through a "dependency graph". Localization using this method works well},
author = {Baah, G K and Podgurski, A and Harrold, M J},
doi = {10.1109/TSE.2009.87},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = {jul},
number = {4},
pages = {528--545},
title = {{The Probabilistic Program Dependence Graph and Its Application to Fault Diagnosis}},
url = {http://ieeexplore.ieee.org/document/5374423/},
volume = {36},
year = {2010}
}
@article{DeDonato2015,
annote = {Report from Team D in the Yanco paper, and the team in the Atkeson paper

Good diagrams of the system and how information flowed from robot to operators and controllers. They had 2 ROS systems that communicated with each other over a reduced bandwidth channel.
Multi-operator setup with screen mirroring so that each operator could see what the other was doing.
However, (according to Yanco) they were poor at manipulation and did not have many interfaces for low-interaction control, for manipulation.
Operators OK'd or intervened for each foot placement. They were given images to make the decision because operators understood images better than laser scans},
author = {DeDonato, Mathew and Dimitrov, Velin and Du, Ruixiang and Giovacchini, Ryan and Knoedler, Kevin and Long, Xianchao and Polido, Felipe and Gennert, Michael A. and Padır, Taşkın and Feng, Siyuan and Moriguchi, Hirotaka and Whitman, Eric and Xinjilefu, X. and Atkeson, Christopher G.},
doi = {10.1002/rob.21567},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {mar},
number = {2},
pages = {275--292},
title = {{Human-in-the-loop Control of a Humanoid Robot for Disaster Response: A Report from the DARPA Robotics Challenge Trials}},
url = {http://doi.wiley.com/10.1002/rob.21567 https://docs.google.com/document/d/1hV6ICOF5n-ba77ct9wo6fZ0GM0L-6EP4J78GzigU{\_}58/edit?usp=sharing},
volume = {32},
year = {2015}
}
@article{Rodler2018,
abstract = {Given a malfunctioning system, sequential diagnosis aims at identifying the root cause of the failure in terms of abnormally behaving system components. As initial system observations usually do not suffice to deterministically pin down just one explanation of the system's misbehavior, additional system measurements can help to differentiate between possible explanations. The goal is to restrict the space of explanations until there is only one (highly probable) explanation left. To achieve this with a minimal-cost set of measurements, various (active learning) heuristics for selecting the best next measurement have been proposed. We report preliminary results of extensive ongoing experiments with a set of selection heuristics on real-world diagnosis cases. In particular, we try to answer questions such as "Is some heuristic always superior to all others?", "On which factors does the (relative) performance of the particular heuristics depend?" or "Under which circumstances should I use which heuristic?"},
annote = {Global optimization of the test sequencing problem is NP-hard, but one-step lookahead strategies are actually really good.

- Focus on a lookahead heurisitic of query selection measures (QSMs)
- Queries are binary true/false tests that are applicable in a wide variety of domains
- Popular QSMs include information gain and split-in-half heuristics. New measures from active learning are slowly becoming more popular
- This work evaluates different query selection strategies along different criteria

- The work has an interesting mathematical introduction to the test sequencing problem by formulating it as the partition sets of each query
- A query selection measure assigns heuristic value to each query
- Measures checked:
(a) Information Gain - based on entropy of diagnoses
(b) Split-in-half - query splits hypothesis space in half
(c) KL-div - query with largest disagreement between a single diagnosis and that everything is wrong
(d) Expected model change - most number of invalidated diagnoses
(e) Most probable singleton - try to bias towards singleton diagnoses, and at least reduce hypothesis space by 1
(f) Biased elimination - Choose a query with the highest bias between positive and negative classifications of diagnosis hypotheses
(g) Risk optimization - Entropy plus some learned heuristic, see prior work

Findings are hard to parse, but I think it's trying to state that measures that are dependent on the probability of the test (entropy for instance) can be very sensitive to imperfect oracles whereas a measure like the split heuristic is not.},
archivePrefix = {arXiv},
arxivId = {1807.03083},
author = {Rodler, Patrick and Schmid, Wolfgang},
eprint = {1807.03083},
month = {jul},
title = {{Evaluating Active Learning Heuristics for Sequential Diagnosis}},
url = {http://arxiv.org/abs/1807.03083},
year = {2018}
}
@article{Wu2017,
abstract = {Robot introspection, as opposed to anomaly detection typical in process monitoring, helps a robot understand what it is doing at all times. A robot should be able to identify its actions not only when failure or novelty occurs, but also as it executes any number of sub-tasks. As robots continue their quest of functioning in unstructured environments, it is imperative they understand what is it that they are actually doing to render them more robust. This work investigates the modeling ability of Bayesian nonparametric techniques on Markov Switching Process to learn complex dynamics typical in robot contact tasks. We study whether the Markov switching process, together with Bayesian priors can outperform the modeling ability of its counterparts: an HMM with Bayesian priors and without. The work was tested in a snap assembly task characterized by high elastic forces. The task consists of an insertion subtask with very complex dynamics. Our approach showed a stronger ability to generalize and was able to better model the subtask with complex dynamics in a computationally efficient way. The modeling technique is also used to learn a growing library of robot skills, one that when integrated with low-level control allows for robot online decision making.},
annote = {Robot introspection (different from anomaly detection) - what robot is doing and how it is doing it. Allows for better handling of unexpected situations.

Related - state estimation/process monitoring. Vast majority of solutions are parametric, with strong assumptions on the underlying model. Poor quality models don't work well, especially in highly dynamic manipulation tasks.

Goal: Use Bayesian Autoregressive HMMs to learn skills and detect anomalies in the skills in real-time. The feedback can then be used for online decision-making.

(Aside: great intro to the different works on change-point detection, especially using HMMs).

Method:
- Use a generalization of the HDP-VAR-HMM, to learn subtask skills (that are presegmented?)
- The expected variance in the log-likelihood can then be used for anomaly detection in a particular subtask/skill.

Evaluation used 44 nominal trials, and 16 anomalous trials. Unclear what we can take away from the evaluations because there are no comparisons to methods that are not their algorithm. Also, the discussion section is incomplete/confusing.},
archivePrefix = {arXiv},
arxivId = {1705.08661},
author = {Wu, Hongmin and Lin, Hongbin and Guan, Yisheng and Harada, Kensuke and Rojas, Juan},
doi = {10.1109/HUMANOIDS.2017.8246976},
eprint = {1705.08661},
month = {may},
title = {{Robot Introspection with Bayesian Nonparametric Vector Autoregressive Hidden Markov Models}},
url = {http://arxiv.org/abs/1705.08661 http://dx.doi.org/10.1109/HUMANOIDS.2017.8246976},
year = {2017}
}
@techreport{vesely1981fault,
annote = {Tutorial on performing FTA. But also a tutorial for performing FMEA, FMECA, and PHA.

Some important points:
- Although analysis can be done in both the "failure"-space and the "success"-space, there are generally more ways to succeed than there are to fail.
- There is no causal relationship between the inputs and outputs of an OR gate. Instead, the OR gate acts as a "generalization" function of all its inputs' effects.
- There is a causal relationship between the inputs and outputs of an AND gate
- Faults vs. Failures - all failures (abnormal occurrences) are faults but not all faults are failures
- Fault occurrence vs. existence - FTA applies only to the occurrence of a fault
- Passive vs. Active components - usually differentiated based on expected failure rates
- Primary fault - fault in nominal context, secondary fault - fault in an context outside recommended thresholds, command fault - fault 
due to usage at wrong place in the wrong time.
- Failure mechanisms produce failure modes, that have failure effects. This is useful in the construction of FTA
- Due to low probability of events (generally), P(A+B) = P(A) + P(B). And as a result OR gates {\~{}} XOR gates
- Unless A and B are independent P(A . B) != P(A) P(B). Use Bayes' Rule instead.},
author = {Vesely, William E and Goldberg, Francine F and Roberts, Norman H and Haasl, David F},
institution = {Nuclear Regulatory Commission Washington dc},
title = {{Fault tree handbook}},
url = {http://www.dtic.mil/dtic/tr/fulltext/u2/a354973.pdf},
year = {1981}
}
@inproceedings{kaiser2018advances,
annote = {Details the use of a hierarchical Fault Tree that is easier to work with and also better integrated with system specification diagrams such as SysML, etc.},
author = {Kaiser, Bernhard and Schneider, Daniel and Adler, Rasmus and Domis, Dominik and M{\"{o}}hrle, Felix and Berres, Axel and Zeller, Marc and H{\"{o}}fig, Kai and Rothfelder, Martin},
booktitle = {Proceedings of the 28th European Safety and Reliability Conference (ESREL)},
title = {{Advances in Component Fault Trees}},
url = {https://www.researchgate.net/profile/Bernhard{\_}Kaiser2/publication/326625530{\_}Advances{\_}in{\_}Component{\_}Fault{\_}Trees/links/5b5997d5458515c4b24946d4/Advances-in-Component-Fault-Trees.pdf},
year = {2018}
}
@inproceedings{edmonds2018human,
annote = {Although associative learning is widely thought of as the reason for causal reasoning in humans, subsequent research shows that this is not true. RL is the modern version of associative learning.

Insight: There is a difference in human causal learning depending on whether it's in a Common Cause (CC) or Common Effect (CE) schema.

Goals:
- Design experiment rooms with CC and CE schemas, and evaluate which ones humans prefer.
- Also, can learning one type of structure facilitate learning another kind with different numbers of variables.
- Finally, how does RL perform compared to humans? DDQN is the RL algortihm of choice.

Human Experiment:
- 240 undergraduates!! 6 conditions with 40 participants per condition.
- CC structure took fewer attempts to solve than a CE structure. CC structure was also easier to learn on for CC, and CE was easier for CE.

RL Experiment:
- Multiple reward functions were tested.
- Unique reward was quite possibly the worst given the training style for DDQN, however, it is also the information given to humans.
- Asymmetry in learning from different environments is less pronounced in RL than in humans
- Conclusion, RL is not learning the causes. But it should, because humans can solve the tasks, and humans are definitely forming causal schemas.

Why is CC better than CE?
- Possibly due to the Monty Hall problem

RL fails?
- Need to learn abstractions; the latent space that the world is operating in. The current methods don't do that.
- The state and action spaces are low dimensional and discrete. Propagating gradients is hard.
- State changes modify the environment mechanism, which makes it hard to make the Markovian assumption unless one reasons in the latent space of causality
- Humans use a non-Markovian (?) optimal policy

Training from humans?
- Humans have an epsilon decay rate that is too low for RL
- Bayes Net to learn the model and help in action selection?},
author = {Edmonds, Mark and {Kubricht James}, Feng and Summers, Colin and Zhu, Yixin and Rothrock, Brandon and Zhu, Song-Chun and Lu, Hongjing},
booktitle = {40th Annual Meeting of the Cognitive Science Society},
title = {{Human Causal Transfer: Challenges for Deep Reinforcement Learning}},
url = {http://www.mjedmonds.com/papers/CogSci18{\_}OpenLock{\_}CausalRL{\_}final.pdf http://www.mjedmonds.com/projects/OpenLock/CogSci18{\_}OpenLock{\_}CausalRL.html},
year = {2018}
}
@inproceedings{Chang2013,
annote = {Method of extending a Petri-Net representation of a task to perform error recoveries. If the error is already known, then it's a trivial matter of changing the task execution sequence. If the error is unknown, then the paper presents a method of augmenting the known set of states from human demonstrations and using those for recovery. The method has large scalability issues.},
author = {Chang, Guoting and Kulic, Dana},
booktitle = {2013 16th International Conference on Advanced Robotics (ICAR)},
doi = {10.1109/ICAR.2013.6766465},
isbn = {978-1-4799-2722-7},
month = {nov},
pages = {1--6},
publisher = {IEEE},
title = {{Robot task error recovery using Petri nets learned from demonstration}},
url = {http://ieeexplore.ieee.org/document/6766465/},
year = {2013}
}
@article{Verma2004,
annote = {Particle Filters for the purposes of fault detection. Not really performing diagnosis. Should cite this as an example of using particle filters in FDD. The authors also claim that this is model-free - need to actually read the paper to double check that},
author = {Verma, V. and Gordon, G. and Simmons, R. and Thrun, S.},
doi = {10.1109/MRA.2004.1310942},
issn = {1070-9932},
journal = {IEEE Robotics {\&} Automation Magazine},
month = {jun},
number = {2},
pages = {56--66},
title = {{Real-time fault diagnosis}},
url = {http://ieeexplore.ieee.org/document/1310942/},
volume = {11},
year = {2004}
}
@inproceedings{Parker2006,
annote = {Slightly more detailed notes (more notes abail
- Developed a system called LeaF to learn fault diagnosis models from experience and add them to a case-based repository of faults.
- Faults are defined within a Causal Graphical model that relates 'macro' faults to individual micro ones. The micro faults have associated symptoms, diagnostic tests, and recovery steps.
- The goal of diagnosis is to pick the most appropriate micro fault in order to recover from the problem correctly
- The contribution of this paper is to provide a mechanism for using CBR to either better diagnose micro faults from experience (add weight to edges in the model) or to add new causal nodes to edges post-human experience.
- Similarity metric for the CBR matching and addition is done with 'similitude', using a method called LID (Lazy Induction of Descriptors).
- They show in robot experiments that a) the robot learns to better diagnose known faults over time, and b) the robot can learn to diagnose unknown faults (however, the mechanism of that first unknown diagnosis is unclear).

On Causal Model
- The causal model papers linked to in this work are simply DAGs of potential causes -{\textgreater} root causes.
- A potential cause when identified, triggers its root cause, and so on until leaf nodes are all triggered.
- It is unclear how the pruning of leaf nodes occurs, but I imagine it is through certain runtime checks of system traces (which are mentioned in the cited works)},
author = {Parker, Lynne and Kannan, Balajee},
booktitle = {2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2006.281993},
isbn = {1-4244-0258-1},
month = {oct},
pages = {2703--2710},
publisher = {IEEE},
title = {{Adaptive Causal Models for Fault Diagnosis and Recovery in Multi-Robot Teams}},
url = {http://ieeexplore.ieee.org/document/4058799/},
year = {2006}
}
@book{isermann2006fault,
annote = {Precursor to the 2011 book. Goes into more detail on Chapter 2 material present in the successor book},
author = {Isermann, Rolf},
publisher = {Springer Science {\&} Business Media},
title = {{Fault-Diagnosis Systems: an introduction from fault detection to fault tolerance}},
url = {http://zkratka.vlicek.eu/Literatura/DIT Fault-Diagnosis Systems.pdf},
year = {2006}
}
@article{Chen2011,
annote = {With a lot of feature engineering and feature selection, SVMs outperform Neural Networks and LDA in classifying between 3 fault types in a power plant.},
author = {Chen, Kai-Ying and Chen, Long-Sheng and Chen, Mu-Chen and Lee, Chia-Lung},
doi = {10.1016/j.compind.2010.05.013},
issn = {01663615},
journal = {Computers in Industry},
month = {jan},
number = {1},
pages = {42--50},
title = {{Using SVM based method for equipment fault detection in a thermal power plant}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0166361510000862},
volume = {62},
year = {2011}
}
@article{Chandola2009,
annote = {Notes in the linked Google Doc},
author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
doi = {10.1145/1541880.1541882},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {jul},
number = {3},
pages = {1--58},
title = {{Anomaly detection}},
url = {http://portal.acm.org/citation.cfm?doid=1541880.1541882 https://drive.google.com/open?id=13YNjUcQmoHM5TbS78PO86jCtn6EqRcQfK4KC1-2EOp0},
volume = {41},
year = {2009}
}
@inproceedings{Hecht2015,
annote = {Case study of using the methods of David et al. to create FMEA from a SysML spec.},
author = {Hecht, Myron and Nguyen, Elisabeth and Chuidian, Aaron and Pinchak, Julia and Dimpfl, Emily},
doi = {10.4271/2015-01-2444},
month = {sep},
title = {{Creation of Failure Modes and Effects Analyses from SysML}},
url = {http://papers.sae.org/2015-01-2444/},
year = {2015}
}
@article{Chanthery2016,
annote = {Graph theory based application of residual (ARR) based fault diagnosis with distributed diagnosis components. Makes the example of creating fault signature matrices much more concrete.},
author = {Chanthery, Elodie and Trave-Massuyes, Louise and Indra, Saurabh},
doi = {10.1109/TSMC.2015.2479192},
issn = {2168-2216},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
month = {may},
number = {5},
pages = {598--610},
title = {{Fault Isolation on Request Based on Decentralized Residual Generation}},
url = {http://ieeexplore.ieee.org/document/7297865/},
volume = {46},
year = {2016}
}
@book{Isermann2011,
address = {Berlin, Heidelberg},
annote = {Notes in linked Google Doc},
author = {Isermann, Rolf},
doi = {10.1007/978-3-642-12767-0},
isbn = {978-3-642-12766-3},
publisher = {Springer Berlin Heidelberg},
title = {{Fault-Diagnosis Applications}},
url = {http://link.springer.com/10.1007/978-3-642-12767-0 https://docs.google.com/document/d/1CCnxs7jWeIbTmghFOj88P3Ejc{\_}Xy7{\_}MLfD3reDY2Dss/edit?usp=sharing},
year = {2011}
}
@inproceedings{Bhattacharjya2008,
abstract = {Decision circuits have been developed to perform efficient evaluation of influence diagrams [Bhattacharjya and Shachter, 2007], building on the advances in arithmetic circuits for belief network inference [Darwiche,2003]. In the process of model building and analysis, we perform sensitivity analysis to understand how the optimal solution changes in response to changes in the model. When sequential decision problems under uncertainty are represented as decision circuits, we can exploit the efficient solution process embodied in the decision circuit and the wealth of derivative information available to compute the value of information for the uncertainties in the problem and the effects of changes to model parameters on the value and the optimal strategy.},
annote = {Sensitivity Analysis in Decision Circuits. Keeping the reference around in case it proves necessary.

Best guess of sensitivity analysis - the sensitivity of the decisions made to different values of the conditional probabilities of tests and outcomes.},
archivePrefix = {arXiv},
arxivId = {1206.3551},
author = {Bhattacharjya, Debarun and Shachter, Ross D.},
booktitle = {UAI},
eprint = {1206.3551},
month = {jun},
title = {{Sensitivity analysis in decision circuits}},
url = {http://arxiv.org/abs/1206.3551},
year = {2008}
}
@book{costa2006discrete,
annote = {Clearest introduction to Markov Linear Jump Systems. Essentially a case of switching dynamics linear systems.},
author = {Costa, Oswaldo Luiz Valle and Fragoso, Marcelo Dutra and Marques, Ricardo Paulino},
publisher = {Springer Science {\&} Business Media},
title = {{Discrete-time Markov jump linear systems}},
url = {https://books.google.com/books?id=gP-mNSGiRaUC{\&}dq=markov+jump+systems{\&}lr={\&}source=gbs{\_}navlinks{\_}s},
year = {2006}
}
@inproceedings{javdani2014near,
annote = {Bayesian active learning is another name for the test sequencing problem. In this paper, instead of minimizing uncertainty in hypotheses directly, they minimize uncertainty to facilitate decision-making. Specifically, hypothesis space is made of decision regions and the goal is to select tests to quickly concentrate all consistent hypotheses to a single decision region.

Contributions:
- tackle the case of overlapping decision regions
- develop a surrogate objective called Hyperedge Cutting that can provide strong theoretical guarantees to the solution.

Formalization:
- The math intro is well written
- Regions are hyperedges in a hypergraph, and hypotheses are nodes in the hypergraph.
- The goal is to find a single region that all consistent hypotheses lie within
- A policy is a function that takes evidence so far and chooses the next test, or chooses to stop. The policy is feasible iff it drives uncertainty to a single decision region.
- Formally, the problem is a Decision Region Determination Problem

Method:
- Create subregions from the actual decision regions. Then hypotheses are part of disjoint subregions.
- Create a splitting hypergraph where nodes are subregions and hyperedges are multisets of subregions such that no subregion in the hyperedge belongs to the same region.
- Given the splitting hypergraph, simply cutting all hyperedges leaves one with a single decision region.
- Such an algorithm satisfies adaptive monotonicity - the benefit of each test is non-negative, and adaptive submodularity - the marginal gain of any test cannot increase with more evidence.
- It is unclear why the hyperedges need to be multisets. My best guess is because of the following point, which I don't understand.
- In order to efficiently compute the greedy policy, the method uses Complete Homogenous Symmetric Polynomials, which (I think) approximate the weight of hyperedges by using the properties of how subregions are constructed.},
author = {Javdani, Shervin and Chen, Yuxin and Karbasi, Amin and Krause, Andreas and Bagnell, Drew and Srinivasa, Siddhartha S},
booktitle = {AISTATS},
pages = {430--438},
title = {{Near Optimal Bayesian Active Learning for Decision Making.}},
url = {http://www.jmlr.org/proceedings/papers/v33/javdani14-supp.pdf https://drive.google.com/file/d/1-WhDw4J11LnleCKvvNS1PxnEkk{\_}ONgtM/view},
volume = {14},
year = {2014}
}
@inproceedings{Abreu2008,
address = {New York, New York, USA},
annote = {Prerequisite paper to the Abreu journal paper on diagnosing multiple faults.

- A weak system model only specifies nominal (correct) behaviour. A strong model also specifies faulty behaviour in the logical model.
- If we use the similarity score of a column (or set of columns) as an indication of the probability of the error vector given those components failed, then we can use Bayes' rule to calculate the probability of those components actually having failed, given the observation of errors.
- Contribution: create a weak system model from the observation of errors

Method:
- Order the components in the order of their execution.
- Assume first component receives a correct input
- Then, if a fault is observed, then there must have been a failure somewhere up the chain
- Each row in an observation matrix is a submodel of the entire system matrix based on the components that were actually involved.
- Combined with the error incidence, the submodel provides hitting sets in the case of detected faults

Calculating probabilities:
- If there are no faults detected, then the hitting set that would have been generated can be used to compute probabilities to rank the fault.
- Include the probabilty of "goodness" (g) of all components in a hypothesized diagnosis - also called testing fault coverage
- Probability of component included in a test - testing code coverage
- Probability of a failed run, f = 1 - (1 - r(1-g)){\^{}}C, C = number of actual simultaneous failures.

Authors evealuate their method based on the metric of "wasted effort"},
author = {Abreu, Rui and Zoeteweij, Peter and van Gemund, Arjan J. C.},
booktitle = {Proceedings of the 2008 international workshop on dynamic analysis held in conjunction with the ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2008) - WODA '08},
doi = {10.1145/1401827.1401841},
isbn = {9781605580548},
pages = {64},
publisher = {ACM Press},
title = {{An observation-based model for fault localization}},
url = {http://portal.acm.org/citation.cfm?doid=1401827.1401841},
year = {2008}
}
@article{Boumen2009,
annote = {Consider the case of hierarchical test sequencing where there are hierarchies of signature matrices for different "levels" of the problem.

- Use the AND-OR method of Pattipati to solve the problem
- Funnily enough, they don't manage to minimize the cost of the test sequence, but they argue that the "modeling effort" (unclear how this is measured) is minimized.},
author = {Boumen, R. and Ruan, S. and de Jong, I. and van de Mortel-Fronczak, J.M. and Rooda, J.E. and Pattipati, Krishna R.},
doi = {10.1109/TSMCA.2009.2014550},
issn = {1083-4427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
month = {may},
number = {3},
pages = {640--649},
title = {{Hierarchical Test Sequencing for Complex Systems}},
url = {http://ieeexplore.ieee.org/document/4787094/},
volume = {39},
year = {2009}
}
@article{Meskin2010,
annote = {Proves a bunch of theorems on how the residuals for MJLS's can be constructed so that fault isolation is straightforward and as simple as checking if residual 'r' was generated.},
author = {Meskin, Nader and Khorasani, Khashayar},
doi = {10.1109/TAC.2010.2042007},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
month = {jun},
number = {6},
pages = {1343--1357},
title = {{A Geometric Approach to Fault Detection and Isolation of Continuous-Time Markovian Jump Linear Systems}},
url = {http://ieeexplore.ieee.org/document/5404353/ https://spectrum.library.concordia.ca/974140/1/TAC{\_}FP{\_}08-563{\_}Khorasani.pdf},
volume = {55},
year = {2010}
}
@inproceedings{Brady2011,
annote = {In rehabilitation settings, adequate ability to troubleshoot and repair robots might be necessary in order to minimize risk to patients. This paper proposes training methods for care-givers on how to use a robot.},
author = {Brady, K. and Hidler, J. and Nichols, D. and Ryerson, S.},
booktitle = {2011 IEEE International Conference on Rehabilitation Robotics},
doi = {10.1109/ICORR.2011.5975378},
isbn = {978-1-4244-9862-8},
month = {jun},
pages = {1--5},
publisher = {IEEE},
title = {{Clinical training and competency guidelines for using robotic devices}},
url = {http://ieeexplore.ieee.org/document/5975378/},
year = {2011}
}
@article{McGuire2018,
annote = {Contribution:
- Framework that enables a team to cooperatively recover from failure of autonomy by learning optimal task allocations
- Use contextual knowledge during the allocation
- Outperforms conventional static policies and non-contextual policy learning approaches
- Do not attempt to estimate internal parameters for any particular actor; only estimate rewards based on past experiences

Assumptions:
- Human aid sources have other primary responsibilities besides monitoring robot operations
- Subtask capabilities of robots are fixed at deploy time
- Any subtask can fail, and the failure likelihoods are unknown a priori
- Assistance outcomes are only known for those actors assigned to assist the robot
- Robot knows the common context vectors across all actors. Also knows the actor specific context vector.
- Poor selection of assistance will not result in mission-ending damage.

Method:
- Based on history of context vector knowledge, actor-specific context vectors, and rewards, one can solve a linear regression problem to estimate the expected reward of asking in a new situation

Implementation:
- Common context: season, light level
- Actor context: time in location, time awake, time since last food, stress level
- Benefits: function of actor proficiency at subtask
- Costs: fixed value based on subtasks, with additional penalty for sleeping humans and location
- Reward: ratio of benefit to cost

Notes:
- It is natural to consider how autonomous robots can leverage external assistance to recover from failures and resume operations.
- Assistants consulted for recovery, but also to observe exemplars from which to learn better quality solutions in the future
- Other possible solution: POMDP. Requires accurate system model to evaulate actions and rewards though
- Adaptive bandit algorithms that require tuning are likely to overfit to the underlying dynamics of the problem.

Additional notes in scanned copy.},
author = {McGuire, Steve and Furlong, P. Michael and Heckman, Christoffer and Julier, Simon and Szafir, Daniel and Ahmed, Nisar},
doi = {10.1109/LRA.2018.2801468},
issn = {2377-3766},
journal = {IEEE Robotics and Automation Letters},
month = {jul},
number = {3},
pages = {1639--1646},
title = {{Failure is Not an Option: Policy Learning for Adaptive Recovery in Space Operations}},
url = {http://ieeexplore.ieee.org/document/8279454/ https://drive.google.com/open?id=1sZBL6dGiQ5CUzzOOto9Yoa6SQrIbSFqJ},
volume = {3},
year = {2018}
}
@article{Khalastchi2018a,
annote = {Targeted exhaustive survey of FDD (and its related terms of anomaly detection, etc) in the field of robotics.

See the scanned copy.

Notes below from Research Document:

Faults are precipitated by failures of
Hardware: influence the information feed of the robot and its ability to perform instructions
Software: influence the cognitive behaviour of the robot
Interaction: result of internal faults, or exogenous events.
From survey of Robocup faults:
Faults in sensors have similar frequency to faults in the platform; but sensors affect missions much more
Algorithms are more affected by faults than hardware
Failure of algorithms is a major problem
Possible reasons why existing FDD is not sufficient for robotics
Real time requirements are difficult to meet
Faults related to knowledge are harder to detect and diagnose than hardware faults
Recovery mechanisms are not flexible enough to manage situations arising in complex missions.
FDD based on the different approaches
Data-driven approaches
Heavily dependent on the quality of data
In robotics, unsupervised approaches are more suitable because previously unseen faults might occur
Model-free approaches are often easier to implement because it is hard to correctly model the interactions between different components and behaviours
Unknown faults are often detected using outlier detection
Challenges in data-driven approaches:
Do not often exploit known knowledge about the system
Need to perform some form of dimensionality reduction; otherwise there is often too much data
Not all faults necessarily cause observable unexpected behaviour. Corollary: data that is fault-free might contain relevant, hidden faults
Diagnosis is generally hard
Model-based approaches
Rely on an explicit a priori model of a system's behaviour, structure, and its known faults
Inconsistencies between observations and the model can then help in detection and isolation
Two types of models:
Behavioural - hard to construct, for any reasonably complicated robot
Structural - easier to construct, but they cannot extrapolate to inconsistencies in observed behaviour. Can help with diagnosis though
Two schools of thought for model-based FDD
Diagnosis (DX) MBD: consistency based on logical formulas of model and observations. Challenges:
Constructing the model
Presenting the dynamics
Representing behaviour
Representing interactions
Handling fault propagation
Fault Detection {\&} Isolation (FDI) MBD: numerical analytical models of components compared to fault residuals.
Suitable for internal components that have little interaction with the environment
Less suitable for robot as a whole, or for the behaviour control architectures.
Knowledge-based approaches
Mimic the behaviour of expert humans. Often combine model-based and data-driven approaches
Causal analysis
Use a signed directed graph to represent the topology of a robotic system as causal relationships between the system's components and known fault symptoms
Rely on a priori fault isolation knowledge
Expert systems: IF-THEN rules collected from a human expert.
The challenges of FDD based on the characteristics of robots:
Dependence on sensors used to sense external events (internal sensors are always useful for FDD)
Sensor fault detection can either be based on a priori models (fail in dynamic environments), or based on violations from a fused consensus of other sensors
External sensors might also implicitly provide information about failure of internal components
Obtaining training data for training models to detect faults is hard - fault might not propagate the same way in simulation as it does in practice
Level of autonomy
Autonomous robots need
To quickly detect and handle faults as they occur
An FDD system that has low computational overhead
Fault isolation/diagnosis (for recovery) is the process of selecting (minimal) sets of components which, if regarded as faulty, may explain inconsistencies (Reiter, 1987)
Faults can be diagnosed based on
A priori models of normal system behaviour
Statistical data driven approaches using machine learning for outlier detection. Learning offline often reduces online computational load, but might not fit new behaviours.
Deliberation and Planning
Not as well researched as other aspects of fault detection
Having a plan is useful for FDD as it provides context, expectations, and focus.
Expectations are generally specified through the use of system / behaviour / belief models.
Execution monitoring can compare the observed outcome of an action to the expected outcome.
Goal reasoning can be used to detect and diagnose faults if goals are consistently not being achieved
Belief checking for inconsistencies between beliefs can also indicate faults
Context of operation
Context comprises of robot's state, environment state, and task at hand. What might be normal in one context, could be a fault in another
Handled by context detection, followed by context-appropriate FDD
Interaction with Environment
Gap in the use of FDD techniques to detect and diagnose HRI related faults
Teams of robots increase the challenges of FDD, as there is potential for social FDD based on interactions of faulty robots with healthy ones},
author = {Khalastchi, Eliahu and Kalech, Meir},
doi = {10.1145/3146389},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {jan},
number = {1},
pages = {1--24},
title = {{On Fault Detection and Diagnosis in Robotic Systems}},
url = {http://dl.acm.org/citation.cfm?doid=3177787.3146389 https://drive.google.com/file/d/11O95gzkejG4yw0A48oZUIAcDm907Qrym/view?usp=sharing https://docs.google.com/document/d/1uxM1ZNC{\_}BqdM7AGrHiS2bYLFSOQoSvbVbNj3Hxudr2E/edit},
volume = {51},
year = {2018}
}
@inproceedings{Cakmak2012,
address = {New York, New York, USA},
annote = {Focus: incorporation of Active Learning with LFD impacts a robot's interaction with its user because compliance with robot questions is not guaranteed. Human teachers ignore robot queries when they judge the question to be bad.

Label Query:
- Execute a motion and ask if the skill was correct
- What to do with negative examples is not straightforward

Demo(-nstration) Query:
- Create a new scenario and ask for demonstration from the user
- Gives less control over the information that is acquired
- Learner only specifies constraints
- Instance of active class selection - requesting an example of a particular class
- Some methods of constraint - select start state, manipulate the target object, allow control of a subset of joints, etc.

Feature Query:
- Whether a feature is important or relevant to the target context
- "Features with which instances are represented are meaningful for humans and the way they contribute to the classification of an instance is intuitive"

Method 1: Experiment to evaluate how humans ask questions
- 2 tasks focused on learning a task goal (desired end state) and 2 tasks focused on learning skills to perform the task (desired actions or movements)
- Majority of queries were feature queries
- 4 subcategories of feature queries - relevance tests (does f matter), invariance tests (does f have to be g), value tests (can f be g), value requests (what can f be). Value requests are the least common.
- 3 types of yes/no questions and the choice between them depends on person's expectations on the answer/their confidence in the answer
- Label queries are inherently embodied; demo could be either. Feature queries can be purely verbal; they require a common ground where both parties understand the features and how they work.
- Features involved in skill learning are about movements. Features involved in goal learning are about objects in the world.

Method 2: human perceptions of the 3 types of queries
- Label queries are costly to formulate, easy to answer for the human
- Demo queries are easy to formulate, hard to answer for the human
- Feature queries can be as easy as label queries, but can take longer if the human cannot interpret the query, which is possible.

Design Implications:
- Primary usage of feature queries. But these are the most challenging to produce automatically in a manner that is interpretable by the human
- For label queries, partial label queries are a good option
- Closed form queries are desirable
- Limiting answers to yes/no could result in inaccurate information or missed opportunities for additional information
- Question forms are a transparency mechanism for confidence
- Some things, such as skills, are represented by features that are hard to verbalize but easy to instantiate.},
author = {Cakmak, Maya and Thomaz, Andrea L.},
booktitle = {Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction - HRI '12},
doi = {10.1145/2157689.2157693},
isbn = {9781450310635},
pages = {17},
publisher = {ACM Press},
title = {{Designing robot learners that ask good questions}},
url = {http://dl.acm.org/citation.cfm?doid=2157689.2157693 https://drive.google.com/file/d/1VE8EQPYJNE1ZRHArmt5ub63Wtk0lKppl/view},
year = {2012}
}
@inproceedings{Yang2014,
annote = {Use particle filters to estimate the remaining useful life of a an aircraft power unit. Need a system model, transition model, specified (and the system model is quite simplistic, tbh). The parameters for the model were estimated from the data, though.},
author = {Yang, Chunsheng and Lou, Qingfeng and Liu, Jie and Yang, Yubin and Bai, Yun},
booktitle = {International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems},
doi = {10.1007/978-3-319-07455-9_21},
pages = {198--207},
publisher = {Springer},
title = {{Particle Filter-Based Method for Prognostics with Application to Auxiliary Power Unit}},
url = {http://link.springer.com/10.1007/978-3-319-07455-9{\_}21},
year = {2014}
}
@phdthesis{jahnke2015machine,
annote = {Really comprehensive literature review of the methods followed by researchers in pursuit of fault isolation and CBM (condition-based monitoring)

One of the insights of the work is that there isn't a failure dependency modeling system that exists, to date.},
author = {Jahnke, Patrick},
school = {Technische Universit{\"{a}}t Darmstadt},
title = {{Machine learning approaches for failure type detection and predictive maintenance}},
url = {https://www.ke.tu-darmstadt.de/lehre/arbeiten/master/2015/Jahnke{\_}Patrick.pdf},
volume = {19},
year = {2015}
}
@inproceedings{Trave-Massuyes2007,
annote = {Extends traditional DX approaches to diagnoses by incorporating the use of time to do some forecasting.

Uses time-labels that might be included in the system model to provide incremental time-labeled},
author = {Trave-Massuyes, Louise and Calderon-Espinoza, Gabriela},
booktitle = {2007 European Control Conference (ECC)},
doi = {10.23919/ECC.2007.7068364},
isbn = {978-3-9524173-8-6},
month = {jul},
pages = {2272--2279},
publisher = {IEEE},
title = {{Timed fault diagnosis}},
url = {https://ieeexplore.ieee.org/document/7068364/},
year = {2007}
}
@inproceedings{breese1996decision,
annote = {- "Inference focuses on identifying the set of faults consistent with the observations and, in the probabilistic case, assigning probabilities to the feasible diagnoses. Information gathering proceeds until a single cause has been identified or the current diagnosis is sufficiently restricted to support an action"
- We can do more than just diagnose: "At any stage of the process, there are many possible observations, tests, or repairs that can be applied"

Framework:
- Represent the device as a Bayes Net
- There's a problem defining variable that has one state == normal. If !normal, then troubleshooting applies
- The device is made of observable and unobservable components.
- Observable components can be observed and repaired, unobservable components can only be repaired
- There is a probability that the problem defining variable is fixed if component i is fixed. Alongwith the cost of observation and the cost of repair for the component, there is a metric of expected cost of repair (ECR) that is important.

Method:
- The Bayes Net is slightly modified to include a node that defines all possible mappings between components. By including an independent mapping component, they are essentially honouring the causal independence assumption, where changes to the cause, should not change the mechanism. They call the presence of the mapping nodes as "causal persistence"
- Causes can influence the effect by way of mediator nodes. Since the repair is a "do" action, and might not be available, the probability of fixing the device with a "do" action can be approximated as the probability of an intermediate mediator node as being faulty.
- Similar to the expected cost of repair, there is an expected cost of observation metric (ECO) that help reason about the cost of making an observation at an intermediate node.
- For reasoning about configuration changes, incorporate a switch node. Then there is an "expected cost of configuration change + observation change" metric (ECCO) for reasoning. To evaluate the configuration change, create another "persistence Bayes Net"
- In order to troubleshoot, check for the lowest cost among ECR, ECO, and ECCO, and pick the action that has the least cost.

This method was deployed in the Windows 95 troubleshooter. The method saved minutes, and empirically, there is a benefit to the configuration approach.},
author = {Breese, John S and Heckerman, David},
booktitle = {Proceedings of the Twelfth international conference on Uncertainty in artificial intelligence},
organization = {Morgan Kaufmann Publishers Inc.},
pages = {124--132},
title = {{Decision-Theoretic Troubleshooting: A Framework for Repair and Experiment}},
url = {https://arxiv.org/pdf/1302.3563.pdf},
year = {1996}
}
@article{Zhang2018c,
annote = {Parameter (and structure!) learning for the coupled HMMs in the Zhang, Pattipati work. Applied in the domain of diagnosis for diesel engines.

Details are unclear.},
author = {Zhang, Shigang and Luo, Xu and Yang, Yongmin and Wang, Long and Zhang, Xiaofei},
doi = {10.1109/ACCESS.2018.2877959},
issn = {2169-3536},
journal = {IEEE Access},
pages = {65065--65077},
title = {{Optimization of a Dynamic Fault Diagnosis Model Based on Machine Learning}},
url = {https://ieeexplore.ieee.org/document/8509575/},
volume = {6},
year = {2018}
}
@inproceedings{Rodriguez2011,
annote = {Goal - minimize the expected time to a grasp in a cluttered bin pick

Model the sensory signals from arm sensors as a Markov Chain to predict the likelihood of a grasp from a cluttered bin failing on approach. If the grasp is about to fail, then abort early. There is some analysis in the paper that proves (and shows) that such an approach minimizes the overall time to a successful grasp.},
author = {Rodriguez, Alberto and Mason, Matthew T. and Srinivasa, Siddhartha S. and Bernstein, Matthew and Zirbel, Alex},
booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6095100},
isbn = {978-1-61284-456-5},
month = {sep},
pages = {1804--1810},
publisher = {IEEE},
title = {{Abort and retry in grasping}},
url = {http://ieeexplore.ieee.org/document/6095100/},
year = {2011}
}
@book{peters2017elements,
annote = {Causal inference textbook that can be used as another source of citations, if need be},
author = {Peters, Jonas and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
publisher = {MIT press},
title = {{Elements of causal inference: foundations and learning algorithms}},
year = {2017}
}
@inproceedings{Fainekos,
annote = {The final missing piece in the Hadas pipeline for generating motion from LTL. Woeks by:
- Discretize the continuous space of the robot
- Create an LTL formulation of the motion plan through discrete space
- Verify that the motion is admissible according to rules of LTL task specification
- If admissible, use bisimulation (a mathematical property allowing the mapping from continuous to discrete domain) to create the continuous controller
- The last is a form of hybrid control.},
author = {Fainekos, G.E. and Kress-Gazit, H. and Pappas, G.J.},
booktitle = {Proceedings of the 2005 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2005.1570410},
isbn = {0-7803-8914-X},
pages = {2020--2025},
publisher = {IEEE},
title = {{Temporal Logic Motion Planning for Mobile Robots}},
url = {http://ieeexplore.ieee.org/document/1570410/},
year = {2005}
}
@inproceedings{steinbauer2005detecting,
annote = {Model-based diagnosis of the software system of a robot. Requires a system model of the software architecture, models of expectations from observers.

Then based on the observation models and the system model, we can diagnose and repair any detected errors.},
author = {Steinbauer, Gerald and Wotawa, Franz},
booktitle = {IJCAI},
organization = {Citeseer},
pages = {1742--1743},
title = {{Detecting and locating faults in the control software of autonomous mobile robots.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.488.8790{\&}rep=rep1{\&}type=pdf},
volume = {5},
year = {2005}
}
@article{Ehsan2017,
abstract = {We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation.},
annote = {Rationalization is the preferred method of explaining autonomous agent behaviour; compared to simply declaring, mechanistically, the actions.

Based on a thematic analysis, the reasons for the preference were:
- Explanatory power
- Relatability
- Ludic (playfulness) quality
- Adequately detailed},
archivePrefix = {arXiv},
arxivId = {1702.07826},
author = {Ehsan, Upol and Harrison, Brent and Chan, Larry and Riedl, Mark O.},
eprint = {1702.07826},
month = {feb},
title = {{Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations}},
url = {http://arxiv.org/abs/1702.07826},
year = {2017}
}
@article{Reiter1987,
annote = {Referenced by pretty much everyone as the start of model-based diagnosis.

Defines diagnosis as a conjecture that some minimal set of components are faulty. ={\textgreater} the minimal set of components misbehaving along with the other components behaving normally is consistent with observations and the system model.

Solves the diagnosis problem as a set-cover/hitting-set problem.

Section 8 contains all the contributions that are used in later works.},
author = {Reiter, Raymond},
doi = {10.1016/0004-3702(87)90062-2},
issn = {00043702},
journal = {Artificial Intelligence},
month = {apr},
number = {1},
pages = {57--95},
title = {{A theory of diagnosis from first principles}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0004370287900622},
volume = {32},
year = {1987}
}
@inproceedings{Lee2010a,
annote = {- In the event of a service failure, especially within the context of large organizations or lots of technology, the blame and recovery process can be complicated
- Service failure can negatively impact perceptions of the robot. Recovery strategies and setting expectations can mitigate these effects
- Recovery strategies include apology, compensation for future services, and offering alternative actions
- People have social or utilitarian orientations towards a service. The former emphasizes the interaction during service, while the latter the efficiency or correctness. Robot's choice of recovery should match people's orientation.

User study conducted by watching participants on mechanical turk to test satisfaction ratings of robot as a function of mitigation and recovery strategies. Also as a function of service orientation of the person.},
author = {Lee, Min Kyung and Kiesler, Sara and Forlizzi, Jodi and Srinivasa, Siddhartha and Rybski, Paul},
booktitle = {2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
doi = {10.1109/HRI.2010.5453195},
isbn = {978-1-4244-4892-0},
month = {mar},
pages = {203--210},
publisher = {IEEE},
title = {{Gracefully mitigating breakdowns in robotic services}},
url = {http://ieeexplore.ieee.org/document/5453195/},
year = {2010}
}
@inproceedings{Wurhofer2018,
abstract = {In production environments, the number of distributed, net-worked, and automated systems has grown rapidly and is expected to continue to grow in the future. This affects hu-mans' work fundamentally, in terms of their tasks and routines. Increasing automation and digitalization leads to a substan-tial change of human-machine interactions on the shop floor, raising the question about humans' role in highly automated environments. In this paper, we shed light on how work in in-creasingly automated and digitalized factories is experienced, drawing on interviews with operators and maintenance engi-neers from three different industrial contexts. By reflecting on actual and anticipated developments in smart production environments, we point out how workers will experience those contexts. We finally discuss resulting challenges and leverage points for smart factories, i.e., areas where HCI and CSCW can contribute to positively influence workers' experiences in times of increasing automation and digitalization.},
address = {New York, New York, USA},
annote = {In future factories:
- workers will adopt roles more as troubleshooters or problem solvers
- workload of such a new worker can be greatly reduced by including troubleshooting assistance into robots (alongside more training on the equipment).},
author = {Wurhofer, Daniela and Meneweger, Thomas and Fuchsberger, Verena and Tscheligi, Manfred},
booktitle = {Proceedings of the 2018 ACM Conference on Supporting Groupwork - GROUP '18},
doi = {10.1145/3148330.3148349},
isbn = {9781450355629},
pages = {284--296},
publisher = {ACM Press},
title = {{Reflections on Operators' and Maintenance Engineers' Experiences of Smart Factories}},
url = {http://dl.acm.org/citation.cfm?doid=3148330.3148349},
year = {2018}
}
@article{Chen2017,
abstract = {We consider the optimal value of information (VoI) problem, where the goal is to sequentially select a set of tests with a minimal cost, so that one can efficiently make the best decision based on the observed outcomes. Existing algorithms are either heuristics with no guarantees, or scale poorly (with exponential run time in terms of the number of available tests). Moreover, these methods assume a known distribution over the test outcomes, which is often not the case in practice. We propose an efficient sampling-based online learning framework to address the above issues. First, assuming the distribution over hypotheses is known, we propose a dynamic hypothesis enumeration strategy, which allows efficient information gathering with strong theoretical guarantees. We show that with sufficient amount of samples, one can identify a near-optimal decision with high probability. Second, when the parameters of the hypotheses distribution are unknown, we propose an algorithm which learns the parameters progressively via posterior sampling in an online fashion. We further establish a rigorous bound on the expected regret. We demonstrate the effectiveness of our approach on a real-world interactive troubleshooting application and show that one can efficiently make high-quality decisions with low cost.},
annote = {Value-of-Information is used as a metric for active learning, where the value is determined based on estimates for values of hidden states.

Decision-Region Determination (DRD) - resolve uncertainty in state values only until a decision can be made; no need to solve for the actual value of a state.

This paper deals with the problem of asking the fewest, but most informative questions, during conversational troubleshooting to help figure out the root cause of an observed failure. Current methods assume a limited number of tests (questions), and as a result are able to enumerate all possible hypotheses to choose between tests to use. However, in many real-world scenarios, there are many-many tests; this work addresses that problem.

Broad strokes solution - sample hypotheses from a large space. But MC-methods usually require a large number of samples. The authors here propose using a local hypothesis enumeration strategy to exploit the structure of the problem as a sequential decision problem.

Problem definition - the problem needs to be defined as the association of tests to root causes, almost akin to the signature matrix.

Compared to the previous Javdani et al work:
- this work enumerates hypotheses used to calculate the average expected information differently by using coverage-based MCMC sampling
- Filtering locally enumerated tests with EC2, but making sure to honour the sampled conditional probabilities
- Learning the conditional probability of tests given the hypothesis in an online manner.},
archivePrefix = {arXiv},
arxivId = {1703.05452},
author = {Chen, Yuxin and Renders, Jean-Michel and Chehreghani, Morteza Haghir and Krause, Andreas},
eprint = {1703.05452},
month = {mar},
title = {{Efficient Online Learning for Optimizing Value of Information: Theory and Application to Interactive Troubleshooting}},
url = {http://arxiv.org/abs/1703.05452},
year = {2017}
}
@article{Leucker2009,
annote = {Traditionally, there are three verification techniques:
- theorem proving - mostly manual
- model checking - mainly applicable in finite-state systems; an automatic verification technique
- testing - a wide range of ad-hoc and incomplete methods mostly used to find bugs

Runtime verification is a lightweight verification technique that is used to check whether a run of the system under scrutiny satisfies or violates a given correctness property. It deals only with the detection of violations.

Monitor - a device that reads a finite trace and yields a verdict of true/false in some truth domain. Monitors are typically used for runtime verification. Monitors are typically specified with high-level logic specifications such as LTL.

Ideal properties of a monitor:
- Impartiality - a subset of a trace should not yield true/false if a continuation yields a different result
- Anticipation - a subset of a complete trace should not yield a different result from the larger trace.

As opposed to model checking, runtime verification is often applicable to black-box systems for which no models exist

Runtime verification can be considered a form of oracle-based or passive testing.

Going beyond runtime verification:
- Fault Diagnosis, Isolation, and Recovery - finding the causes of a detected runtime fault
- Runtime Reflection - in addition to verification, perform a simplified Reiter's diagnosis. Usually consists of 4 layers - logging, monitoring (verification), diagnosis (identification), mitigation
- A variant of runtime reflection (a method) is the paradigm of monitor oriented programming},
author = {Leucker, Martin and Schallhart, Christian},
doi = {10.1016/j.jlap.2008.08.004},
issn = {15678326},
journal = {The Journal of Logic and Algebraic Programming},
month = {may},
number = {5},
pages = {293--303},
title = {{A brief account of runtime verification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1567832608000775},
volume = {78},
year = {2009}
}
@article{Jaakkola1999,
abstract = {We describe a variational approximation method for efficient inference in large-scale probabilistic models. Variational methods are deterministic procedures that provide approximations to marginal and conditional probabilities of interest. They provide alternatives to approximate inference methods based on stochastic sampling or search. We describe a variational approach to the problem of diagnostic inference in the `Quick Medical Reference' (QMR) network. The QMR network is a large-scale probabilistic graphical model built on statistical and expert knowledge. Exact probabilistic inference is infeasible in this model for all but a small set of cases. We evaluate our variational inference algorithm on a large set of diagnostic test cases, comparing the algorithm to a state-of-the-art stochastic sampling method.},
annote = {Common bipartite graph representation used in fault diagnosis relating the different faults to the different diseases.},
author = {Jaakkola, T. S. and Jordan, M. I.},
doi = {10.1613/jair.583},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
month = {may},
pages = {291--322},
title = {{Variational Probabilistic Inference and the QMR-DT Network}},
url = {https://jair.org/index.php/jair/article/view/10229},
volume = {10},
year = {1999}
}
@inproceedings{steinbauer2010way,
annote = {Assuming that the robot enters room A in reality, but believes it entered room B, then how do you go about ascertaining that wrong belief and repairing it?

Not a clear answer, but essentially assuming that the KB consists of logical clauses, then simulate alternate values of suspect values (only!) in the KB to arrive at the diagnosis.

The paper doesn't seem to provide a clear method of doing that.},
author = {Steinbauer, Gerald and Wotawa, Franz},
booktitle = {the 21st International Workshop on Principles of Diagnosis (DX-10)},
title = {{On the way to automated belief repair for autonomous robots}},
url = {http://ftp.phmsociety.org/sites/phmsociety.org/files/phm{\_}submission/2010/phmc{\_}10{\_}121.pdf},
year = {2010}
}
@article{FangTu2003,
annote = {Extension of the Pattipati {\&} Alexandridis, 1990 work.

- Introduce a strategy called Rollout (unclear what exactly the strategy is)
- Also use Information Gain and their Huffman coding based approximation of cost-to-go in a test sequence (the Huffman coding metric is simply a heuristic approximation, like the one for A*, that the authors have shown in prior work is an admissible heuristic).
- Show that using Rollout with Information Gain or the Huffman coding heuristic is better for fault diagnosis in a variety of domains in terms of minimizing the costs of the tests.},
author = {{Fang Tu} and Pattipati, Krishna R.},
doi = {10.1109/TSMCA.2003.809206},
issn = {1083-4427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
month = {jan},
number = {1},
pages = {86--99},
title = {{Rollout strategies for sequential fault diagnosis}},
url = {http://ieeexplore.ieee.org/document/1206458/},
volume = {33},
year = {2003}
}
@article{Reppa2016,
annote = {Tutorial detailing the process of performing ARR-based FDD for sensors. Also includes a learning component for learning the (adaptive) thresholds for residual generation based on modeling uncertainties

For fault isolation, the authors discuss multiple distributed decentralized fault isolation methodologies (using ARRs and signature matrices)},
author = {Reppa, Vasso and Polycarpou, Marios M and Panayiotou, Christos G.},
doi = {10.1561/2600000007},
issn = {2325-6818},
journal = {Foundations and Trends{\textregistered} in Systems and Control},
number = {1-2},
pages = {1--248},
title = {{Sensor Fault Diagnosis}},
url = {http://www.nowpublishers.com/article/Details/SYS-007},
volume = {3},
year = {2016}
}
@article{Yu2017,
annote = {- A prior work (Baah et al 2010) is capable of generating a "dependency" graph given a program
- This work converts the dependency graph into a Bayes Net for fault localization.
- The first step is structure learning of the net where the cyclic program graph must be made into a DAG. Key contribution here is the use of mutual information
- Then they use correct program executions to learn parameters for the Bayes Net
- Finally, use Junction Tree on the learned Bayes Net to assign probability to unobserved nodes given outputs.
- Some heuristics are present for deciding when a node is at fault vs. not.
- Strangely, Abreu et al. is never mentioned.},
author = {Yu, Xiao and Liu, Jin and Yang, Zijiang and Liu, Xiao},
doi = {10.1016/j.jss.2017.08.025},
issn = {01641212},
journal = {Journal of Systems and Software},
month = {dec},
pages = {44--53},
title = {{The Bayesian Network based program dependence graph and its application to fault localization}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121217301796},
volume = {134},
year = {2017}
}
@inproceedings{nushi2017human,
annote = {- Diagnose and fix errors in component-based ML systems through troubleshooting
- "Human intervention is crucial to the approach as human fixes simulate improved component output that cannot be produced otherwise without significant system development efforts"
- Assume that the I/O of components and how they connect is provided to the system that has been developed
- Troubleshooting involves answering 2 questions: how does the system fail? how to improve the system?
- Problems to deal with: a) accuracy/inaccuracy measures are not binary because partial correctness is a thing, b) there can be complex interactions between ML components, c) non-monotonic error propagation, e.g. fixing a component error might not result in a system fix

Method:
- Assume - a) modular system defined through component I/O, b) the I/O is human interpretable
- Given - a component connection diagram
- First, assess the regular system I/O on desired quality measures
- Then, for each "fix", complete micro-tasks to correct component output (if output is wrong). Fixes are do operators to the component outputs
- Then execute the fixed outputs through the whole system
- Evaluate the fixed system after all the micro-tasks are complete
- Such a method addresses the three challenges for troubleshooting component-based ML systems.},
author = {Nushi, Besmira and Kamar, Ece and Horvitz, Eric and Kossmann, Donald},
booktitle = {Thirty-First AAAI Conference on Artificial Intelligence},
title = {{On human intellect and machine failures: Troubleshooting integrative machine learning systems}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/15032/13875},
year = {2017}
}
@inproceedings{zhong2002hmms,
annote = {Can use this as a helper to understand the mixed memory Markov model introduced by Saul and Jordan, 1999.},
author = {Zhong, Shi and Ghosh, Joydeep},
booktitle = {proceedings of the IEEE international joint conference on neural networks},
pages = {1159--1254},
title = {{HMMs and coupled HMMs for multi-channel EEG classification}},
url = {https://www.researchgate.net/profile/Shi{\_}Zhong/publication/3949928{\_}HMMs{\_}and{\_}coupled{\_}HMMs{\_}for{\_}multi-channel{\_}EEG{\_}classification/links/02e7e520ea0a8e7fa3000000.pdf},
volume = {2},
year = {2002}
}
@article{Dogramadzi2014,
annote = {Aim of hazard analysis if to identify all plausible and reasonably forseeable hazards associated with a system's operation in its environment. Two broad approaches, both equivalent:
- Function-oriented view - failure conditions modeled as failures with a system's functional process
- Interface-oriented view - hazards occur from failures in the I/O values observed at system interface

Methods of hazard identification:
- Functional Hazard Assessment (FHA) - example of Function-oriented hazard analysis; frequently carried out in aerospace
- HAZOP (Hazard and Operability Studies) - example of an Interface-oriented approach.
- SHARD is a derivative of HAZOP with different guide words. Those guide words are also function-oriented. Hence chosen by the authors
- FMEA - typically used much later in the design process and tied to specific system components. Often succeeds a Hierarchical Task Analysis
- FTA - hard to find failure rates and success probabilities for robots.

There is a distinction between Automatic and Autonomous. The former operates in constrained environments and only performs mission tasks; the latter in unconstrained environments might be needed to perform non-mission tasks.

SHARD, as is, cannot model autonomous agents; authors propose the use of Environmental Survey Hazard Analysis (ESHA). Environment consists of env features, objects, and agents. Use guide words and the ESHA worksheet to identify hazards in the interactions between those entities.

Before doing a hazard analysis, it helps to do a Hierarchical Task Analysis. Use a NASA Goddard Agent Model to identify the functional components of the agent when doing the analysis; especially in the abstract.},
author = {Dogramadzi, Sanja and Giannaccini, Maria Elena and Harper, Christopher and Sobhani, Mohammad and Woodman, Roger and Choung, Jiyeon},
doi = {10.1007/s10846-013-0020-7},
issn = {0921-0296},
journal = {Journal of Intelligent {\&} Robotic Systems},
month = {sep},
number = {1},
pages = {73--117},
title = {{Environmental Hazard Analysis - a Variant of Preliminary Hazard Analysis for Autonomous Mobile Robots}},
url = {http://link.springer.com/10.1007/s10846-013-0020-7},
volume = {76},
year = {2014}
}
@article{Pettersson2005,
annote = {Essentially a precursor to the Khalastchi and Kalech paper on FDD. It goes into more depth in the section on FDD as broken down by various methods.

Execution monitoring: a continuous real-time task of determining the conditions of a physical system by recording information, recognizing, and indicating anomalies in the behaviour.

Diagnosis consists of Isolation (what is going wrong) and Identification (how wrong is it - the magnitude of the fault).

Execution monitoring is analytical, data-driven, or knowledge-based.

Analytical - residual generation and then decision making based on residuals. Three approaches to residual generation:
- Parameter estimation
- Parity relations
- Observers
All three have been shown to be equivalent to each other.

In addition to the methods of analytical models presented in the other works, this work also provides a very good introduction to the implementation of FDD for logic-based planners.

Data-driven approaches can either be univariate or multivariate.

Knowledge based approaches - causal, expert systems, or neural nets.

Causal:
- Signed Directed Graph - relationsip between process variables. Propagate the sign of a threshold violation based on linkages to diagnose the symptom of an observed fault
- Heterogenous knowledge-based approach with the different types of knowledge linked with partitioned graphs.

Expert Systems:
- IF-THEN rules
- More general allows for probabilistic and time-dependent specification of rules. Use LTL for that.
- Can also ground qualitative descriptors to signals, and specify rules based on qualitative descriptions

Neural Nets:
- Usually used in cases where the model is not as accurate.
- Can be used hierarchically for fault detection and diagnosis.},
author = {Pettersson, Ola},
doi = {10.1016/j.robot.2005.09.004},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
month = {nov},
number = {2},
pages = {73--88},
title = {{Execution monitoring in robotics: A survey}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S092188900500134X},
volume = {53},
year = {2005}
}
@inproceedings{Zhang2015,
annote = {An implementation of the Zhang and Pattipati work. The Zhang of that work is the second Zhang in this work.},
author = {Zhang, Wei and Zhang, Shigang and Song, Lijun and Hu, Zheng},
booktitle = {2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)},
doi = {10.1109/CYBER.2015.7288253},
isbn = {978-1-4799-8728-3},
month = {jun},
pages = {1992--1996},
publisher = {IEEE},
title = {{A prognostic framework based on factorial Hidden Markov Model}},
url = {http://ieeexplore.ieee.org/document/7288253/},
year = {2015}
}
@incollection{Leitner-Fischer2013,
annote = {Extension of the Kuntz et al., 2011 work.

- Causality checks are mapped to sub/super-set comparisons of execution traces
- Propose a data structure called a subset graph to store counterexamples and speed up the search process.

Notes:
- Define traces as sequences of states and actions
- Use LTL to specify hazards - reachability of unsafe states
- Introduce the notion of events, and the ordering of them through event order logic
- Then a bunch of long-winded logic is used to define causality as the occurrence or non-occurrence of events within traces that are possible

(I think) The main contribution here is that by storing possible traces in memory as events happen, on-the-fly causality checks with counter factuals speeds up},
author = {Leitner-Fischer, Florian and Leue, Stefan},
doi = {10.1007/978-3-642-35873-9_16},
pages = {248--267},
title = {{Causality Checking for Complex System Models}},
url = {http://link.springer.com/10.1007/978-3-642-35873-9{\_}16 http://kops.uni-konstanz.de/bitstream/handle/123456789/23316/leitner{\_}fischer{\_}233165.pdf?sequence=2 https://www.sen.uni-konstanz.de/research/research/causcheck/},
year = {2013}
}
@inproceedings{kappler2015data,
annote = {TODO: Scan notes on physical paper

Introduces the concept of a manipulation graph and proposes a structure called Associative Skill Memories (ASM) to automatically transition between states in the graph. ASMs are classifications of the robot sensory signals to different states in the manipulation graph and as a result can be used to identify when environmental factors might've caused a transition to new states or if failures have occured and different portions of the manipulation graph might need to be executed.},
author = {Kappler, Daniel and Pastor, Peter and Kalakrishnan, Mrinal and W{\"{u}}thrich, Manuel and Schaal, Stefan},
booktitle = {Robotics: Science and Systems},
title = {{Data-Driven Online Decision Making for Autonomous Manipulation.}},
url = {http://roboticsproceedings.org/rss11/p44.pdf},
year = {2015}
}
@inproceedings{tumer2003requirements,
annote = {Failure modes taxonomy that goes beyond the proposed failure taxonomy for mechanical systems that were present until the time of the papers' writing. There is no taxonomy for a robotic system however.

Also points out that a failure mode taxonomy can assist in conducting an FMEA},
author = {Tumer, Irem Y and Stone, Robert B and Bell, David G},
booktitle = {DS 31: Proceedings of ICED 03, the 14th International Conference on Engineering Design, Stockholm},
title = {{Requirements for a failure mode taxonomy for use in conceptual design}},
year = {2003}
}
@article{Chandola2012,
annote = {Notes in linked Google Doc},
author = {Chandola, V. and Banerjee, A. and Kumar, V.},
doi = {10.1109/TKDE.2010.235},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
month = {may},
number = {5},
pages = {823--839},
title = {{Anomaly Detection for Discrete Sequences: A Survey}},
url = {http://ieeexplore.ieee.org/document/5645624/ https://drive.google.com/open?id=13YNjUcQmoHM5TbS78PO86jCtn6EqRcQfK4KC1-2EOp0},
volume = {24},
year = {2012}
}
@inproceedings{Park2017,
annote = {Fault isolation mechanism. Takes as input HMM log likelihoods over time, and some other features over time. Combines them with VGG features from the moment of anomaly detection in order to classify the type of anomaly.

There are two types of anomaly classes - the anomaly type and the anomaly cause. (See Fig. 3 for reference). The proposed method is able to accurately classify well for both types of classification tasks.

Also includes an evaluation and mini-study of the isolation system with Henry Evans. It shows that Evans was pleased with the results and that the system performed well.

Note to self: Not sure where to put this w.r.t the RA-L paper. The code is however a helpful reference for how to run things with keras.},
author = {Park, Daehyung and Kim, Hokeun and Hoshi, Yuuna and Erickson, Zackory and Kapusta, Ariel and Kemp, Charles C.},
booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2017.8206437},
isbn = {978-1-5386-2682-5},
month = {sep},
pages = {5406--5413},
publisher = {IEEE},
title = {{A multimodal execution monitor with anomaly classification for robot-assisted feeding}},
url = {http://ieeexplore.ieee.org/document/8206437/},
year = {2017}
}
@article{Howard2006,
annote = {Prior work to the Causal Model usage by Parker and Kannan. Uses a simple table lookup of fault -{\textgreater} recovery in order to recover from failures. The diagnosis process was probably with custom-built specific fault detectors.},
author = {Howard, Andrew and Parker, Lynne E. and Sukhatme, Gaurav S.},
doi = {10.1177/0278364906065378},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
month = {may},
number = {5-6},
pages = {431--447},
title = {{Experiments with a Large Heterogeneous Mobile Robot Team: Exploration, Mapping, Deployment and Detection}},
url = {http://journals.sagepub.com/doi/10.1177/0278364906065378},
volume = {25},
year = {2006}
}
@incollection{lee2012risk,
annote = {Perform a systematic and thorough safety assessment of the safety systems in a robot designed to augment human power. (Such systems are termed Safety Related Systems, SRS)

- First determine the Safety-Integrity Level of the system. This is done with a graph that evaluates the Severity of potential injury, the Frequency of exposure to those hazards, and the Possibility of avoiding that hazard. Their system needed an SIL of 2.
- Second, create an FTA to analyze the potential causes of system level failure
- Third, conduct an FMEA to quantify the risk priority number of the different faults identified through FTA
- Once the highest risk items are found, the authors created a controller to identify risks (based on threshold monitoring) and shutdown actuators accordingly.
- To analyze the updates to the controller, the authors performed an FMEDA (D - Diagnostic). The output of this is to provide estimates of different failure rates. The failure rates can then be combined into metrics called SFF (Safety Failure Fraction) and PFH (Probability of Failure per Hour). SIL levels determined at the start of the process dictate acceptable thresholds for these other 2 metrics.

Takeaway: the current processes are insufficient and a multi-pronged approach needs to be adopted to safety analysis.},
author = {Lee, Suwoong and Yamada, Yoji},
booktitle = {Human Machine Interaction-Getting Closer},
publisher = {InTech},
title = {{Risk assessment and functional safety analysis to design safety function of a human-cooperative robot}},
url = {https://www.intechopen.com/download/pdf/26322},
year = {2012}
}
@article{Akram2018,
annote = {Uses a reactive approach to fault detection and recovery. Models the robot as an organism with an immune response. The immune response is then used to diagnose a fault and recover from it.},
author = {Akram, Maria and Raza, Ali},
doi = {10.1016/j.biosystems.2018.08.003},
issn = {03032647},
journal = {Biosystems},
month = {oct},
pages = {52--67},
title = {{Towards the development of robot immune system: A combined approach involving innate immune cells and T-lymphocytes}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0303264718301795},
volume = {172},
year = {2018}
}
@article{Shi2015a,
annote = {Survey of stability results for Markov Jump Systems as well as figuring out controllers and/or observers for them under various conditions, including when the transition probabilities are known, partially known, or unknown},
author = {Shi, Peng and Li, Fanbiao},
doi = {10.1007/s12555-014-0576-4},
issn = {1598-6446},
journal = {International Journal of Control, Automation and Systems},
month = {feb},
number = {1},
pages = {1--16},
title = {{A survey on Markovian jump systems: Modeling and design}},
url = {http://link.springer.com/10.1007/s12555-014-0576-4},
volume = {13},
year = {2015}
}
@article{Zenil2019,
annote = {Best understanding of the paper thus far, and it could be completely off-base:
- Parts of an observation that have similar measures of information are likely to have been generated by the same underlying generative model
- Use some notions of Turing machines and Kolmogrov complexity to figure out the complexity of parts of the data
- The goal is to separate observed data into clusters that are likely to have been generated from the same underlying mechanism on the basis of this Turing machine analogy and Kolmogrov complexity

I have no idea how this actually translates to a real-world application (or if it will). The paper presents results with trying to distinguish between multiple graph generating mechanisms and/or multiple cellular automata but I don't know how to relate that back to the real world.},
author = {Zenil, Hector and Kiani, Narsis A. and Zea, Allan A. and Tegn{\'{e}}r, Jesper},
doi = {10.1038/s42256-018-0005-0},
issn = {2522-5839},
journal = {Nature Machine Intelligence},
month = {jan},
number = {1},
pages = {58--66},
title = {{Causal deconvolution by algorithmic generative models}},
url = {http://www.nature.com/articles/s42256-018-0005-0 https://arxiv.org/abs/1802.09904 https://github.com/allgebrist/Causal-Deconvolution-of-Networks},
volume = {1},
year = {2019}
}
@article{Peeters2018,
annote = {Proposes:
- a structured method that is efficient (does not take too much time) and effective (finds all relevant failure modes)
- Start with a system FTA, then use FMEA on that to decide where to do a function FTA
- Then do a function FTA followed by a function FMEA to decide which components to expand upon
- Finally do a component level FTA

Justification:
- The failure modes of a new system are not known from practice
- New systems are typically large and complex with multiple hierarchies of system design
- Often hard to achieve enough depth of analysis to get a full understanding of the failure behaviour

Goals of each step of the iteration: 1) identify the failure modes (FTA) and 2) assess the criticality of those failure modes (FMEA)

Old notes:
Case study paper. Recommends performing recursive FTA-FMEA

Start with FTA at system level. Then perform FMEA on the failures and pick a critcality threshold. Set threshold so that at most 50{\%} are analyzed at the next level.

The next levels below system level are the functional and component levels.},
author = {Peeters, J.F.W. and Basten, R.J.I. and Tinga, T.},
doi = {10.1016/j.ress.2017.11.024},
issn = {09518320},
journal = {Reliability Engineering {\&} System Safety},
month = {apr},
pages = {36--44},
title = {{Improving failure analysis efficiency by combining FTA and FMEA in a recursive manner}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0951832017304192},
volume = {172},
year = {2018}
}
@article{Odrey2005,
annote = {Use Petri-Net augmentations to incorporate recovery into the nominal Petri Net. The paper is 

Input conditioning - returning to the state that failed post recovery
Forward recovery - move to a later state in the nominal state trajectory
Backward recovery - move to a previous state in the nominal state trajectory

Most of the paper is an analysis of the effects on the entire Petri-Net of adding recovery subnets for the different types of recoveries. The main challenges from adding recoveries are:
- the reachability graph can become infinite
- there might be deadlocks

The paper proposes the addition of negative tokens (somehow) in order to resolve these deadlocks.},
author = {Odrey, Nicholas G. and Mej{\'{i}}a, Gonzalo},
doi = {10.1016/j.rcim.2004.11.004},
issn = {07365845},
journal = {Robotics and Computer-Integrated Manufacturing},
month = {aug},
number = {4-5},
pages = {346--354},
title = {{An augmented Petri Net approach for error recovery in manufacturing systems control}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0736584504001176},
volume = {21},
year = {2005}
}
@article{gerstenberg2017intuitive,
annote = {- Human knowledge is organized in terms of intuitive theories (mental models)
- Human cognition is essentially causal inference over those intuitive theories

Intuitive Theory:
- Ontology of concepts - some of which are unobservable/abstract/latent (e.g. Force in Physical Intuitive Theories)
- System of causal laws that govern relationships between the concepts (e.g. Law of conservation of energy)
- Do not simply describe what happened but interpret the evidence through the vocabulary of the theory
- Operation of the theory might be implicit and unknown to the user
- The theories embody uncertainty because general inferences are drawn based on limited and potentially ambiguous evidence.
- Seeing through the lens of a theory may lead to a) explaining away evidence, b) reinterpret evidence to make it consistent, c) adopt a new theory if the evidence against the theory is too strong.

The purpose of Intuitive Theories that are generative:
- Prediction - what will happen in the future
- Inference - the sequence of events that led to the current situation
- Guide for action - based on cost function, the theory (model) can help guide action selection
- Counterfactual inferences - simulate alternative situations
- Explanation - narrow down on particular causes that are explanations.

Note: general causal statements that are based on repeated observations can be dissociated from the particular statement pertaining to the situation at hand. (E.g. generally A causes E, but in this case it did not).

Counterfactual Simulation Model (CSM) - model of simulating counterfactuals to arrive at different types of causes:
- Start with the assumption that for an event to be a cause, it needs to have made a difference, however minute, to the event. (Strict)
- Based on that filter, the type of cause can be determined on the type of difference made by the event / object.
- The process of mental simulation is central to the framework

Types of causes evaluated by the CSM:
- Whether-cause: Necessity. The event would not have happened if the cause had been removed
- How-cause: Process check. The particulars of the event would have changed if the particulars of the cause were changed.
- Sufficient-cause: If all other potential causes had been removed, the event would still have happened
- Robust-cause: Robustness of the causal connection; affected by the causal mechanism and causal distance. Measured by changing the particulars of the situation (factors other than the cause itself) and checking the change in the situation.

Note: people do not seem to consider the robustness of a cause in their calculations on causation...?

For psychological models of causation, how much we can learn from others' behaviour depends on our assumptions on their knowledge state and intentions.

There are 5 axes along which causal models can be characterized:
- Simple-Complex
- Proximal-Distal
- Past-Future: Evaluating past events, or predicting future events, or both
- Stable-Unstable: Robustness of the structure...?
- Actual-Potential: Fit to actual data vs. imagined counterfactuals...?},
author = {Gerstenberg, Tobias and Tenenbaum, Joshua B},
journal = {Oxford handbook of causal reasoning},
pages = {515--548},
publisher = {Oxford University Press New York, NY},
title = {{Intuitive theories}},
url = {http://cbmm.mit.edu/sites/default/files/publications/Intuitive Theories (Gerstenberg, Tenenbaum, 2016.pdf},
year = {2017}
}
@article{Yanco2015,
annote = {Abstract:
Analyzed the processes of 8/15 teams at the DRC. Evaluated their performance along various metrics of success
Discuss lessons learned from the study
General take-away: more sensor fusion, fewer operators, more automation

Previous research in RoboCup has found (Yanco {\&} Drury, 2007):
Utilize a single monitor for the interface
Avoid small video windows in the interface
Avoid window occlusion
Use one robot to view another when multiple are available
Design for the intended user. Not the developer
SA, of the human, the robot, and the resulting shared SA, should be the guiding principle of design
These guidelines apply in the case of a single operator; not multiple as in the DRC.

Tried to observe all tasks. However, there were three tasks that were observed for all teams:
Terrain - primarily a mobility task. Move from one end to another through obstacles
Valve - primarily a manipulation task. Three valves that needed to be rotated closed
Door - combined mobility + manipulation task. Go through doors by pressing door handle. One of the doors was weighted.

Analysis methods
YouTube videos of the DRC
Combining tandem field notes with control room notes
9 types of critical incidents identified. Observed 77 in the field; 33 of these were Falls (F) and Resets (R).
Tasks were broken into subtasks, and the subtasks classified according to Unobstructed Traverse (UT), Obstructed Traverse (OT), First-order manipulation (FOM), and Second-order manipulation (SOM) (using a tool): look at Table 5. The timing of all these subtasks was recorded.
Dialogue in the control room was coded based on:
The main topic under discussion (Subject) - there were 10 topics.
Situation Awareness - three levels of SA, good, neutral, and bad.
Type - Command, Inquiry, Response, Notification
Classification - Positive, Neutral, Negative
Emotion - None, Stress, Encouragement

Observed Interaction Methods
Interface Displays
Main types of windows:
Camera feeds - best teams have an average of 3
Point clouds - 1 point cloud
3D robot avatar - only the worst performing team did not have one
Simulated objects/obstacles
2D distance visualization - 2 teams in the bottom 8 had this
2D height maps
Status messages and sensor readings - only 1 window for all teams
Unified displays
The best 4 teams had 2 instances of unified displays that fused multiple sensor data streams. The worst 4 teams only had 1 instance of an unified display.
The unification of the streams of data was often contingent on operator preferences
Operators, Input Devices, and Screens
Check out Figure 1
Best team had:
1 active controller, 1 passive controller (supervisor), and 3 screens shared by both.
1 screen was dedicated to launching the robot, 1 was dedicated to actual control for the operator, and 1 was dedicated to supervision and monitoring tailored to the supervisor.
Artificially throttled all communication, so that when there was actually a degradation in comms, there was no difference to the UI
Types of displays and controls
Point clouds and images
Identifying points and interest and inserting models in the world model
Control of actuators and planning
Strategy, Reminders, Task Execution Cues and Configurations
Communications and status checks with robot
Almost all control was through keyboard and mouse. Cannot say anything about other methods such as game controller and gesture recognition because of single data points, and the fact that developers of the systems were also the controllers.
Control Methods
Two axes of evaluating the control methods - available interaction methods, and the amount of interaction needed for task. Missing: Reliability, because that is implementation dependent.
Available interaction methods (6):
JN - type individual joint numbers
JA - use a visualization of a cartesian rotation tool to rotate joints
PM - use a premade script with some input parameters
OBJ - used in manipulation where a simulated model plans and executes trajectories; trajectories are sent to the robot
FP - place individual footsteps on a surface
WP - place waypoints/goals for path/motion planning
Amount of interaction needed:
Low - Basically just WP
Medium - Basically FP or OBJ
High - JA/JN or PM where a lot of interaction is needed with the control interfaces
The best performing teams were able to use all the types of interaction methods and could span the scale on the amount of interaction needed to control the robot.

Results:
The number of falls and resets were indicative of a team's performance
Teams with the ability to use “low” amounts of interaction for control performed better. Evidenced by E outperforming D for manipulation
No correlation between the interaction methods, SA, and the speed at which tasks were completed. However, there was no incentive in the reward structure for completing subtasks quickly either.
Number of stressed utterances lowest in the top teams; except for Team C in Door, when they used a new operator for the task.
Teams scored poorly when they were observed talking about the state of communication.
Teams with more communication about the task at hand (objects, obstacles, etc.) performed better than teams with communication about ways to control the robot. Consistent between mobility and manipulation. E, with better manipulation interface than D, spoke less about robot control than D, and also performed better at manipulation than D.
Teams that used high/medium amounts of sensor fusion in displaying data to the operator, and used simulated object models (and/or foveated vision) performed significantly better than their counterparts.
Correlation between the number of interactions required and the success rate of the team. Also, negative correlation between the number of operators and the success rate (“too many cooks”)

Lessons:
Teams need to be successful at: Mobility, Manipulation, Situation Awareness of robot and surroundings, ways of commanding the robot.
Lower interaction leads to better success. If the operator is focused on controlling the robot's mobility instead of the task, performance on the task tends to suffer.
High amounts of sensor fusion is key. Best team had everything needed to control robot on a single monitor.
Many operators also requires operator fusion; which was often not well-implemented. Lack of operator fusion reduces the SA of operators
More sensor streams do not necessarily improve SA; especially if they're not fused together.
There needs to be more of an active effort in moving from multi-operator -{\textgreater} one robot paradigm to one operator -{\textgreater} one robot. It's easier and often tempting to throw multiple operators at the problem.
Always perform at the lowest common denominator in terms of conditions

Guidelines:
Operator effort was applied towards
Interacting with the robot through the interface. Effort reduced by:
More robot automation
More sensor fusion
Less number of operators
Lower bandwidth, when training and operating
Providing an understanding of the scene to the robot.  Effort reduced by:
Combining live video with simulation
Placing virtual objects in robot's world
Interacting with environment. Effort reduced by:
Low to medium amounts of control needed for mobility. Most control effort applied to “task”
(The interface is effectively invisible in terms of robot control if all effort is applied at the task (environment) interface level).
Therefore:
Increase sensor fusion
Decrease the number of operators
Decrease the amount of operator input needed to control robot mobility
Don't separate the control of components of robot. Also don't separate control according to task semantics, such as manipulation vs. mobility.
Plan for low bandwidth. Always.
Design for the intended user

Old:
More sensor fusion, fewer operators, and more automation -{\textgreater} better performance.

Interaction methods and situation awareness did not impact speed. Possibly because of a lack of correlation between SA and speed.

Talking about task is more useful than talking about robot control

Degraded comms - data rate control rather than execution speed control

Centralized SA is most important. You cannot substitute more operators to create more SA},
author = {Yanco, Holly A. and Norton, Adam and Ober, Willard and Shane, David and Skinner, Anna and Vice, Jack},
doi = {10.1002/rob.21568},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {may},
number = {3},
pages = {420--444},
title = {{Analysis of Human-robot Interaction at the DARPA Robotics Challenge Trials}},
url = {http://doi.wiley.com/10.1002/rob.21568 https://docs.google.com/document/d/1hV6ICOF5n-ba77ct9wo6fZ0GM0L-6EP4J78GzigU{\_}58/edit?usp=sharing},
volume = {32},
year = {2015}
}
@article{Christensen2008,
annote = {Contribution:
- Requires no special hardware
- Lightweight fault detection software
- Record sensory data when robot is operating as intended and when various hardware faults are present
- Use knowledge of how information flows post fault to detect the fault

Methods used in the past to detect faults:

- Artificial Neural Networks
- Banks of Kalman filters for state estimation
- Dynamic Bayesian Networks, solved with paricle filters
- Aritificial Immune-Systems - trained on correct behaviour of the robot and then capacity to detect anomalies is tested
- Novelty filter - similar in that it ignores inputs similar to trained data.

Properties of this work:
- Authors use Neural Networks - Time-Delay Neural Networks. These are simply feed-forward networks; not RNNs.
- The system they are trying to detect faults on is treated as a black-box program.
- Inject faults using software implemented fault injection (SWIFI)
- Formally, given a set of past sensor data and control signals, indicate a fault or no fault.

Performs no fault diagnosis; only detection.},
author = {Christensen, Anders Lyhne and O'Grady, Rehan and Birattari, Mauro and Dorigo, Marco},
doi = {10.1007/s10514-007-9060-9},
issn = {0929-5593},
journal = {Autonomous Robots},
month = {jan},
number = {1},
pages = {49--67},
title = {{Fault detection in autonomous robots based on fault injection and learning}},
url = {http://link.springer.com/10.1007/s10514-007-9060-9},
volume = {24},
year = {2008}
}
@article{Pashazadeh2018,
annote = {Business economics dictate better FDI on wind turbines to minimize downtime and maximize profits. Wind turbines are often in hard places, so maintainence is high - using better FDD is necessary. There are two approaches - model-based and "black-box". This is a "black-box" data-driven approach to fault isolation.

Contribution: isolate faults using feature extraction, feature selection, parallel classifiers, and classifier fusion},
author = {Pashazadeh, Vahid and Salmasi, Farzad R. and Araabi, Babak N.},
doi = {10.1016/j.renene.2017.03.051},
issn = {09601481},
journal = {Renewable Energy},
month = {feb},
pages = {99--106},
title = {{Data driven sensor and actuator fault detection and isolation in wind turbine using classifier fusion}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0960148117302422},
volume = {116},
year = {2018}
}
@article{Johnson2015,
annote = {Placed 1st in the Virtual Robotics Challenge, and 2nd in the main event.

Problems in moving from sim to real:
The sensors were not time synchronized anymore
There was some issue in state estimation from the joint states
The controller frequency was too low, and not time synchronized. Had to redo the JVM to make sure timing constraints were top priority.
Needed to modify robot hardware, because the reach of the real robot did not match its reach in simulation

UI Design:
Used a coactive (?) design approach
Added modes to the UI to allow for more dense point clouds, as well as adding colour and texture to the point clouds
Fused the input from multiple cameras into a virtual dome to allow for virtual movement of the head. This allowed for one UI window, but more SA
Colour indicators on joints to know if the robot was near joint limits
Create a trajectory visualizer for safe motion preview
Create interactable objects - generate trajectories for the robot based on the motions of the interactable objects

Lessons on Interaction Design
Operator must have appropriate amount of skill and training
The interface must leverage that skill
Coactive Design
Factors being optimized: observability, predictability, directability
Things being optimized: the interface, and the robot algorithms+behaviours
Basically, create runtime “options” of the interface and behaviors, and then design based on iterative feedback of using those options

Software Practices
Invest in debugging tools
Write unit tests. Tests ran in under 20 min for every commit, and nightly for an “acceptance”
Know when to be sufficient instead of minimize/maximize. For example, there is no benefit to reducing bandwidth utilization below the minimum expected.
Optimize, always, for completion of task instead of any other metric. And favour resiliency over any other solution (also a point from Kiva cofounder's talk)},
author = {Johnson, Matthew and Shrewsbury, Brandon and Bertrand, Sylvain and Wu, Tingfan and Duran, Daniel and Floyd, Marshall and Abeles, Peter and Stephen, Douglas and Mertins, Nathan and Lesman, Alex and Carff, John and Rifenburgh, William and Kaveti, Pushyami and Straatman, Wessel and Smith, Jesper and Griffioen, Maarten and Layton, Brooke and de Boer, Tomas and Koolen, Twan and Neuhaus, Peter and Pratt, Jerry},
doi = {10.1002/rob.21571},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {mar},
number = {2},
pages = {192--208},
title = {{Team IHMC's Lessons Learned from the DARPA Robotics Challenge Trials}},
url = {http://doi.wiley.com/10.1002/rob.21571 https://docs.google.com/document/d/1hV6ICOF5n-ba77ct9wo6fZ0GM0L-6EP4J78GzigU{\_}58/edit?usp=sharing},
volume = {32},
year = {2015}
}
@inproceedings{Kim2006,
address = {New York, New York, USA},
annote = {A robot software system that reconfigures the components that are used to perform certain tasks based on a capability (?) specification and a suitability metric for each component in each situation. The details of the component specification or the performance metric are unclear.

The robot also uses RL to learn which architecture adaptations might be suitable to a failure situation. Again, the details are unclear on how the action space is specified or how the state space is expressed. Case-based reasoning of some form is used though...},
author = {Kim, Dongsun and Lee, Sukhan and Park, Sooyong and Jin, Youngkyun and Chang, Hyeongsoo and Park, Yu-Sik and Ko, In-Young and Lee, Kwanwoo and Lee, Junhee and Park, Yeon-Chool},
booktitle = {Proceedings of the 2006 international workshop on Self-adaptation and self-managing systems - SEAMS '06},
doi = {10.1145/1137677.1137693},
isbn = {1595934030},
pages = {79},
publisher = {ACM Press},
title = {{SHAGE: a framework for self-managed robot software}},
url = {http://portal.acm.org/citation.cfm?doid=1137677.1137693},
year = {2006}
}
@article{Khalastchi2017,
annote = {More work by Khalastchi and Kalech showing that hybrid unsupervised + supervised learning approaches to anomaly detection are expected to perform best.},
author = {Khalastchi, Eliahu and Kalech, Meir and Rokach, Lior},
doi = {10.1016/j.eswa.2017.03.058},
issn = {09574174},
journal = {Expert Systems with Applications},
month = {sep},
pages = {372--383},
title = {{A hybrid approach for improving unsupervised fault detection for robotic systems}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S095741741730221X},
volume = {81},
year = {2017}
}
@inproceedings{Birnbaum2015,
annote = {For UAVs, detect faults or malicious actors by creating an expected behavioural profile and then detecting anomalies away from that profile.

One of the contributions is a method of taking a UAV flight plan and automatically generating the expected behavioural profile through simulation.},
author = {Birnbaum, Zachary and Dolgikh, Andrey and Skormin, Victor and O'Brien, Edward and Muller, Daniel and Stracquodaine, Christina},
booktitle = {2015 International Conference on Unmanned Aircraft Systems (ICUAS)},
doi = {10.1109/ICUAS.2015.7152425},
isbn = {978-1-4799-6010-1},
month = {jun},
pages = {1310--1319},
publisher = {IEEE},
title = {{Unmanned Aerial Vehicle security using behavioral profiling}},
url = {http://ieeexplore.ieee.org/document/7152425/},
year = {2015}
}
@article{Laursen2018,
abstract = {Programming robotic assembly for industrial small-batch production is challenging; hence, it is vital to increase robustness and reduce development effort in order to achieve flexible robotic automation. A human who has made an assembly error will often simply undo the process until the error is undone and then restart the assembly. Conceptually, robots could do the same. This paper introduces a programming model that enables robot assembly programs to be executed in reverse. We investigate the challenges in running robot programs backwards and present a classification of reversibility characteristics. We demonstrate how temporarily switching the direction of program execution can be an efficient error recovery mechanism. Moreover, we demonstrate additional benefits arising from supporting reversibility in an assembly language, such as increased code reuse and automatically derived disassembly sequences. As a default approach to reversibility, we use program inversion and statement-level inversion of commands, but with a novel override option providing alternative sequences for asymmetric reverse actions. To efficiently program for this model, this paper introduces a new domain-specific language, SCP-RASQ (Simple C++ Reversible Assembly SeQuences). In initial experiments, where 200 consecutive assemblies of two industrial cases were performed, 18 of 22 errors were corrected automatically using only the trial-and-error capabilities that come from reverse execution.},
annote = {Cool idea of creating a reversible programming language where in the event of an error, programs can be executed in reverse. There's a lot of things to keep in mind when making such a language, but this is a step in that direction.

This is the journal paper to the 2015 IROS paper.},
author = {Laursen, Johan Sund and Ellekilde, Lars-Peter and Schultz, Ulrik Pagh},
doi = {10.1017/S0263574717000613},
issn = {0263-5747},
journal = {Robotica},
month = {may},
number = {5},
pages = {625--654},
title = {{Modelling reversible execution of robotic assembly}},
url = {https://www.cambridge.org/core/product/identifier/S0263574717000613/type/journal{\_}article},
volume = {36},
year = {2018}
}
@incollection{Chandola2017,
address = {Boston, MA},
annote = {Redux of the previous survey papers by the same authors with easier to understand graphs, etc. Keeping this reference incase I want some quick reference material.},
author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
booktitle = {Encyclopedia of Machine Learning and Data Mining},
doi = {10.1007/978-1-4899-7687-1_912},
pages = {42--56},
publisher = {Springer US},
title = {{Anomaly Detection}},
url = {http://link.springer.com/10.1007/978-1-4899-7687-1{\_}912},
year = {2017}
}
@inproceedings{uesato2018rigorous,
annote = {Use adversarial training to estimate the probability of execution failure given the initial conditions. Used in an RL setting.},
author = {Uesato, Jonathan and Kumar, Ananya and Szepesvari, Csaba and Erez, Tom and Ruderman, Avraham and Anderson, Keith and Dvijotham, Krishnamurthy (Dj) and Heess, Nicolas and Kohli, Pushmeet},
booktitle = {International Conference on Learning Representations},
title = {{Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures}},
url = {https://openreview.net/forum?id=B1xhQhRcK7},
year = {2019}
}
@inproceedings{wang2019learning,
annote = {- Create an embedding-like representation of the states to learn transitions from clusters of states to other clusters
- Recovery learning is trying to use simple heuristics when trying actions to transition between clusters of states.},
author = {Wang, Austin S and Kroemer, Oliver},
booktitle = {Proceedings of the 2019 IEEE International Conference on Robotics and Automation},
title = {{Learning Robust Manipulation Strategies with Multimodal State Transition Models and Recovery Heuristics}},
url = {https://www.ri.cmu.edu/wp-content/uploads/2019/03/Kroemer{\_}Wang{\_}ICRA{\_}2019.pdf},
year = {2019}
}
@article{Baydar2004,
annote = {Work in simulation that tries to use offline simulation of a system to generate diagnoses and then recoveries. For diagnosis, it uses simple Bayes' Nets based on the errors that were observed after Monte-Carlo simulation. For recoveries, the simulation software seems to have the ability to generate controller programs from simulation parameters, so the authors create a GA to generate these parameters and resimulate until the error is gone. Not widely applicable, and not tested on a real robot.},
author = {Baydar, Cem and Saitou, Kazuhiro},
doi = {10.1023/B:JIMS.0000037716.69868.d0},
issn = {0956-5515},
journal = {Journal of Intelligent Manufacturing},
month = {oct},
number = {5},
pages = {679--692},
title = {{Off-line error prediction, diagnosis and recovery using virtual assembly systems}},
url = {http://link.springer.com/10.1023/B:JIMS.0000037716.69868.d0},
volume = {15},
year = {2004}
}
@inproceedings{Abreu2009,
annote = {The conference paper that was a result of the workshop paper, and came before the journal paper's many experiments. Easier to digest format of the journal.

Note: The program is called BARINEL here, and ZOLTAR in the journal.},
author = {Abreu, Rui and Zoeteweij, Peter and van Gemund, Arjan J.C.},
booktitle = {2009 IEEE/ACM International Conference on Automated Software Engineering},
doi = {10.1109/ASE.2009.25},
isbn = {978-1-4244-5259-0},
month = {nov},
pages = {88--99},
publisher = {IEEE},
title = {{Spectrum-Based Multiple Fault Localization}},
url = {http://ieeexplore.ieee.org/document/5431781/},
year = {2009}
}
@article{Wang2016,
annote = {Data driven method of fault localization in cloud computing - uses Canonical Correlation Analysis (CCA) to extract features that are fed into EWMA, which then detects faults with feature selection and SVMs.},
author = {Wang, Tao and Zhang, Wenbo and Ye, Chunyang and Wei, Jun and Zhong, Hua and Huang, Tao},
doi = {10.1109/TSMC.2015.2430834},
issn = {2168-2216},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
month = {jan},
number = {1},
pages = {61--75},
title = {{FD4C: Automatic Fault Diagnosis Framework for Web Applications in Cloud Computing}},
url = {http://ieeexplore.ieee.org/document/7116586/},
volume = {46},
year = {2016}
}
@article{Fire2015,
annote = {Traditional Causal Discovery is insufficient:
- Does not align with human perceptions
- Do not indicate detection variables humans would label as cause and effect
- The segmentation of time is also a problem

Perceptual causality links agent actions to detected changes in fluents: actions alone are not enough. Actions are also hierarchically defined. Extends a lot of prior works by the same authors.

Setting up the problem:
- Assumption: agent actions are causes of effects observed in video.
- Assumption: temporal lag between cause effect is small and bounded by a predefined epsilon
- Assumption: causes can be observed by measuring the cooccurence of actions and effects.
- Assumption: the set of prespecified actions is sufficient. Computer knows when they occur.
- Assumption: causal faithfulness - multiple causes do not exactly cancel.
- Assumption: each effect is a function of its immediate cause and an independent error. Actions depend on their own exogenous variables.

Solving:
- The total space of causal relationships is the number of possible fluent changes multiplied by the number of actions.
- The probability of a cause is simply the relative frequency of an observed effect given an action.
- Use the relative frequencies to build a Bayes' Net causal model such that you maximize information gain.
- And then iterate. There is an algortihm to that in Sec. 5.2 that bears more scrutiny.
- Then, to ascribe causes to actions, represent them in an AND-OR graph. This is dealing with the potential of ascribing multiple actions to the same (set) of causes. Not relevant to my project?
- Causal AND-OR perceptual graphs present an unentangled view of the causal structure.
- The AND-OR graph also provides a "prior" on the Causal Bayes Net.

Experiments:
- In simulated data of figuring out vending machine causes, the method is able to show robustness
- Show that noise and time window choice are major factors.
- More training examples leads to better detection of causal structures.

Thoughts and Future Directions:
- General causal networks were too vague. Using CogSci, they grounded to the AND-OR structure displayed with action hierarchies
- "... getting causality wrong is OK. Humans create a causal explanation even from insufficient info, and that still yields useful results..."
- Adapt the model to "surprising" data.},
author = {Fire, Amy and Zhu, Song-Chun},
doi = {10.1145/2809782},
issn = {21576904},
journal = {ACM Transactions on Intelligent Systems and Technology},
month = {nov},
number = {2},
pages = {1--22},
title = {{Learning Perceptual Causality from Video}},
url = {http://dl.acm.org/citation.cfm?doid=2850424.2809782 http://amyfire.com/papers/ACM2014{\_}Final.pdf},
volume = {7},
year = {2015}
}
@inproceedings{misra2015environment,
annote = {Dataset used by the She et al. to evaluate hypothesis spaces.},
author = {Misra, Dipendra Kumar and Tao, Kejia and Liang, Percy and Saxena, Ashutosh},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
pages = {992--1002},
title = {{Environment-driven lexicon induction for high-level instructions}},
url = {https://cs.stanford.edu/{~}pliang/papers/environment-acl2015.pdf https://worksheets.codalab.org/worksheets/0x7f9151ec074f4f589e4d4786db7bb6de/},
volume = {1},
year = {2015}
}
@article{Kress-Gazit2009,
annote = {Given an LTL specification of a task, robot model, and a model of assumptions on admissible environments, construct a discrete automaton to satisfy the task specification using the available robot controllers.

To generate automata from robot and environment descriptions, they use an iterative game where the algorithm creates an automaton and the environment model checker tries to break it.

Once the automaton is created, it can be generated into a continuous controller based on prior work.

The method of generation and specification guarantees that faults disallowed by the static checks are never seen.},
author = {Kress-Gazit, H. and Fainekos, G.E. and Pappas, G.J.},
doi = {10.1109/TRO.2009.2030225},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
month = {dec},
number = {6},
pages = {1370--1381},
title = {{Temporal-Logic-Based Reactive Mission and Motion Planning}},
url = {http://ieeexplore.ieee.org/document/5238617/},
volume = {25},
year = {2009}
}
@inproceedings{gao2016physical,
annote = {The physical causality of action verbs is underexplored. Identified 18 categories of physical causality for action verbs. Then built detectors for a subset of these changes.

Possibly relevant related work: Fathi and Rehg, 13, have used detection of actions as detection of changes between video frames.

Work on the TACoS dataset where most verbs are goal directed and deal with a change of state. Then used Mechanical Turk to characterize the actions along 18 dimensions.

There are two ways to get and use causality knowledge. The first is knowledge driven and the other is learnt (using CRFs). The training data for both is from Mechanical Turkers. The causal knowledge is represented as agent, patient, source, destination. The effect is annotated as a change-of-state, which contains is a 0, 1 bit vector depending on the Mechanical Turk annotations.

First set of results show that the causal approach (in this paper) performs best for the tasks of grounding the role of patient given an action, as well as in overall grounding of roles.

Second set of results begins to show that the causailty structure of new words can be predicted from the learned models from the other verbs.},
author = {Gao, Qiaozi and Doering, Malcolm and Yang, Shaohua and Chai, Joyce},
booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
pages = {1814--1824},
title = {{Physical causality of action verbs in grounded language understanding}},
url = {http://www.aclweb.org/anthology/P16-1171},
volume = {1},
year = {2016}
}
@article{Zenil2017,
abstract = {We demonstrate that the algorithmic information content of a system is deeply connected to its potential dynamics, thus affording an avenue for moving systems in the information-theoretic space and controlling them in the phase space. To this end we performed experiments and validated the results on (1) a very large set of small graphs, (2) a number of larger networks with different topologies, and (3) biological networks from a widely studied and validated genetic network (e.coli) as well as on a significant number of differentiating (Th17) and differentiated human cells from high quality databases (Harvard's CellNet) with results conforming to experimentally validated biological data. Based on these results we introduce a conceptual framework, a model-based interventional calculus and a reprogrammability measure with which to steer, manipulate, and reconstruct the dynamics of non- linear dynamical systems from partial and disordered observations. The method consists in finding and applying a series of controlled interventions to a dynamical system to estimate how its algorithmic information content is affected when every one of its elements are perturbed. The approach represents an alternative to numerical simulation and statistical approaches for inferring causal mechanistic/generative models and finding first principles. We demonstrate the framework's capabilities by reconstructing the phase space of some discrete dynamical systems (cellular automata) as case study and reconstructing their generating rules. We thus advance tools for reprogramming artificial and living systems without full knowledge or access to the system's actual kinetic equations or probability distributions yielding a suite of universal and parameter-free algorithms of wide applicability ranging from causation, dimension reduction, feature selection and model generation.},
annote = {(Lots of typos, etc. so some parts are better explained in the previous previous Zenil2016 paper, reference 20 in this work)

- Algorithmic complexity is a measure such that if an object 's' is random, then the algorithmic complexity of the object is about the length of 's' itself. It is the accepted measure of the intrinsic randomness of an object
- Complexity is also a measure of compressibility, but current compression techniques are entropy estimators, which don't capture all the information about a process.
- Complexity is the length of the shortest compressed file reproducing 's' when decompressing it.
- CTM is a measure of complexity of strings upto 12 bits in length. For longer strings, we can use BDM
- For graphs, use the adjacency matrix of the graph as a representation that can be operated upon by BDM

Key ideas:
- In an algorithmically random object, any change goes unnoticed because no perturbation can lead to a dramatic change of its already high algorithmic content.
- In a deterministic system, the shortest length of the generating mechanism for the system's state in binary complexity can only grow by log(t). Therefore, any deviations outside of those bounds are non-causal and unrelated.
- The paper introduces a "reprogrammability index" that can be used as a measure of system "sophistication".},
archivePrefix = {arXiv},
arxivId = {1709.05429},
author = {Zenil, Hector and Kiani, Narsis A. and Marabita, Francesco and Deng, Yue and Elias, Szabolcs and Schmidt, Angelika and Ball, Gordon and Tegn{\'{e}}r, Jesper},
eprint = {1709.05429},
month = {sep},
title = {{An Algorithmic Information Calculus for Causal Discovery and Reprogramming Systems}},
url = {http://arxiv.org/abs/1709.05429 http://complexitycalculator.com/HowItWorks.html},
year = {2017}
}
@inproceedings{Gizzi2019,
address = {Oslo, Norway},
annote = {(after the paper is published, add the URL)

Motivation:
- Some old works can only use existing action primitives and cannot discover new ones (planning), or can discover new ones with RL but require a lot of interaction
- The solution is MacGyvering - solution exists but is not immediately available to a planning agent given its knowledge so far
- Humans and animals use a) object exploration and b) behavioural variation to creatively problem solve
- Use change-point detection to break existing actions into sub-parts, and indepently use sub-parts for subsequent exploration of new actions

Experiment and other notes:
- Creates a framework for action discovery that is agnostic to planner, change-point detector, etc.
- Representation of actions (and sub-actions?) are parameterized by the objects that are used and the affordances of those objects.
- Change point detection algorithm actually used is "Bayesian Changepoint detection" in trajectories},
author = {Gizzi, Evana and Castro, Mateo Guaman and Sinapov, Jivko},
booktitle = {IEEE 9th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
publisher = {IEEE},
title = {{Creative Problem Solving by Robots Using Action Primitive Discovery}},
year = {2019}
}
