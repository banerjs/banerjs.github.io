---
title: "Example of Bayesian Analysis"
output: html_notebook
---

Note: I am not a statistician and am often prone to confusing myself in data analysis. This notebook captures some of _my_ understanding of the subject, particularly from a practical stand-point of _using_ the methods in our HRI studies. There are much better resources for actually conducting Bayesian Analysis that are out there, and I will link to those when I can. Otherwise, I have found that Stack Overflow is a wonderfull resource.


## Load a dataset

We'll use the `warpbreaks` dataset that comes with R

```{r}
data("warpbreaks")
head(warpbreaks)
```

The goal of this dataset is to predict the number of breaks in a yarn of wool given the type of wool (A or B), and the tension in the loom (L, M, H). This is a Two-Way ANOVA.

```{r}
table(warpbreaks$wool, warpbreaks$tension)
```

Visualize the data in a boxplot

```{r}
library(car)
library(grid)
library(gridExtra)
library(ggplot2)
library(tidyverse)
library(broom)

warpbreaks %>%
  ggplot(aes(log(breaks), x = wool, fill = tension)) +
  geom_boxplot()
```

## Using built-in frequentist functions

Let's check if the data are normal by group.

```{r}
# p1 = warpbreaks %>%
#   ggplot(aes(y = log(breaks), x = wool, fill = wool)) +
#   geom_boxplot(alpha = 0.2) +
#   geom_jitter()
# 
# p2 = warpbreaks %>%
#   ggplot(aes(y = log(breaks), x = tension, fill = tension)) +
#   geom_boxplot(alpha = 0.2) +
#   geom_jitter()
p1 = warpbreaks %>%
  ggplot(aes(log(breaks), fill = wool)) +
  geom_density(alpha = 0.2)

p2 = warpbreaks %>%
  ggplot(aes(log(breaks), fill = tension)) +
  geom_density(alpha = 0.2)

grid.arrange(p1, p2)

# We can also run quick bartlett.test and shapiro.test to check for
# homoskedasticity and normality
print(bartlett.test(log(breaks) ~ tension, data = warpbreaks))
print(bartlett.test(log(breaks) ~ wool, data = warpbreaks))
print(with(warpbreaks, shapiro.test(log(breaks)[wool == 'A'])))
print(with(warpbreaks, shapiro.test(log(breaks)[wool == 'B'])))
print(with(warpbreaks, shapiro.test(log(breaks)[tension == 'L'])))
print(with(warpbreaks, shapiro.test(log(breaks)[tension == 'M'])))
print(with(warpbreaks, shapiro.test(log(breaks)[tension == 'H'])))
```

The data is approximately normal and does have equal variances between groups. So if we want to, we can run our regular frequentist stats.

```{r}
# print(wilcox.test(log(breaks) ~ wool, data = warpbreaks))
print(t.test(log(breaks) ~ wool, data = warpbreaks))
```
So according to the naive tests, there is no difference between the wool types... Should we believe this?

In the meanwhile, let's run Omnibus tests on the tension parameter.

```{r}
aov1 = aov(log(breaks) ~ tension, data = warpbreaks)
print(summary(aov1))
```
What does this mean? It shows that overall, a large portion of the breaks are explained by the tension variable. Let's assume that we're interested in all possible pairwise comparisons of the tension variables, then using Tukey's HSD:

```{r}
print(TukeyHSD(aov1))
```

So there is a significant difference between L and H, and no significant difference otherwise. This seems a bit restrictive, perhaps?


## The Linear Model

As shown in this wonderful [link](https://lindeloev.github.io/tests-as-linear/), the tests above are just linear regression tests on the data and the associated coefficients. So assume a linear model like the one below:

$$log(breaks_i) = \beta_0 + \beta_1 tension_{i==M} + \beta_2 tension_{i==H}$$
Then the act of running ANOVA is simply fitting the above model to derive values for $\beta_0, \beta_1, \beta_2$, and then making inferences on the importance of the tension variable's levels based on the confidence interval of those coefficients (this is a super simplified and potentially problematic statement). But what do I mean by that?

Assume that data point $i$ has a $tension$ of $L$. Then, by the equation, we **expect** the value of $log(breaks_i) = \beta_0$.

Similarly, assume that data point $i$ has a tension of $M$. Then, by the equation, we **expect** the value of $log(breaks_i) = \beta_0 + \beta_1$.

And continuing that logic, once we derive the confidence intervals of the variables $\beta_1$ and $\beta_2$, we can comment on the significance of the difference in breaks between levels M and H to the level L (we can also get the third, difference because it is a difference of the first two).

A quick example:

```{r}
aov2 = lm(log(breaks) ~ tension, data = warpbreaks)
print(anova(aov2))
print(summary(aov2))
```

Notice that the first summary table is exactly the same as the ANOVA from earlier. And the coefficient estimates show significant effects of L compared to the other levels. Why are the p-values in the second summary table more significant than in the pairwise comparisons earlier? It's because the pairwise comparisons earlier account for the fact that you're making multiple comparisons. But again, note that you didn't have to do that! You could've used the actual regression equation and the problem setup of how the **expectation** of $log(breaks)$ values relate for the different levels.

The above paragraph subtly brings in the notion of probability by mentioning **expectation**, and this is important. What we're doing every time that we conduct a hypothesis test is that we're coming up with an equation for how the factors affect the expected value of our dependent variable. Mathematically, the problem above has actually been set up so that:

$$\begin{aligned}
\mathop{\mathbb{E}}[log(breaks_{i==L})] &= \beta_0 \\
\mathop{\mathbb{E}}[log(breaks_{i==M})] &= \beta_0 + \beta_1 \\
\mathop{\mathbb{E}}[log(breaks_{i==H})] &= \beta_0 + \beta_2
\end{aligned}$$

And all our inferences have been borne out by analyzing our confidence of the $\beta$ estimates in those formulae.


## Plotting the differences so far




## The Two-Way ANOVA

Clearly from the first figure, there is an effect of the wool type when we consider tension (an interaction effect).